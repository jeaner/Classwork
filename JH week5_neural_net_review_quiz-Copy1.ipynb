{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from keras.layers import Input, Dense, Dropout, BatchNormalization\n",
    "from keras.models import Model, load_model\n",
    "import keras.backend as K\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>cigs</th>\n",
       "      <th>years</th>\n",
       "      <th>fbs</th>\n",
       "      <th>famhist</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>thal</th>\n",
       "      <th>num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>145</td>\n",
       "      <td>233</td>\n",
       "      <td>50.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>150</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>67</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>160</td>\n",
       "      <td>286</td>\n",
       "      <td>40.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>108</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>67</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>120</td>\n",
       "      <td>229</td>\n",
       "      <td>20.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>129</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>130</td>\n",
       "      <td>250</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>187</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>130</td>\n",
       "      <td>204</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>172</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  sex  cp  trestbps  chol  cigs  years  fbs  famhist  restecg  thalach  \\\n",
       "0   63    1   1       145   233  50.0   20.0    1        1        2      150   \n",
       "1   67    1   4       160   286  40.0   40.0    0        1        2      108   \n",
       "2   67    1   4       120   229  20.0   35.0    0        1        2      129   \n",
       "3   37    1   3       130   250   0.0    0.0    0        1        0      187   \n",
       "4   41    0   2       130   204   0.0    0.0    0        1        2      172   \n",
       "\n",
       "   exang  thal  num  \n",
       "0      0     6    0  \n",
       "1      1     3    2  \n",
       "2      1     7    1  \n",
       "3      0     3    0  \n",
       "4      0     3    0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the csv file is under the week 2 content\n",
    "filename = 'C:/MSDS/MachL/Wk6/heart.disease.data.clean.csv'\n",
    "heart_df = pd.read_csv(filename)\n",
    "heart_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set values of target column to 0 (no heart disease) and 1 (heart disease)\n",
    "heart_df.loc[heart_df['num'] > 0, 'num'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    157\n",
       "1    125\n",
       "Name: num, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heart_df['num'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feat_cols = [c for c in heart_df.columns if c != 'num']\n",
    "# select features and columns from the dataframe as we did in the neural net exercise\n",
    "features = heart_df[feat_cols].values\n",
    "targets = heart_df['num'].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(features,\n",
    "                                                    targets,\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=42,\n",
    "                                                    stratify=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Layer dense_1 was called with an input that isn't a symbolic tensor. Received type: <class 'type'>. Full input: [<class 'keras.layers.core.Dense'>]. All inputs to the layer should be tensors.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\Jeane\\Anaconda3\\lib\\site-packages\\keras\\engine\\topology.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    439\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 440\u001b[1;33m                 \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_keras_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    441\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Jeane\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36mis_keras_tensor\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    464\u001b[0m                           tf.SparseTensor)):\n\u001b[1;32m--> 465\u001b[1;33m         raise ValueError('Unexpectedly found an instance of type `' + str(type(x)) + '`. '\n\u001b[0m\u001b[0;32m    466\u001b[0m                          'Expected a symbolic tensor instance.')\n",
      "\u001b[1;31mValueError\u001b[0m: Unexpectedly found an instance of type `<class 'type'>`. Expected a symbolic tensor instance.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-ae67e827f519>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mx1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDense\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mx2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDense\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'sigmoid'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Jeane\\Anaconda3\\lib\\site-packages\\keras\\engine\\topology.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[0;32m    571\u001b[0m                 \u001b[1;31m# Raise exceptions in case the input is not compatible\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    572\u001b[0m                 \u001b[1;31m# with the input_spec specified in the layer constructor.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 573\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massert_input_compatibility\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    574\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    575\u001b[0m                 \u001b[1;31m# Collect input shapes to build layer.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Jeane\\Anaconda3\\lib\\site-packages\\keras\\engine\\topology.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    444\u001b[0m                                  \u001b[1;34m'Received type: '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    445\u001b[0m                                  \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'. Full input: '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 446\u001b[1;33m                                  \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'. All inputs to the layer '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    447\u001b[0m                                  'should be tensors.')\n\u001b[0;32m    448\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Layer dense_1 was called with an input that isn't a symbolic tensor. Received type: <class 'type'>. Full input: [<class 'keras.layers.core.Dense'>]. All inputs to the layer should be tensors."
     ]
    }
   ],
   "source": [
    "K.clear_session()\n",
    "inputs = Input(shape=(features.shape[1], ))\n",
    "# complete the neural net design\n",
    "x1 = Dense\n",
    "x2 = Dense\n",
    "predictions = Dense(1, activation='sigmoid')(x2)\n",
    "\n",
    "model = Model(inputs, predictions)\n",
    "# choose an optimizer and use the correct loss for binary classification\n",
    "model.compile(optimizer='adam', loss='ms', metrics=['accuracy'])\n",
    "#verbose=0 makes so you don't freeze up "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 191 samples, validate on 34 samples\n",
      "Epoch 1/500\n",
      "191/191 [==============================] - 0s 1ms/step - loss: 1.8920 - acc: 0.5654 - val_loss: 5.7660 - val_acc: 0.3824\n",
      "Epoch 2/500\n",
      "191/191 [==============================] - 0s 10us/step - loss: 2.9930 - acc: 0.5864 - val_loss: 3.6515 - val_acc: 0.3529\n",
      "Epoch 3/500\n",
      "191/191 [==============================] - 0s 11us/step - loss: 1.8765 - acc: 0.5602 - val_loss: 2.5165 - val_acc: 0.4706\n",
      "Epoch 4/500\n",
      "191/191 [==============================] - 0s 11us/step - loss: 1.9357 - acc: 0.5340 - val_loss: 2.1848 - val_acc: 0.5000\n",
      "Epoch 5/500\n",
      "191/191 [==============================] - 0s 14us/step - loss: 2.2429 - acc: 0.5288 - val_loss: 2.1605 - val_acc: 0.4706\n",
      "Epoch 6/500\n",
      "191/191 [==============================] - 0s 14us/step - loss: 1.9952 - acc: 0.5288 - val_loss: 2.3915 - val_acc: 0.4412\n",
      "Epoch 7/500\n",
      "191/191 [==============================] - 0s 12us/step - loss: 1.6344 - acc: 0.5445 - val_loss: 2.9070 - val_acc: 0.4118\n",
      "Epoch 8/500\n",
      "191/191 [==============================] - 0s 11us/step - loss: 1.4817 - acc: 0.5759 - val_loss: 3.3994 - val_acc: 0.3235\n",
      "Epoch 9/500\n",
      "191/191 [==============================] - 0s 12us/step - loss: 1.5642 - acc: 0.5654 - val_loss: 3.4456 - val_acc: 0.3529\n",
      "Epoch 10/500\n",
      "191/191 [==============================] - 0s 15us/step - loss: 1.5636 - acc: 0.5550 - val_loss: 3.0338 - val_acc: 0.3824\n",
      "Epoch 11/500\n",
      "191/191 [==============================] - 0s 16us/step - loss: 1.3958 - acc: 0.5916 - val_loss: 2.3880 - val_acc: 0.5000\n",
      "Epoch 12/500\n",
      "191/191 [==============================] - 0s 12us/step - loss: 1.1853 - acc: 0.5864 - val_loss: 1.7711 - val_acc: 0.5588\n",
      "Epoch 13/500\n",
      "191/191 [==============================] - 0s 13us/step - loss: 1.0271 - acc: 0.5864 - val_loss: 1.3213 - val_acc: 0.5294\n",
      "Epoch 14/500\n",
      "191/191 [==============================] - 0s 13us/step - loss: 0.9380 - acc: 0.5759 - val_loss: 1.0471 - val_acc: 0.5588\n",
      "Epoch 15/500\n",
      "191/191 [==============================] - 0s 16us/step - loss: 0.8773 - acc: 0.5445 - val_loss: 0.8641 - val_acc: 0.6176\n",
      "Epoch 16/500\n",
      "191/191 [==============================] - 0s 14us/step - loss: 0.8334 - acc: 0.5288 - val_loss: 0.7772 - val_acc: 0.6471\n",
      "Epoch 17/500\n",
      "191/191 [==============================] - 0s 14us/step - loss: 0.8128 - acc: 0.5759 - val_loss: 0.7885 - val_acc: 0.6765\n",
      "Epoch 18/500\n",
      "191/191 [==============================] - 0s 17us/step - loss: 0.8308 - acc: 0.6073 - val_loss: 0.8376 - val_acc: 0.6176\n",
      "Epoch 19/500\n",
      "191/191 [==============================] - 0s 13us/step - loss: 0.8700 - acc: 0.6073 - val_loss: 0.8653 - val_acc: 0.6176\n",
      "Epoch 20/500\n",
      "191/191 [==============================] - 0s 10us/step - loss: 0.8898 - acc: 0.6073 - val_loss: 0.8408 - val_acc: 0.6471\n",
      "Epoch 21/500\n",
      "191/191 [==============================] - 0s 12us/step - loss: 0.8781 - acc: 0.6021 - val_loss: 0.7750 - val_acc: 0.6176\n",
      "Epoch 22/500\n",
      "191/191 [==============================] - 0s 12us/step - loss: 0.8506 - acc: 0.5969 - val_loss: 0.7105 - val_acc: 0.7059\n",
      "Epoch 23/500\n",
      "191/191 [==============================] - 0s 12us/step - loss: 0.8313 - acc: 0.6021 - val_loss: 0.6745 - val_acc: 0.7353\n",
      "Epoch 24/500\n",
      "191/191 [==============================] - 0s 12us/step - loss: 0.8322 - acc: 0.5969 - val_loss: 0.6653 - val_acc: 0.6765\n",
      "Epoch 25/500\n",
      "191/191 [==============================] - 0s 9us/step - loss: 0.8417 - acc: 0.5707 - val_loss: 0.6679 - val_acc: 0.6765\n",
      "Epoch 26/500\n",
      "191/191 [==============================] - 0s 9us/step - loss: 0.8391 - acc: 0.5602 - val_loss: 0.6746 - val_acc: 0.6765\n",
      "Epoch 27/500\n",
      "191/191 [==============================] - 0s 12us/step - loss: 0.8183 - acc: 0.5916 - val_loss: 0.6916 - val_acc: 0.7059\n",
      "Epoch 28/500\n",
      "191/191 [==============================] - 0s 11us/step - loss: 0.7923 - acc: 0.6073 - val_loss: 0.7237 - val_acc: 0.7059\n",
      "Epoch 29/500\n",
      "191/191 [==============================] - 0s 9us/step - loss: 0.7768 - acc: 0.6230 - val_loss: 0.7611 - val_acc: 0.5588\n",
      "Epoch 30/500\n",
      "191/191 [==============================] - 0s 9us/step - loss: 0.7730 - acc: 0.6283 - val_loss: 0.7839 - val_acc: 0.5882\n",
      "Epoch 31/500\n",
      "191/191 [==============================] - 0s 9us/step - loss: 0.7689 - acc: 0.6387 - val_loss: 0.7789 - val_acc: 0.5882\n",
      "Epoch 32/500\n",
      "191/191 [==============================] - 0s 9us/step - loss: 0.7549 - acc: 0.6440 - val_loss: 0.7514 - val_acc: 0.6176\n",
      "Epoch 33/500\n",
      "191/191 [==============================] - 0s 11us/step - loss: 0.7338 - acc: 0.6545 - val_loss: 0.7187 - val_acc: 0.7059\n",
      "Epoch 34/500\n",
      "191/191 [==============================] - 0s 11us/step - loss: 0.7158 - acc: 0.6440 - val_loss: 0.6960 - val_acc: 0.6765\n",
      "Epoch 35/500\n",
      "191/191 [==============================] - 0s 10us/step - loss: 0.7072 - acc: 0.6230 - val_loss: 0.6874 - val_acc: 0.6765\n",
      "Epoch 36/500\n",
      "191/191 [==============================] - 0s 11us/step - loss: 0.7028 - acc: 0.6178 - val_loss: 0.6901 - val_acc: 0.7059\n",
      "Epoch 37/500\n",
      "191/191 [==============================] - 0s 10us/step - loss: 0.6933 - acc: 0.6230 - val_loss: 0.7028 - val_acc: 0.7059\n",
      "Epoch 38/500\n",
      "191/191 [==============================] - 0s 13us/step - loss: 0.6780 - acc: 0.6335 - val_loss: 0.7266 - val_acc: 0.6471\n",
      "Epoch 39/500\n",
      "191/191 [==============================] - 0s 12us/step - loss: 0.6641 - acc: 0.6702 - val_loss: 0.7568 - val_acc: 0.6471\n",
      "Epoch 40/500\n",
      "191/191 [==============================] - 0s 12us/step - loss: 0.6572 - acc: 0.7068 - val_loss: 0.7812 - val_acc: 0.6471\n",
      "Epoch 41/500\n",
      "191/191 [==============================] - 0s 10us/step - loss: 0.6536 - acc: 0.6963 - val_loss: 0.7882 - val_acc: 0.6471\n",
      "Epoch 42/500\n",
      "191/191 [==============================] - 0s 9us/step - loss: 0.6469 - acc: 0.6911 - val_loss: 0.7778 - val_acc: 0.6471\n",
      "Epoch 43/500\n",
      "191/191 [==============================] - 0s 14us/step - loss: 0.6368 - acc: 0.6911 - val_loss: 0.7602 - val_acc: 0.6471\n",
      "Epoch 44/500\n",
      "191/191 [==============================] - 0s 10us/step - loss: 0.6286 - acc: 0.7016 - val_loss: 0.7474 - val_acc: 0.6471\n",
      "Epoch 45/500\n",
      "191/191 [==============================] - 0s 9us/step - loss: 0.6254 - acc: 0.7016 - val_loss: 0.7446 - val_acc: 0.5882\n",
      "Epoch 46/500\n",
      "191/191 [==============================] - 0s 9us/step - loss: 0.6233 - acc: 0.6963 - val_loss: 0.7520 - val_acc: 0.5882\n",
      "Epoch 47/500\n",
      "191/191 [==============================] - 0s 9us/step - loss: 0.6183 - acc: 0.7016 - val_loss: 0.7689 - val_acc: 0.5882\n",
      "Epoch 48/500\n",
      "191/191 [==============================] - 0s 9us/step - loss: 0.6117 - acc: 0.6963 - val_loss: 0.7919 - val_acc: 0.6471\n",
      "Epoch 49/500\n",
      "191/191 [==============================] - 0s 11us/step - loss: 0.6075 - acc: 0.6859 - val_loss: 0.8129 - val_acc: 0.6176\n",
      "Epoch 50/500\n",
      "191/191 [==============================] - 0s 13us/step - loss: 0.6061 - acc: 0.6963 - val_loss: 0.8227 - val_acc: 0.6176\n",
      "Epoch 51/500\n",
      "191/191 [==============================] - 0s 11us/step - loss: 0.6042 - acc: 0.7016 - val_loss: 0.8182 - val_acc: 0.6176\n",
      "Epoch 52/500\n",
      "191/191 [==============================] - 0s 11us/step - loss: 0.6001 - acc: 0.6963 - val_loss: 0.8051 - val_acc: 0.5882\n",
      "Epoch 53/500\n",
      "191/191 [==============================] - 0s 9us/step - loss: 0.5964 - acc: 0.6806 - val_loss: 0.7931 - val_acc: 0.5882\n",
      "Epoch 54/500\n",
      "191/191 [==============================] - 0s 9us/step - loss: 0.5951 - acc: 0.6702 - val_loss: 0.7895 - val_acc: 0.6176\n",
      "Epoch 55/500\n",
      "191/191 [==============================] - 0s 10us/step - loss: 0.5942 - acc: 0.6702 - val_loss: 0.7952 - val_acc: 0.5882\n",
      "Epoch 56/500\n",
      "191/191 [==============================] - 0s 12us/step - loss: 0.5916 - acc: 0.6806 - val_loss: 0.8088 - val_acc: 0.5882\n",
      "Epoch 57/500\n",
      "191/191 [==============================] - 0s 15us/step - loss: 0.5884 - acc: 0.6754 - val_loss: 0.8252 - val_acc: 0.5588\n",
      "Epoch 58/500\n",
      "191/191 [==============================] - 0s 16us/step - loss: 0.5867 - acc: 0.6911 - val_loss: 0.8364 - val_acc: 0.5588\n",
      "Epoch 59/500\n",
      "191/191 [==============================] - 0s 12us/step - loss: 0.5854 - acc: 0.6911 - val_loss: 0.8375 - val_acc: 0.5588\n",
      "Epoch 60/500\n",
      "191/191 [==============================] - 0s 10us/step - loss: 0.5831 - acc: 0.6859 - val_loss: 0.8304 - val_acc: 0.5588\n",
      "Epoch 61/500\n",
      "191/191 [==============================] - 0s 13us/step - loss: 0.5801 - acc: 0.6859 - val_loss: 0.8219 - val_acc: 0.5294\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62/500\n",
      "191/191 [==============================] - 0s 14us/step - loss: 0.5779 - acc: 0.6806 - val_loss: 0.8178 - val_acc: 0.5882\n",
      "Epoch 63/500\n",
      "191/191 [==============================] - 0s 12us/step - loss: 0.5764 - acc: 0.6806 - val_loss: 0.8203 - val_acc: 0.5882\n",
      "Epoch 64/500\n",
      "191/191 [==============================] - 0s 25us/step - loss: 0.5743 - acc: 0.6806 - val_loss: 0.8283 - val_acc: 0.5294\n",
      "Epoch 65/500\n",
      "191/191 [==============================] - 0s 11us/step - loss: 0.5717 - acc: 0.6806 - val_loss: 0.8388 - val_acc: 0.5588\n",
      "Epoch 66/500\n",
      "191/191 [==============================] - 0s 11us/step - loss: 0.5697 - acc: 0.6911 - val_loss: 0.8468 - val_acc: 0.5588\n",
      "Epoch 67/500\n",
      "191/191 [==============================] - 0s 10us/step - loss: 0.5681 - acc: 0.7068 - val_loss: 0.8481 - val_acc: 0.5294\n",
      "Epoch 68/500\n",
      "191/191 [==============================] - 0s 11us/step - loss: 0.5662 - acc: 0.7068 - val_loss: 0.8430 - val_acc: 0.5294\n",
      "Epoch 69/500\n",
      "191/191 [==============================] - 0s 11us/step - loss: 0.5640 - acc: 0.7068 - val_loss: 0.8354 - val_acc: 0.5294\n",
      "Epoch 70/500\n",
      "191/191 [==============================] - 0s 11us/step - loss: 0.5620 - acc: 0.6963 - val_loss: 0.8297 - val_acc: 0.5294\n",
      "Epoch 71/500\n",
      "191/191 [==============================] - 0s 12us/step - loss: 0.5603 - acc: 0.7068 - val_loss: 0.8286 - val_acc: 0.5588\n",
      "Epoch 72/500\n",
      "191/191 [==============================] - 0s 12us/step - loss: 0.5585 - acc: 0.7120 - val_loss: 0.8318 - val_acc: 0.5294\n",
      "Epoch 73/500\n",
      "191/191 [==============================] - 0s 10us/step - loss: 0.5565 - acc: 0.7225 - val_loss: 0.8371 - val_acc: 0.5294\n",
      "Epoch 74/500\n",
      "191/191 [==============================] - 0s 9us/step - loss: 0.5546 - acc: 0.7120 - val_loss: 0.8408 - val_acc: 0.5294\n",
      "Epoch 75/500\n",
      "191/191 [==============================] - 0s 12us/step - loss: 0.5531 - acc: 0.7173 - val_loss: 0.8401 - val_acc: 0.5588\n",
      "Epoch 76/500\n",
      "191/191 [==============================] - 0s 11us/step - loss: 0.5514 - acc: 0.7225 - val_loss: 0.8348 - val_acc: 0.5294\n",
      "Epoch 77/500\n",
      "191/191 [==============================] - 0s 10us/step - loss: 0.5496 - acc: 0.7225 - val_loss: 0.8282 - val_acc: 0.5294\n",
      "Epoch 78/500\n",
      "191/191 [==============================] - 0s 9us/step - loss: 0.5479 - acc: 0.7330 - val_loss: 0.8238 - val_acc: 0.5588\n",
      "Epoch 79/500\n",
      "191/191 [==============================] - 0s 9us/step - loss: 0.5465 - acc: 0.7330 - val_loss: 0.8238 - val_acc: 0.5882\n",
      "Epoch 80/500\n",
      "191/191 [==============================] - 0s 13us/step - loss: 0.5449 - acc: 0.7330 - val_loss: 0.8279 - val_acc: 0.5588\n",
      "Epoch 81/500\n",
      "191/191 [==============================] - 0s 12us/step - loss: 0.5432 - acc: 0.7277 - val_loss: 0.8339 - val_acc: 0.5588\n",
      "Epoch 82/500\n",
      "191/191 [==============================] - 0s 14us/step - loss: 0.5417 - acc: 0.7330 - val_loss: 0.8390 - val_acc: 0.5588\n",
      "Epoch 83/500\n",
      "191/191 [==============================] - 0s 11us/step - loss: 0.5404 - acc: 0.7330 - val_loss: 0.8416 - val_acc: 0.5588\n",
      "Epoch 84/500\n",
      "191/191 [==============================] - 0s 11us/step - loss: 0.5389 - acc: 0.7330 - val_loss: 0.8420 - val_acc: 0.5588\n",
      "Epoch 85/500\n",
      "191/191 [==============================] - 0s 12us/step - loss: 0.5373 - acc: 0.7277 - val_loss: 0.8420 - val_acc: 0.5588\n",
      "Epoch 86/500\n",
      "191/191 [==============================] - 0s 13us/step - loss: 0.5357 - acc: 0.7277 - val_loss: 0.8438 - val_acc: 0.5588\n",
      "Epoch 87/500\n",
      "191/191 [==============================] - 0s 14us/step - loss: 0.5341 - acc: 0.7277 - val_loss: 0.8479 - val_acc: 0.5588\n",
      "Epoch 88/500\n",
      "191/191 [==============================] - 0s 11us/step - loss: 0.5325 - acc: 0.7330 - val_loss: 0.8542 - val_acc: 0.5588\n",
      "Epoch 89/500\n",
      "191/191 [==============================] - 0s 12us/step - loss: 0.5308 - acc: 0.7382 - val_loss: 0.8609 - val_acc: 0.5588\n",
      "Epoch 90/500\n",
      "191/191 [==============================] - 0s 12us/step - loss: 0.5293 - acc: 0.7382 - val_loss: 0.8655 - val_acc: 0.5588\n",
      "Epoch 91/500\n",
      "191/191 [==============================] - 0s 13us/step - loss: 0.5280 - acc: 0.7382 - val_loss: 0.8667 - val_acc: 0.5588\n",
      "Epoch 92/500\n",
      "191/191 [==============================] - 0s 11us/step - loss: 0.5269 - acc: 0.7487 - val_loss: 0.8649 - val_acc: 0.5588\n",
      "Epoch 93/500\n",
      "191/191 [==============================] - 0s 17us/step - loss: 0.5258 - acc: 0.7435 - val_loss: 0.8625 - val_acc: 0.5882\n",
      "Epoch 94/500\n",
      "191/191 [==============================] - 0s 12us/step - loss: 0.5247 - acc: 0.7487 - val_loss: 0.8615 - val_acc: 0.5588\n",
      "Epoch 95/500\n",
      "191/191 [==============================] - 0s 10us/step - loss: 0.5236 - acc: 0.7539 - val_loss: 0.8622 - val_acc: 0.5882\n",
      "Epoch 96/500\n",
      "191/191 [==============================] - 0s 13us/step - loss: 0.5224 - acc: 0.7487 - val_loss: 0.8635 - val_acc: 0.5882\n",
      "Epoch 97/500\n",
      "191/191 [==============================] - 0s 15us/step - loss: 0.5213 - acc: 0.7435 - val_loss: 0.8639 - val_acc: 0.5882\n",
      "Epoch 98/500\n",
      "191/191 [==============================] - 0s 19us/step - loss: 0.5202 - acc: 0.7435 - val_loss: 0.8625 - val_acc: 0.5882\n",
      "Epoch 99/500\n",
      "191/191 [==============================] - 0s 9us/step - loss: 0.5190 - acc: 0.7435 - val_loss: 0.8598 - val_acc: 0.5882\n",
      "Epoch 100/500\n",
      "191/191 [==============================] - 0s 9us/step - loss: 0.5179 - acc: 0.7435 - val_loss: 0.8573 - val_acc: 0.5882\n",
      "Epoch 101/500\n",
      "191/191 [==============================] - 0s 12us/step - loss: 0.5167 - acc: 0.7382 - val_loss: 0.8565 - val_acc: 0.5882\n",
      "Epoch 102/500\n",
      "191/191 [==============================] - 0s 13us/step - loss: 0.5156 - acc: 0.7382 - val_loss: 0.8577 - val_acc: 0.5882\n",
      "Epoch 103/500\n",
      "191/191 [==============================] - 0s 11us/step - loss: 0.5144 - acc: 0.7382 - val_loss: 0.8602 - val_acc: 0.5882\n",
      "Epoch 104/500\n",
      "191/191 [==============================] - 0s 12us/step - loss: 0.5133 - acc: 0.7382 - val_loss: 0.8629 - val_acc: 0.5882\n",
      "Epoch 105/500\n",
      "191/191 [==============================] - 0s 12us/step - loss: 0.5122 - acc: 0.7435 - val_loss: 0.8656 - val_acc: 0.5882\n",
      "Epoch 106/500\n",
      "191/191 [==============================] - 0s 14us/step - loss: 0.5110 - acc: 0.7487 - val_loss: 0.8682 - val_acc: 0.5882\n",
      "Epoch 107/500\n",
      "191/191 [==============================] - 0s 11us/step - loss: 0.5099 - acc: 0.7487 - val_loss: 0.8717 - val_acc: 0.5882\n",
      "Epoch 108/500\n",
      "191/191 [==============================] - 0s 13us/step - loss: 0.5087 - acc: 0.7487 - val_loss: 0.8768 - val_acc: 0.5882\n",
      "Epoch 109/500\n",
      "191/191 [==============================] - 0s 12us/step - loss: 0.5075 - acc: 0.7487 - val_loss: 0.8838 - val_acc: 0.5882\n",
      "Epoch 110/500\n",
      "191/191 [==============================] - 0s 9us/step - loss: 0.5063 - acc: 0.7487 - val_loss: 0.8911 - val_acc: 0.5882\n",
      "Epoch 111/500\n",
      "191/191 [==============================] - 0s 9us/step - loss: 0.5050 - acc: 0.7487 - val_loss: 0.8979 - val_acc: 0.5882\n",
      "Epoch 112/500\n",
      "191/191 [==============================] - 0s 9us/step - loss: 0.5033 - acc: 0.7487 - val_loss: 0.9041 - val_acc: 0.5882\n",
      "Epoch 113/500\n",
      "191/191 [==============================] - 0s 15us/step - loss: 0.5014 - acc: 0.7487 - val_loss: 0.9106 - val_acc: 0.5882\n",
      "Epoch 114/500\n",
      "191/191 [==============================] - 0s 12us/step - loss: 0.4993 - acc: 0.7487 - val_loss: 0.9164 - val_acc: 0.5882\n",
      "Epoch 115/500\n",
      "191/191 [==============================] - 0s 11us/step - loss: 0.4973 - acc: 0.7435 - val_loss: 0.9212 - val_acc: 0.5882\n",
      "Epoch 116/500\n",
      "191/191 [==============================] - 0s 11us/step - loss: 0.4959 - acc: 0.7382 - val_loss: 0.9233 - val_acc: 0.5882\n",
      "Epoch 117/500\n",
      "191/191 [==============================] - 0s 10us/step - loss: 0.4952 - acc: 0.7382 - val_loss: 0.9185 - val_acc: 0.6176\n",
      "Epoch 118/500\n",
      "191/191 [==============================] - 0s 11us/step - loss: 0.4943 - acc: 0.7487 - val_loss: 0.9108 - val_acc: 0.5882\n",
      "Epoch 119/500\n",
      "191/191 [==============================] - 0s 10us/step - loss: 0.4930 - acc: 0.7487 - val_loss: 0.9057 - val_acc: 0.6176\n",
      "Epoch 120/500\n",
      "191/191 [==============================] - 0s 15us/step - loss: 0.4915 - acc: 0.7487 - val_loss: 0.9054 - val_acc: 0.5882\n",
      "Epoch 121/500\n",
      "191/191 [==============================] - 0s 10us/step - loss: 0.4900 - acc: 0.7330 - val_loss: 0.9073 - val_acc: 0.5882\n",
      "Epoch 122/500\n",
      "191/191 [==============================] - 0s 11us/step - loss: 0.4888 - acc: 0.7435 - val_loss: 0.9080 - val_acc: 0.5882\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 123/500\n",
      "191/191 [==============================] - 0s 12us/step - loss: 0.4880 - acc: 0.7487 - val_loss: 0.9061 - val_acc: 0.5882\n",
      "Epoch 124/500\n",
      "191/191 [==============================] - 0s 13us/step - loss: 0.4871 - acc: 0.7487 - val_loss: 0.9031 - val_acc: 0.5882\n",
      "Epoch 125/500\n",
      "191/191 [==============================] - 0s 12us/step - loss: 0.4859 - acc: 0.7435 - val_loss: 0.9024 - val_acc: 0.5882\n",
      "Epoch 126/500\n",
      "191/191 [==============================] - 0s 12us/step - loss: 0.4847 - acc: 0.7382 - val_loss: 0.9057 - val_acc: 0.6176\n",
      "Epoch 127/500\n",
      "191/191 [==============================] - 0s 14us/step - loss: 0.4833 - acc: 0.7330 - val_loss: 0.9120 - val_acc: 0.6176\n",
      "Epoch 128/500\n",
      "191/191 [==============================] - 0s 11us/step - loss: 0.4822 - acc: 0.7382 - val_loss: 0.9173 - val_acc: 0.6176\n",
      "Epoch 129/500\n",
      "191/191 [==============================] - 0s 10us/step - loss: 0.4813 - acc: 0.7382 - val_loss: 0.9183 - val_acc: 0.6176\n",
      "Epoch 130/500\n",
      "191/191 [==============================] - 0s 12us/step - loss: 0.4803 - acc: 0.7382 - val_loss: 0.9151 - val_acc: 0.6471\n",
      "Epoch 131/500\n",
      "191/191 [==============================] - 0s 11us/step - loss: 0.4792 - acc: 0.7382 - val_loss: 0.9109 - val_acc: 0.6471\n",
      "Epoch 132/500\n",
      "191/191 [==============================] - 0s 11us/step - loss: 0.4781 - acc: 0.7435 - val_loss: 0.9089 - val_acc: 0.6471\n",
      "Epoch 133/500\n",
      "191/191 [==============================] - 0s 11us/step - loss: 0.4769 - acc: 0.7435 - val_loss: 0.9096 - val_acc: 0.6176\n",
      "Epoch 134/500\n",
      "191/191 [==============================] - 0s 10us/step - loss: 0.4759 - acc: 0.7487 - val_loss: 0.9113 - val_acc: 0.5882\n",
      "Epoch 135/500\n",
      "191/191 [==============================] - 0s 10us/step - loss: 0.4749 - acc: 0.7487 - val_loss: 0.9114 - val_acc: 0.5882\n",
      "Epoch 136/500\n",
      "191/191 [==============================] - 0s 10us/step - loss: 0.4740 - acc: 0.7592 - val_loss: 0.9095 - val_acc: 0.5882\n",
      "Epoch 137/500\n",
      "191/191 [==============================] - 0s 10us/step - loss: 0.4729 - acc: 0.7592 - val_loss: 0.9075 - val_acc: 0.5882\n",
      "Epoch 138/500\n",
      "191/191 [==============================] - 0s 13us/step - loss: 0.4719 - acc: 0.7592 - val_loss: 0.9075 - val_acc: 0.5882\n",
      "Epoch 139/500\n",
      "191/191 [==============================] - 0s 9us/step - loss: 0.4709 - acc: 0.7592 - val_loss: 0.9102 - val_acc: 0.5882\n",
      "Epoch 140/500\n",
      "191/191 [==============================] - 0s 11us/step - loss: 0.4698 - acc: 0.7644 - val_loss: 0.9135 - val_acc: 0.6176\n",
      "Epoch 141/500\n",
      "191/191 [==============================] - 0s 11us/step - loss: 0.4689 - acc: 0.7644 - val_loss: 0.9150 - val_acc: 0.6176\n",
      "Epoch 142/500\n",
      "191/191 [==============================] - 0s 9us/step - loss: 0.4679 - acc: 0.7696 - val_loss: 0.9136 - val_acc: 0.6176\n",
      "Epoch 143/500\n",
      "191/191 [==============================] - 0s 10us/step - loss: 0.4669 - acc: 0.7696 - val_loss: 0.9106 - val_acc: 0.6176\n",
      "Epoch 144/500\n",
      "191/191 [==============================] - 0s 10us/step - loss: 0.4660 - acc: 0.7696 - val_loss: 0.9082 - val_acc: 0.6176\n",
      "Epoch 145/500\n",
      "191/191 [==============================] - 0s 11us/step - loss: 0.4650 - acc: 0.7749 - val_loss: 0.9075 - val_acc: 0.5882\n",
      "Epoch 146/500\n",
      "191/191 [==============================] - 0s 12us/step - loss: 0.4640 - acc: 0.7801 - val_loss: 0.9082 - val_acc: 0.6176\n",
      "Epoch 147/500\n",
      "191/191 [==============================] - 0s 14us/step - loss: 0.4631 - acc: 0.7801 - val_loss: 0.9087 - val_acc: 0.6176\n",
      "Epoch 148/500\n",
      "191/191 [==============================] - 0s 12us/step - loss: 0.4621 - acc: 0.7853 - val_loss: 0.9080 - val_acc: 0.6176\n",
      "Epoch 149/500\n",
      "191/191 [==============================] - 0s 13us/step - loss: 0.4612 - acc: 0.7853 - val_loss: 0.9070 - val_acc: 0.6176\n",
      "Epoch 150/500\n",
      "191/191 [==============================] - 0s 13us/step - loss: 0.4602 - acc: 0.7906 - val_loss: 0.9069 - val_acc: 0.6176\n",
      "Epoch 151/500\n",
      "191/191 [==============================] - 0s 12us/step - loss: 0.4593 - acc: 0.7906 - val_loss: 0.9084 - val_acc: 0.6176\n",
      "Epoch 152/500\n",
      "191/191 [==============================] - 0s 12us/step - loss: 0.4583 - acc: 0.7906 - val_loss: 0.9108 - val_acc: 0.6176\n",
      "Epoch 153/500\n",
      "191/191 [==============================] - 0s 10us/step - loss: 0.4574 - acc: 0.7906 - val_loss: 0.9124 - val_acc: 0.6176\n",
      "Epoch 154/500\n",
      "191/191 [==============================] - 0s 9us/step - loss: 0.4565 - acc: 0.7906 - val_loss: 0.9124 - val_acc: 0.6176\n",
      "Epoch 155/500\n",
      "191/191 [==============================] - 0s 10us/step - loss: 0.4556 - acc: 0.7906 - val_loss: 0.9111 - val_acc: 0.6176\n",
      "Epoch 156/500\n",
      "191/191 [==============================] - 0s 10us/step - loss: 0.4546 - acc: 0.7853 - val_loss: 0.9103 - val_acc: 0.6176\n",
      "Epoch 157/500\n",
      "191/191 [==============================] - 0s 12us/step - loss: 0.4537 - acc: 0.7853 - val_loss: 0.9107 - val_acc: 0.6176\n",
      "Epoch 158/500\n",
      "191/191 [==============================] - 0s 11us/step - loss: 0.4528 - acc: 0.7906 - val_loss: 0.9120 - val_acc: 0.6176\n",
      "Epoch 159/500\n",
      "191/191 [==============================] - 0s 10us/step - loss: 0.4519 - acc: 0.7906 - val_loss: 0.9131 - val_acc: 0.6176\n",
      "Epoch 160/500\n",
      "191/191 [==============================] - 0s 12us/step - loss: 0.4510 - acc: 0.7906 - val_loss: 0.9133 - val_acc: 0.6176\n",
      "Epoch 161/500\n",
      "191/191 [==============================] - 0s 11us/step - loss: 0.4501 - acc: 0.7906 - val_loss: 0.9128 - val_acc: 0.6176\n",
      "Epoch 162/500\n",
      "191/191 [==============================] - 0s 14us/step - loss: 0.4492 - acc: 0.7906 - val_loss: 0.9126 - val_acc: 0.6176\n",
      "Epoch 163/500\n",
      "191/191 [==============================] - 0s 15us/step - loss: 0.4483 - acc: 0.7906 - val_loss: 0.9131 - val_acc: 0.6176\n",
      "Epoch 164/500\n",
      "191/191 [==============================] - 0s 12us/step - loss: 0.4474 - acc: 0.7906 - val_loss: 0.9138 - val_acc: 0.6176\n",
      "Epoch 165/500\n",
      "191/191 [==============================] - 0s 11us/step - loss: 0.4465 - acc: 0.7958 - val_loss: 0.9139 - val_acc: 0.6176\n",
      "Epoch 166/500\n",
      "191/191 [==============================] - 0s 12us/step - loss: 0.4456 - acc: 0.7958 - val_loss: 0.9131 - val_acc: 0.6176\n",
      "Epoch 167/500\n",
      "191/191 [==============================] - 0s 11us/step - loss: 0.4447 - acc: 0.7958 - val_loss: 0.9121 - val_acc: 0.6176\n",
      "Epoch 168/500\n",
      "191/191 [==============================] - 0s 15us/step - loss: 0.4438 - acc: 0.7958 - val_loss: 0.9117 - val_acc: 0.6176\n",
      "Epoch 169/500\n",
      "191/191 [==============================] - 0s 17us/step - loss: 0.4430 - acc: 0.7958 - val_loss: 0.9121 - val_acc: 0.6176\n",
      "Epoch 170/500\n",
      "191/191 [==============================] - 0s 10us/step - loss: 0.4421 - acc: 0.8010 - val_loss: 0.9128 - val_acc: 0.6176\n",
      "Epoch 171/500\n",
      "191/191 [==============================] - 0s 11us/step - loss: 0.4412 - acc: 0.8010 - val_loss: 0.9132 - val_acc: 0.6176\n",
      "Epoch 172/500\n",
      "191/191 [==============================] - 0s 13us/step - loss: 0.4403 - acc: 0.8010 - val_loss: 0.9131 - val_acc: 0.6176\n",
      "Epoch 173/500\n",
      "191/191 [==============================] - 0s 15us/step - loss: 0.4394 - acc: 0.7958 - val_loss: 0.9129 - val_acc: 0.6176\n",
      "Epoch 174/500\n",
      "191/191 [==============================] - 0s 12us/step - loss: 0.4386 - acc: 0.8010 - val_loss: 0.9130 - val_acc: 0.6176\n",
      "Epoch 175/500\n",
      "191/191 [==============================] - 0s 10us/step - loss: 0.4377 - acc: 0.8063 - val_loss: 0.9134 - val_acc: 0.6176\n",
      "Epoch 176/500\n",
      "191/191 [==============================] - 0s 11us/step - loss: 0.4368 - acc: 0.8063 - val_loss: 0.9137 - val_acc: 0.6176\n",
      "Epoch 177/500\n",
      "191/191 [==============================] - 0s 18us/step - loss: 0.4360 - acc: 0.8063 - val_loss: 0.9135 - val_acc: 0.6176\n",
      "Epoch 178/500\n",
      "191/191 [==============================] - 0s 15us/step - loss: 0.4351 - acc: 0.8063 - val_loss: 0.9131 - val_acc: 0.6176\n",
      "Epoch 179/500\n",
      "191/191 [==============================] - 0s 14us/step - loss: 0.4342 - acc: 0.8063 - val_loss: 0.9129 - val_acc: 0.6176\n",
      "Epoch 180/500\n",
      "191/191 [==============================] - 0s 11us/step - loss: 0.4334 - acc: 0.8063 - val_loss: 0.9131 - val_acc: 0.6176\n",
      "Epoch 181/500\n",
      "191/191 [==============================] - 0s 11us/step - loss: 0.4325 - acc: 0.8115 - val_loss: 0.9135 - val_acc: 0.6176\n",
      "Epoch 182/500\n",
      "191/191 [==============================] - 0s 15us/step - loss: 0.4317 - acc: 0.8115 - val_loss: 0.9137 - val_acc: 0.6176\n",
      "Epoch 183/500\n",
      "191/191 [==============================] - 0s 14us/step - loss: 0.4308 - acc: 0.8115 - val_loss: 0.9135 - val_acc: 0.6176\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 184/500\n",
      "191/191 [==============================] - 0s 15us/step - loss: 0.4300 - acc: 0.8115 - val_loss: 0.9131 - val_acc: 0.6176\n",
      "Epoch 185/500\n",
      "191/191 [==============================] - 0s 10us/step - loss: 0.4291 - acc: 0.8168 - val_loss: 0.9130 - val_acc: 0.6176\n",
      "Epoch 186/500\n",
      "191/191 [==============================] - 0s 10us/step - loss: 0.4283 - acc: 0.8220 - val_loss: 0.9132 - val_acc: 0.6176\n",
      "Epoch 187/500\n",
      "191/191 [==============================] - 0s 14us/step - loss: 0.4274 - acc: 0.8220 - val_loss: 0.9134 - val_acc: 0.6176\n",
      "Epoch 188/500\n",
      "191/191 [==============================] - 0s 15us/step - loss: 0.4266 - acc: 0.8220 - val_loss: 0.9133 - val_acc: 0.6176\n",
      "Epoch 189/500\n",
      "191/191 [==============================] - 0s 14us/step - loss: 0.4258 - acc: 0.8220 - val_loss: 0.9130 - val_acc: 0.6176\n",
      "Epoch 190/500\n",
      "191/191 [==============================] - 0s 11us/step - loss: 0.4250 - acc: 0.8220 - val_loss: 0.9128 - val_acc: 0.6176\n",
      "Epoch 191/500\n",
      "191/191 [==============================] - 0s 10us/step - loss: 0.4241 - acc: 0.8220 - val_loss: 0.9130 - val_acc: 0.6176\n",
      "Epoch 192/500\n",
      "191/191 [==============================] - 0s 12us/step - loss: 0.4233 - acc: 0.8220 - val_loss: 0.9133 - val_acc: 0.6176\n",
      "Epoch 193/500\n",
      "191/191 [==============================] - 0s 18us/step - loss: 0.4225 - acc: 0.8220 - val_loss: 0.9136 - val_acc: 0.6176\n",
      "Epoch 194/500\n",
      "191/191 [==============================] - 0s 12us/step - loss: 0.4217 - acc: 0.8220 - val_loss: 0.9135 - val_acc: 0.6176\n",
      "Epoch 195/500\n",
      "191/191 [==============================] - 0s 15us/step - loss: 0.4209 - acc: 0.8220 - val_loss: 0.9134 - val_acc: 0.6176\n",
      "Epoch 196/500\n",
      "191/191 [==============================] - 0s 13us/step - loss: 0.4200 - acc: 0.8168 - val_loss: 0.9134 - val_acc: 0.6176\n",
      "Epoch 197/500\n",
      "191/191 [==============================] - 0s 11us/step - loss: 0.4192 - acc: 0.8168 - val_loss: 0.9134 - val_acc: 0.6176\n",
      "Epoch 198/500\n",
      "191/191 [==============================] - 0s 14us/step - loss: 0.4184 - acc: 0.8168 - val_loss: 0.9134 - val_acc: 0.6176\n",
      "Epoch 199/500\n",
      "191/191 [==============================] - 0s 14us/step - loss: 0.4176 - acc: 0.8168 - val_loss: 0.9136 - val_acc: 0.6176\n",
      "Epoch 200/500\n",
      "191/191 [==============================] - 0s 12us/step - loss: 0.4168 - acc: 0.8115 - val_loss: 0.9140 - val_acc: 0.6176\n",
      "Epoch 201/500\n",
      "191/191 [==============================] - 0s 12us/step - loss: 0.4160 - acc: 0.8115 - val_loss: 0.9142 - val_acc: 0.6176\n",
      "Epoch 202/500\n",
      "191/191 [==============================] - 0s 17us/step - loss: 0.4152 - acc: 0.8168 - val_loss: 0.9140 - val_acc: 0.6176\n",
      "Epoch 203/500\n",
      "191/191 [==============================] - 0s 16us/step - loss: 0.4144 - acc: 0.8168 - val_loss: 0.9138 - val_acc: 0.6176\n",
      "Epoch 204/500\n",
      "191/191 [==============================] - 0s 13us/step - loss: 0.4136 - acc: 0.8168 - val_loss: 0.9140 - val_acc: 0.6176\n",
      "Epoch 205/500\n",
      "191/191 [==============================] - 0s 15us/step - loss: 0.4128 - acc: 0.8168 - val_loss: 0.9145 - val_acc: 0.6176\n",
      "Epoch 206/500\n",
      "191/191 [==============================] - 0s 11us/step - loss: 0.4120 - acc: 0.8168 - val_loss: 0.9147 - val_acc: 0.6176\n",
      "Epoch 207/500\n",
      "191/191 [==============================] - 0s 12us/step - loss: 0.4112 - acc: 0.8168 - val_loss: 0.9143 - val_acc: 0.6176\n",
      "Epoch 208/500\n",
      "191/191 [==============================] - 0s 15us/step - loss: 0.4105 - acc: 0.8168 - val_loss: 0.9140 - val_acc: 0.6176\n",
      "Epoch 209/500\n",
      "191/191 [==============================] - 0s 14us/step - loss: 0.4097 - acc: 0.8220 - val_loss: 0.9142 - val_acc: 0.6176\n",
      "Epoch 210/500\n",
      "191/191 [==============================] - 0s 15us/step - loss: 0.4089 - acc: 0.8220 - val_loss: 0.9146 - val_acc: 0.6176\n",
      "Epoch 211/500\n",
      "191/191 [==============================] - 0s 10us/step - loss: 0.4081 - acc: 0.8220 - val_loss: 0.9148 - val_acc: 0.6176\n",
      "Epoch 212/500\n",
      "191/191 [==============================] - 0s 14us/step - loss: 0.4074 - acc: 0.8220 - val_loss: 0.9145 - val_acc: 0.6176\n",
      "Epoch 213/500\n",
      "191/191 [==============================] - 0s 15us/step - loss: 0.4066 - acc: 0.8220 - val_loss: 0.9141 - val_acc: 0.6176\n",
      "Epoch 214/500\n",
      "191/191 [==============================] - 0s 13us/step - loss: 0.4058 - acc: 0.8220 - val_loss: 0.9140 - val_acc: 0.6176\n",
      "Epoch 215/500\n",
      "191/191 [==============================] - 0s 14us/step - loss: 0.4051 - acc: 0.8220 - val_loss: 0.9142 - val_acc: 0.6176\n",
      "Epoch 216/500\n",
      "191/191 [==============================] - 0s 12us/step - loss: 0.4043 - acc: 0.8220 - val_loss: 0.9143 - val_acc: 0.6176\n",
      "Epoch 217/500\n",
      "191/191 [==============================] - 0s 14us/step - loss: 0.4035 - acc: 0.8220 - val_loss: 0.9141 - val_acc: 0.6176\n",
      "Epoch 218/500\n",
      "191/191 [==============================] - 0s 15us/step - loss: 0.4028 - acc: 0.8220 - val_loss: 0.9137 - val_acc: 0.6176\n",
      "Epoch 219/500\n",
      "191/191 [==============================] - 0s 13us/step - loss: 0.4020 - acc: 0.8220 - val_loss: 0.9136 - val_acc: 0.6176\n",
      "Epoch 220/500\n",
      "191/191 [==============================] - 0s 11us/step - loss: 0.4013 - acc: 0.8220 - val_loss: 0.9137 - val_acc: 0.6176\n",
      "Epoch 221/500\n",
      "191/191 [==============================] - 0s 10us/step - loss: 0.4005 - acc: 0.8220 - val_loss: 0.9138 - val_acc: 0.6176\n",
      "Epoch 222/500\n",
      "191/191 [==============================] - 0s 10us/step - loss: 0.3998 - acc: 0.8220 - val_loss: 0.9136 - val_acc: 0.6176\n",
      "Epoch 223/500\n",
      "191/191 [==============================] - 0s 12us/step - loss: 0.3990 - acc: 0.8220 - val_loss: 0.9133 - val_acc: 0.6176\n",
      "Epoch 224/500\n",
      "191/191 [==============================] - 0s 10us/step - loss: 0.3983 - acc: 0.8220 - val_loss: 0.9132 - val_acc: 0.6176\n",
      "Epoch 225/500\n",
      "191/191 [==============================] - 0s 9us/step - loss: 0.3975 - acc: 0.8220 - val_loss: 0.9133 - val_acc: 0.6176\n",
      "Epoch 226/500\n",
      "191/191 [==============================] - 0s 10us/step - loss: 0.3968 - acc: 0.8220 - val_loss: 0.9132 - val_acc: 0.6176\n",
      "Epoch 227/500\n",
      "191/191 [==============================] - 0s 10us/step - loss: 0.3960 - acc: 0.8220 - val_loss: 0.9130 - val_acc: 0.6176\n",
      "Epoch 228/500\n",
      "191/191 [==============================] - 0s 12us/step - loss: 0.3953 - acc: 0.8220 - val_loss: 0.9128 - val_acc: 0.6176\n",
      "Epoch 229/500\n",
      "191/191 [==============================] - 0s 12us/step - loss: 0.3945 - acc: 0.8220 - val_loss: 0.9129 - val_acc: 0.6176\n",
      "Epoch 230/500\n",
      "191/191 [==============================] - 0s 12us/step - loss: 0.3938 - acc: 0.8272 - val_loss: 0.9130 - val_acc: 0.6176\n",
      "Epoch 231/500\n",
      "191/191 [==============================] - 0s 11us/step - loss: 0.3931 - acc: 0.8325 - val_loss: 0.9128 - val_acc: 0.6176\n",
      "Epoch 232/500\n",
      "191/191 [==============================] - 0s 10us/step - loss: 0.3923 - acc: 0.8325 - val_loss: 0.9125 - val_acc: 0.6176\n",
      "Epoch 233/500\n",
      "191/191 [==============================] - 0s 11us/step - loss: 0.3916 - acc: 0.8325 - val_loss: 0.9124 - val_acc: 0.6176\n",
      "Epoch 234/500\n",
      "191/191 [==============================] - 0s 13us/step - loss: 0.3909 - acc: 0.8325 - val_loss: 0.9126 - val_acc: 0.6176\n",
      "Epoch 235/500\n",
      "191/191 [==============================] - 0s 12us/step - loss: 0.3901 - acc: 0.8325 - val_loss: 0.9128 - val_acc: 0.6176\n",
      "Epoch 236/500\n",
      "191/191 [==============================] - 0s 9us/step - loss: 0.3894 - acc: 0.8272 - val_loss: 0.9127 - val_acc: 0.6176\n",
      "Epoch 237/500\n",
      "191/191 [==============================] - 0s 10us/step - loss: 0.3887 - acc: 0.8325 - val_loss: 0.9126 - val_acc: 0.6176\n",
      "Epoch 238/500\n",
      "191/191 [==============================] - 0s 10us/step - loss: 0.3879 - acc: 0.8325 - val_loss: 0.9126 - val_acc: 0.6176\n",
      "Epoch 239/500\n",
      "191/191 [==============================] - 0s 10us/step - loss: 0.3872 - acc: 0.8325 - val_loss: 0.9125 - val_acc: 0.6176\n",
      "Epoch 240/500\n",
      "191/191 [==============================] - 0s 13us/step - loss: 0.3865 - acc: 0.8325 - val_loss: 0.9124 - val_acc: 0.6176\n",
      "Epoch 241/500\n",
      "191/191 [==============================] - 0s 12us/step - loss: 0.3857 - acc: 0.8325 - val_loss: 0.9125 - val_acc: 0.6176\n",
      "Epoch 242/500\n",
      "191/191 [==============================] - 0s 10us/step - loss: 0.3850 - acc: 0.8377 - val_loss: 0.9125 - val_acc: 0.6176\n",
      "Epoch 243/500\n",
      "191/191 [==============================] - 0s 14us/step - loss: 0.3843 - acc: 0.8377 - val_loss: 0.9124 - val_acc: 0.6176\n",
      "Epoch 244/500\n",
      "191/191 [==============================] - 0s 10us/step - loss: 0.3836 - acc: 0.8377 - val_loss: 0.9122 - val_acc: 0.6176\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 245/500\n",
      "191/191 [==============================] - 0s 11us/step - loss: 0.3829 - acc: 0.8377 - val_loss: 0.9119 - val_acc: 0.6471\n",
      "Epoch 246/500\n",
      "191/191 [==============================] - 0s 10us/step - loss: 0.3822 - acc: 0.8325 - val_loss: 0.9120 - val_acc: 0.6471\n",
      "Epoch 247/500\n",
      "191/191 [==============================] - 0s 13us/step - loss: 0.3815 - acc: 0.8325 - val_loss: 0.9118 - val_acc: 0.6471\n",
      "Epoch 248/500\n",
      "191/191 [==============================] - 0s 11us/step - loss: 0.3808 - acc: 0.8325 - val_loss: 0.9117 - val_acc: 0.6471\n",
      "Epoch 249/500\n",
      "191/191 [==============================] - 0s 10us/step - loss: 0.3801 - acc: 0.8325 - val_loss: 0.9116 - val_acc: 0.6471\n",
      "Epoch 250/500\n",
      "191/191 [==============================] - 0s 10us/step - loss: 0.3794 - acc: 0.8325 - val_loss: 0.9113 - val_acc: 0.6471\n",
      "Epoch 251/500\n",
      "191/191 [==============================] - 0s 15us/step - loss: 0.3787 - acc: 0.8325 - val_loss: 0.9113 - val_acc: 0.6471\n",
      "Epoch 252/500\n",
      "191/191 [==============================] - 0s 11us/step - loss: 0.3780 - acc: 0.8325 - val_loss: 0.9114 - val_acc: 0.6471\n",
      "Epoch 253/500\n",
      "191/191 [==============================] - 0s 10us/step - loss: 0.3773 - acc: 0.8325 - val_loss: 0.9111 - val_acc: 0.6471\n",
      "Epoch 254/500\n",
      "191/191 [==============================] - 0s 10us/step - loss: 0.3766 - acc: 0.8325 - val_loss: 0.9110 - val_acc: 0.6471\n",
      "Epoch 255/500\n",
      "191/191 [==============================] - 0s 10us/step - loss: 0.3759 - acc: 0.8377 - val_loss: 0.9110 - val_acc: 0.6471\n",
      "Epoch 256/500\n",
      "191/191 [==============================] - 0s 10us/step - loss: 0.3753 - acc: 0.8377 - val_loss: 0.9108 - val_acc: 0.6765\n",
      "Epoch 257/500\n",
      "191/191 [==============================] - 0s 11us/step - loss: 0.3746 - acc: 0.8377 - val_loss: 0.9105 - val_acc: 0.6765\n",
      "Epoch 258/500\n",
      "191/191 [==============================] - 0s 13us/step - loss: 0.3739 - acc: 0.8377 - val_loss: 0.9105 - val_acc: 0.6765\n",
      "Epoch 259/500\n",
      "191/191 [==============================] - 0s 11us/step - loss: 0.3733 - acc: 0.8377 - val_loss: 0.9104 - val_acc: 0.6765\n",
      "Epoch 260/500\n",
      "191/191 [==============================] - 0s 14us/step - loss: 0.3726 - acc: 0.8377 - val_loss: 0.9101 - val_acc: 0.6765\n",
      "Epoch 261/500\n",
      "191/191 [==============================] - 0s 12us/step - loss: 0.3720 - acc: 0.8377 - val_loss: 0.9101 - val_acc: 0.6765\n",
      "Epoch 262/500\n",
      "191/191 [==============================] - 0s 13us/step - loss: 0.3713 - acc: 0.8377 - val_loss: 0.9101 - val_acc: 0.6765\n",
      "Epoch 263/500\n",
      "191/191 [==============================] - 0s 11us/step - loss: 0.3707 - acc: 0.8377 - val_loss: 0.9097 - val_acc: 0.6765\n",
      "Epoch 264/500\n",
      "191/191 [==============================] - 0s 12us/step - loss: 0.3700 - acc: 0.8377 - val_loss: 0.9095 - val_acc: 0.6765\n",
      "Epoch 265/500\n",
      "191/191 [==============================] - 0s 10us/step - loss: 0.3694 - acc: 0.8377 - val_loss: 0.9096 - val_acc: 0.6765\n",
      "Epoch 266/500\n",
      "191/191 [==============================] - 0s 10us/step - loss: 0.3687 - acc: 0.8377 - val_loss: 0.9095 - val_acc: 0.6765\n",
      "Epoch 267/500\n",
      "191/191 [==============================] - 0s 15us/step - loss: 0.3681 - acc: 0.8377 - val_loss: 0.9090 - val_acc: 0.6765\n",
      "Epoch 268/500\n",
      "191/191 [==============================] - 0s 14us/step - loss: 0.3674 - acc: 0.8377 - val_loss: 0.9087 - val_acc: 0.6765\n",
      "Epoch 269/500\n",
      "191/191 [==============================] - 0s 11us/step - loss: 0.3668 - acc: 0.8377 - val_loss: 0.9089 - val_acc: 0.6765\n",
      "Epoch 270/500\n",
      "191/191 [==============================] - 0s 16us/step - loss: 0.3662 - acc: 0.8377 - val_loss: 0.9089 - val_acc: 0.6765\n",
      "Epoch 271/500\n",
      "191/191 [==============================] - 0s 12us/step - loss: 0.3656 - acc: 0.8377 - val_loss: 0.9087 - val_acc: 0.6765\n",
      "Epoch 272/500\n",
      "191/191 [==============================] - 0s 11us/step - loss: 0.3650 - acc: 0.8377 - val_loss: 0.9085 - val_acc: 0.6765\n",
      "Epoch 273/500\n",
      "191/191 [==============================] - 0s 14us/step - loss: 0.3643 - acc: 0.8377 - val_loss: 0.9085 - val_acc: 0.6765\n",
      "Epoch 274/500\n",
      "191/191 [==============================] - 0s 13us/step - loss: 0.3637 - acc: 0.8429 - val_loss: 0.9087 - val_acc: 0.6765\n",
      "Epoch 275/500\n",
      "191/191 [==============================] - 0s 14us/step - loss: 0.3631 - acc: 0.8429 - val_loss: 0.9086 - val_acc: 0.6765\n",
      "Epoch 276/500\n",
      "191/191 [==============================] - 0s 15us/step - loss: 0.3625 - acc: 0.8429 - val_loss: 0.9085 - val_acc: 0.6765\n",
      "Epoch 277/500\n",
      "191/191 [==============================] - 0s 11us/step - loss: 0.3619 - acc: 0.8429 - val_loss: 0.9084 - val_acc: 0.6765\n",
      "Epoch 278/500\n",
      "191/191 [==============================] - 0s 15us/step - loss: 0.3613 - acc: 0.8429 - val_loss: 0.9084 - val_acc: 0.6765\n",
      "Epoch 279/500\n",
      "191/191 [==============================] - 0s 17us/step - loss: 0.3607 - acc: 0.8429 - val_loss: 0.9084 - val_acc: 0.6765\n",
      "Epoch 280/500\n",
      "191/191 [==============================] - 0s 14us/step - loss: 0.3601 - acc: 0.8429 - val_loss: 0.9083 - val_acc: 0.6765\n",
      "Epoch 281/500\n",
      "191/191 [==============================] - 0s 12us/step - loss: 0.3595 - acc: 0.8429 - val_loss: 0.9082 - val_acc: 0.6765\n",
      "Epoch 282/500\n",
      "191/191 [==============================] - 0s 14us/step - loss: 0.3589 - acc: 0.8429 - val_loss: 0.9081 - val_acc: 0.6765\n",
      "Epoch 283/500\n",
      "191/191 [==============================] - 0s 17us/step - loss: 0.3583 - acc: 0.8429 - val_loss: 0.9080 - val_acc: 0.6765\n",
      "Epoch 284/500\n",
      "191/191 [==============================] - 0s 11us/step - loss: 0.3577 - acc: 0.8482 - val_loss: 0.9080 - val_acc: 0.6765\n",
      "Epoch 285/500\n",
      "191/191 [==============================] - 0s 15us/step - loss: 0.3572 - acc: 0.8482 - val_loss: 0.9080 - val_acc: 0.6765\n",
      "Epoch 286/500\n",
      "191/191 [==============================] - 0s 11us/step - loss: 0.3566 - acc: 0.8482 - val_loss: 0.9080 - val_acc: 0.6765\n",
      "Epoch 287/500\n",
      "191/191 [==============================] - 0s 14us/step - loss: 0.3560 - acc: 0.8482 - val_loss: 0.9079 - val_acc: 0.6765\n",
      "Epoch 288/500\n",
      "191/191 [==============================] - 0s 13us/step - loss: 0.3554 - acc: 0.8482 - val_loss: 0.9078 - val_acc: 0.6765\n",
      "Epoch 289/500\n",
      "191/191 [==============================] - 0s 11us/step - loss: 0.3549 - acc: 0.8482 - val_loss: 0.9079 - val_acc: 0.6765\n",
      "Epoch 290/500\n",
      "191/191 [==============================] - 0s 11us/step - loss: 0.3543 - acc: 0.8482 - val_loss: 0.9080 - val_acc: 0.6765\n",
      "Epoch 291/500\n",
      "191/191 [==============================] - 0s 11us/step - loss: 0.3537 - acc: 0.8534 - val_loss: 0.9080 - val_acc: 0.6765\n",
      "Epoch 292/500\n",
      "191/191 [==============================] - 0s 10us/step - loss: 0.3532 - acc: 0.8534 - val_loss: 0.9080 - val_acc: 0.6765\n",
      "Epoch 293/500\n",
      "191/191 [==============================] - 0s 10us/step - loss: 0.3526 - acc: 0.8534 - val_loss: 0.9080 - val_acc: 0.6765\n",
      "Epoch 294/500\n",
      "191/191 [==============================] - 0s 12us/step - loss: 0.3521 - acc: 0.8534 - val_loss: 0.9081 - val_acc: 0.6765\n",
      "Epoch 295/500\n",
      "191/191 [==============================] - 0s 13us/step - loss: 0.3515 - acc: 0.8534 - val_loss: 0.9081 - val_acc: 0.6765\n",
      "Epoch 296/500\n",
      "191/191 [==============================] - 0s 14us/step - loss: 0.3510 - acc: 0.8534 - val_loss: 0.9081 - val_acc: 0.6765\n",
      "Epoch 297/500\n",
      "191/191 [==============================] - 0s 12us/step - loss: 0.3504 - acc: 0.8534 - val_loss: 0.9080 - val_acc: 0.6765\n",
      "Epoch 298/500\n",
      "191/191 [==============================] - 0s 14us/step - loss: 0.3499 - acc: 0.8534 - val_loss: 0.9080 - val_acc: 0.6765\n",
      "Epoch 299/500\n",
      "191/191 [==============================] - 0s 13us/step - loss: 0.3493 - acc: 0.8534 - val_loss: 0.9080 - val_acc: 0.6765\n",
      "Epoch 300/500\n",
      "191/191 [==============================] - 0s 10us/step - loss: 0.3488 - acc: 0.8534 - val_loss: 0.9079 - val_acc: 0.6765\n",
      "Epoch 301/500\n",
      "191/191 [==============================] - 0s 11us/step - loss: 0.3482 - acc: 0.8534 - val_loss: 0.9078 - val_acc: 0.6765\n",
      "Epoch 302/500\n",
      "191/191 [==============================] - 0s 15us/step - loss: 0.3477 - acc: 0.8534 - val_loss: 0.9077 - val_acc: 0.6765\n",
      "Epoch 303/500\n",
      "191/191 [==============================] - 0s 14us/step - loss: 0.3472 - acc: 0.8534 - val_loss: 0.9076 - val_acc: 0.6765\n",
      "Epoch 304/500\n",
      "191/191 [==============================] - 0s 13us/step - loss: 0.3466 - acc: 0.8534 - val_loss: 0.9074 - val_acc: 0.6765\n",
      "Epoch 305/500\n",
      "191/191 [==============================] - 0s 10us/step - loss: 0.3461 - acc: 0.8534 - val_loss: 0.9074 - val_acc: 0.6765\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 306/500\n",
      "191/191 [==============================] - 0s 10us/step - loss: 0.3456 - acc: 0.8534 - val_loss: 0.9073 - val_acc: 0.6765\n",
      "Epoch 307/500\n",
      "191/191 [==============================] - 0s 10us/step - loss: 0.3450 - acc: 0.8534 - val_loss: 0.9072 - val_acc: 0.6765\n",
      "Epoch 308/500\n",
      "191/191 [==============================] - 0s 13us/step - loss: 0.3445 - acc: 0.8534 - val_loss: 0.9071 - val_acc: 0.6765\n",
      "Epoch 309/500\n",
      "191/191 [==============================] - 0s 15us/step - loss: 0.3440 - acc: 0.8534 - val_loss: 0.9070 - val_acc: 0.6765\n",
      "Epoch 310/500\n",
      "191/191 [==============================] - 0s 11us/step - loss: 0.3435 - acc: 0.8534 - val_loss: 0.9070 - val_acc: 0.6765\n",
      "Epoch 311/500\n",
      "191/191 [==============================] - 0s 14us/step - loss: 0.3430 - acc: 0.8534 - val_loss: 0.9069 - val_acc: 0.6765\n",
      "Epoch 312/500\n",
      "191/191 [==============================] - 0s 11us/step - loss: 0.3425 - acc: 0.8534 - val_loss: 0.9069 - val_acc: 0.6765\n",
      "Epoch 313/500\n",
      "191/191 [==============================] - 0s 10us/step - loss: 0.3419 - acc: 0.8482 - val_loss: 0.9068 - val_acc: 0.6765\n",
      "Epoch 314/500\n",
      "191/191 [==============================] - 0s 10us/step - loss: 0.3414 - acc: 0.8482 - val_loss: 0.9068 - val_acc: 0.6765\n",
      "Epoch 315/500\n",
      "191/191 [==============================] - 0s 11us/step - loss: 0.3409 - acc: 0.8482 - val_loss: 0.9067 - val_acc: 0.6765\n",
      "Epoch 316/500\n",
      "191/191 [==============================] - 0s 19us/step - loss: 0.3404 - acc: 0.8482 - val_loss: 0.9068 - val_acc: 0.6765\n",
      "Epoch 317/500\n",
      "191/191 [==============================] - 0s 13us/step - loss: 0.3399 - acc: 0.8482 - val_loss: 0.9067 - val_acc: 0.6765\n",
      "Epoch 318/500\n",
      "191/191 [==============================] - 0s 10us/step - loss: 0.3394 - acc: 0.8482 - val_loss: 0.9065 - val_acc: 0.6765\n",
      "Epoch 319/500\n",
      "191/191 [==============================] - 0s 12us/step - loss: 0.3389 - acc: 0.8482 - val_loss: 0.9065 - val_acc: 0.6765\n",
      "Epoch 320/500\n",
      "191/191 [==============================] - 0s 15us/step - loss: 0.3384 - acc: 0.8482 - val_loss: 0.9066 - val_acc: 0.6765\n",
      "Epoch 321/500\n",
      "191/191 [==============================] - 0s 14us/step - loss: 0.3379 - acc: 0.8482 - val_loss: 0.9065 - val_acc: 0.6765\n",
      "Epoch 322/500\n",
      "191/191 [==============================] - 0s 11us/step - loss: 0.3374 - acc: 0.8482 - val_loss: 0.9063 - val_acc: 0.6765\n",
      "Epoch 323/500\n",
      "191/191 [==============================] - 0s 10us/step - loss: 0.3369 - acc: 0.8482 - val_loss: 0.9062 - val_acc: 0.6765\n",
      "Epoch 324/500\n",
      "191/191 [==============================] - 0s 13us/step - loss: 0.3364 - acc: 0.8482 - val_loss: 0.9062 - val_acc: 0.6765\n",
      "Epoch 325/500\n",
      "191/191 [==============================] - 0s 15us/step - loss: 0.3359 - acc: 0.8482 - val_loss: 0.9062 - val_acc: 0.6765\n",
      "Epoch 326/500\n",
      "191/191 [==============================] - 0s 14us/step - loss: 0.3354 - acc: 0.8482 - val_loss: 0.9063 - val_acc: 0.6765\n",
      "Epoch 327/500\n",
      "191/191 [==============================] - 0s 11us/step - loss: 0.3349 - acc: 0.8482 - val_loss: 0.9061 - val_acc: 0.6765\n",
      "Epoch 328/500\n",
      "191/191 [==============================] - 0s 10us/step - loss: 0.3344 - acc: 0.8482 - val_loss: 0.9060 - val_acc: 0.6765\n",
      "Epoch 329/500\n",
      "191/191 [==============================] - 0s 12us/step - loss: 0.3339 - acc: 0.8482 - val_loss: 0.9062 - val_acc: 0.6765\n",
      "Epoch 330/500\n",
      "191/191 [==============================] - 0s 15us/step - loss: 0.3334 - acc: 0.8482 - val_loss: 0.9064 - val_acc: 0.6765\n",
      "Epoch 331/500\n",
      "191/191 [==============================] - 0s 13us/step - loss: 0.3330 - acc: 0.8482 - val_loss: 0.9063 - val_acc: 0.6765\n",
      "Epoch 332/500\n",
      "191/191 [==============================] - 0s 14us/step - loss: 0.3325 - acc: 0.8482 - val_loss: 0.9062 - val_acc: 0.6765\n",
      "Epoch 333/500\n",
      "191/191 [==============================] - 0s 12us/step - loss: 0.3320 - acc: 0.8482 - val_loss: 0.9062 - val_acc: 0.6765\n",
      "Epoch 334/500\n",
      "191/191 [==============================] - 0s 20us/step - loss: 0.3315 - acc: 0.8482 - val_loss: 0.9061 - val_acc: 0.6765\n",
      "Epoch 335/500\n",
      "191/191 [==============================] - 0s 15us/step - loss: 0.3310 - acc: 0.8482 - val_loss: 0.9061 - val_acc: 0.6765\n",
      "Epoch 336/500\n",
      "191/191 [==============================] - 0s 13us/step - loss: 0.3305 - acc: 0.8482 - val_loss: 0.9060 - val_acc: 0.6765\n",
      "Epoch 337/500\n",
      "191/191 [==============================] - 0s 13us/step - loss: 0.3301 - acc: 0.8482 - val_loss: 0.9058 - val_acc: 0.6765\n",
      "Epoch 338/500\n",
      "191/191 [==============================] - 0s 10us/step - loss: 0.3296 - acc: 0.8482 - val_loss: 0.9057 - val_acc: 0.6765\n",
      "Epoch 339/500\n",
      "191/191 [==============================] - 0s 15us/step - loss: 0.3291 - acc: 0.8482 - val_loss: 0.9056 - val_acc: 0.6765\n",
      "Epoch 340/500\n",
      "191/191 [==============================] - 0s 11us/step - loss: 0.3286 - acc: 0.8482 - val_loss: 0.9054 - val_acc: 0.6765\n",
      "Epoch 341/500\n",
      "191/191 [==============================] - 0s 12us/step - loss: 0.3282 - acc: 0.8482 - val_loss: 0.9053 - val_acc: 0.6765\n",
      "Epoch 342/500\n",
      "191/191 [==============================] - 0s 16us/step - loss: 0.3277 - acc: 0.8482 - val_loss: 0.9051 - val_acc: 0.6765\n",
      "Epoch 343/500\n",
      "191/191 [==============================] - 0s 13us/step - loss: 0.3272 - acc: 0.8482 - val_loss: 0.9049 - val_acc: 0.6765\n",
      "Epoch 344/500\n",
      "191/191 [==============================] - 0s 16us/step - loss: 0.3267 - acc: 0.8482 - val_loss: 0.9047 - val_acc: 0.6765\n",
      "Epoch 345/500\n",
      "191/191 [==============================] - 0s 13us/step - loss: 0.3262 - acc: 0.8534 - val_loss: 0.9046 - val_acc: 0.7059\n",
      "Epoch 346/500\n",
      "191/191 [==============================] - 0s 15us/step - loss: 0.3258 - acc: 0.8534 - val_loss: 0.9042 - val_acc: 0.7059\n",
      "Epoch 347/500\n",
      "191/191 [==============================] - 0s 16us/step - loss: 0.3253 - acc: 0.8534 - val_loss: 0.9038 - val_acc: 0.7059\n",
      "Epoch 348/500\n",
      "191/191 [==============================] - 0s 12us/step - loss: 0.3248 - acc: 0.8534 - val_loss: 0.9035 - val_acc: 0.7059\n",
      "Epoch 349/500\n",
      "191/191 [==============================] - 0s 13us/step - loss: 0.3243 - acc: 0.8534 - val_loss: 0.9032 - val_acc: 0.7059\n",
      "Epoch 350/500\n",
      "191/191 [==============================] - 0s 14us/step - loss: 0.3238 - acc: 0.8534 - val_loss: 0.9028 - val_acc: 0.7059\n",
      "Epoch 351/500\n",
      "191/191 [==============================] - 0s 17us/step - loss: 0.3234 - acc: 0.8534 - val_loss: 0.9024 - val_acc: 0.7059\n",
      "Epoch 352/500\n",
      "191/191 [==============================] - 0s 12us/step - loss: 0.3229 - acc: 0.8534 - val_loss: 0.9021 - val_acc: 0.7059\n",
      "Epoch 353/500\n",
      "191/191 [==============================] - 0s 14us/step - loss: 0.3224 - acc: 0.8534 - val_loss: 0.9016 - val_acc: 0.7059\n",
      "Epoch 354/500\n",
      "191/191 [==============================] - 0s 15us/step - loss: 0.3219 - acc: 0.8534 - val_loss: 0.9011 - val_acc: 0.7059\n",
      "Epoch 355/500\n",
      "191/191 [==============================] - 0s 11us/step - loss: 0.3214 - acc: 0.8534 - val_loss: 0.9006 - val_acc: 0.7059\n",
      "Epoch 356/500\n",
      "191/191 [==============================] - 0s 15us/step - loss: 0.3209 - acc: 0.8534 - val_loss: 0.9001 - val_acc: 0.7059\n",
      "Epoch 357/500\n",
      "191/191 [==============================] - 0s 13us/step - loss: 0.3204 - acc: 0.8534 - val_loss: 0.8998 - val_acc: 0.7059\n",
      "Epoch 358/500\n",
      "191/191 [==============================] - 0s 14us/step - loss: 0.3199 - acc: 0.8534 - val_loss: 0.8992 - val_acc: 0.7059\n",
      "Epoch 359/500\n",
      "191/191 [==============================] - 0s 12us/step - loss: 0.3194 - acc: 0.8534 - val_loss: 0.8984 - val_acc: 0.7059\n",
      "Epoch 360/500\n",
      "191/191 [==============================] - 0s 11us/step - loss: 0.3190 - acc: 0.8534 - val_loss: 0.8976 - val_acc: 0.7059\n",
      "Epoch 361/500\n",
      "191/191 [==============================] - 0s 14us/step - loss: 0.3185 - acc: 0.8534 - val_loss: 0.8970 - val_acc: 0.7059\n",
      "Epoch 362/500\n",
      "191/191 [==============================] - 0s 12us/step - loss: 0.3180 - acc: 0.8586 - val_loss: 0.8964 - val_acc: 0.7059\n",
      "Epoch 363/500\n",
      "191/191 [==============================] - 0s 14us/step - loss: 0.3176 - acc: 0.8586 - val_loss: 0.8961 - val_acc: 0.7059\n",
      "Epoch 364/500\n",
      "191/191 [==============================] - 0s 11us/step - loss: 0.3171 - acc: 0.8586 - val_loss: 0.8955 - val_acc: 0.7059\n",
      "Epoch 365/500\n",
      "191/191 [==============================] - 0s 12us/step - loss: 0.3166 - acc: 0.8586 - val_loss: 0.8949 - val_acc: 0.7059\n",
      "Epoch 366/500\n",
      "191/191 [==============================] - 0s 14us/step - loss: 0.3162 - acc: 0.8586 - val_loss: 0.8944 - val_acc: 0.7059\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 367/500\n",
      "191/191 [==============================] - 0s 11us/step - loss: 0.3157 - acc: 0.8586 - val_loss: 0.8940 - val_acc: 0.7059\n",
      "Epoch 368/500\n",
      "191/191 [==============================] - 0s 17us/step - loss: 0.3153 - acc: 0.8586 - val_loss: 0.8937 - val_acc: 0.7059\n",
      "Epoch 369/500\n",
      "191/191 [==============================] - 0s 11us/step - loss: 0.3148 - acc: 0.8586 - val_loss: 0.8935 - val_acc: 0.7059\n",
      "Epoch 370/500\n",
      "191/191 [==============================] - 0s 14us/step - loss: 0.3144 - acc: 0.8586 - val_loss: 0.8931 - val_acc: 0.7059\n",
      "Epoch 371/500\n",
      "191/191 [==============================] - 0s 14us/step - loss: 0.3139 - acc: 0.8586 - val_loss: 0.8927 - val_acc: 0.7059\n",
      "Epoch 372/500\n",
      "191/191 [==============================] - 0s 13us/step - loss: 0.3135 - acc: 0.8586 - val_loss: 0.8923 - val_acc: 0.7059\n",
      "Epoch 373/500\n",
      "191/191 [==============================] - 0s 13us/step - loss: 0.3130 - acc: 0.8586 - val_loss: 0.8920 - val_acc: 0.7059\n",
      "Epoch 374/500\n",
      "191/191 [==============================] - 0s 13us/step - loss: 0.3126 - acc: 0.8586 - val_loss: 0.8917 - val_acc: 0.7059\n",
      "Epoch 375/500\n",
      "191/191 [==============================] - 0s 10us/step - loss: 0.3121 - acc: 0.8639 - val_loss: 0.8915 - val_acc: 0.7059\n",
      "Epoch 376/500\n",
      "191/191 [==============================] - 0s 15us/step - loss: 0.3117 - acc: 0.8639 - val_loss: 0.8923 - val_acc: 0.7059\n",
      "Epoch 377/500\n",
      "191/191 [==============================] - 0s 18us/step - loss: 0.3112 - acc: 0.8639 - val_loss: 0.8915 - val_acc: 0.7059\n",
      "Epoch 378/500\n",
      "191/191 [==============================] - 0s 15us/step - loss: 0.3108 - acc: 0.8639 - val_loss: 0.8914 - val_acc: 0.7059\n",
      "Epoch 379/500\n",
      "191/191 [==============================] - 0s 12us/step - loss: 0.3103 - acc: 0.8639 - val_loss: 0.8916 - val_acc: 0.7059\n",
      "Epoch 380/500\n",
      "191/191 [==============================] - 0s 11us/step - loss: 0.3099 - acc: 0.8639 - val_loss: 0.8913 - val_acc: 0.7059\n",
      "Epoch 381/500\n",
      "191/191 [==============================] - 0s 12us/step - loss: 0.3095 - acc: 0.8639 - val_loss: 0.8918 - val_acc: 0.7059\n",
      "Epoch 382/500\n",
      "191/191 [==============================] - 0s 14us/step - loss: 0.3090 - acc: 0.8639 - val_loss: 0.8912 - val_acc: 0.7059\n",
      "Epoch 383/500\n",
      "191/191 [==============================] - 0s 12us/step - loss: 0.3086 - acc: 0.8639 - val_loss: 0.8915 - val_acc: 0.7059\n",
      "Epoch 384/500\n",
      "191/191 [==============================] - 0s 13us/step - loss: 0.3082 - acc: 0.8639 - val_loss: 0.8914 - val_acc: 0.7059\n",
      "Epoch 385/500\n",
      "191/191 [==============================] - 0s 11us/step - loss: 0.3077 - acc: 0.8639 - val_loss: 0.8916 - val_acc: 0.7059\n",
      "Epoch 386/500\n",
      "191/191 [==============================] - 0s 11us/step - loss: 0.3073 - acc: 0.8639 - val_loss: 0.8918 - val_acc: 0.7059\n",
      "Epoch 387/500\n",
      "191/191 [==============================] - 0s 13us/step - loss: 0.3069 - acc: 0.8639 - val_loss: 0.8913 - val_acc: 0.7059\n",
      "Epoch 388/500\n",
      "191/191 [==============================] - 0s 12us/step - loss: 0.3064 - acc: 0.8639 - val_loss: 0.8918 - val_acc: 0.7059\n",
      "Epoch 389/500\n",
      "191/191 [==============================] - 0s 15us/step - loss: 0.3060 - acc: 0.8639 - val_loss: 0.8916 - val_acc: 0.7059\n",
      "Epoch 390/500\n",
      "191/191 [==============================] - 0s 12us/step - loss: 0.3056 - acc: 0.8639 - val_loss: 0.8923 - val_acc: 0.7059\n",
      "Epoch 391/500\n",
      "191/191 [==============================] - 0s 11us/step - loss: 0.3052 - acc: 0.8639 - val_loss: 0.8914 - val_acc: 0.7059\n",
      "Epoch 392/500\n",
      "191/191 [==============================] - 0s 14us/step - loss: 0.3048 - acc: 0.8639 - val_loss: 0.8924 - val_acc: 0.7059\n",
      "Epoch 393/500\n",
      "191/191 [==============================] - 0s 12us/step - loss: 0.3043 - acc: 0.8639 - val_loss: 0.8918 - val_acc: 0.7059\n",
      "Epoch 394/500\n",
      "191/191 [==============================] - 0s 15us/step - loss: 0.3039 - acc: 0.8691 - val_loss: 0.8929 - val_acc: 0.7059\n",
      "Epoch 395/500\n",
      "191/191 [==============================] - 0s 14us/step - loss: 0.3035 - acc: 0.8639 - val_loss: 0.8920 - val_acc: 0.7059\n",
      "Epoch 396/500\n",
      "191/191 [==============================] - 0s 12us/step - loss: 0.3031 - acc: 0.8691 - val_loss: 0.8929 - val_acc: 0.7059\n",
      "Epoch 397/500\n",
      "191/191 [==============================] - 0s 16us/step - loss: 0.3027 - acc: 0.8639 - val_loss: 0.8927 - val_acc: 0.7059\n",
      "Epoch 398/500\n",
      "191/191 [==============================] - 0s 12us/step - loss: 0.3022 - acc: 0.8639 - val_loss: 0.8931 - val_acc: 0.7059\n",
      "Epoch 399/500\n",
      "191/191 [==============================] - 0s 15us/step - loss: 0.3018 - acc: 0.8639 - val_loss: 0.8934 - val_acc: 0.7059\n",
      "Epoch 400/500\n",
      "191/191 [==============================] - 0s 13us/step - loss: 0.3014 - acc: 0.8639 - val_loss: 0.8932 - val_acc: 0.7059\n",
      "Epoch 401/500\n",
      "191/191 [==============================] - 0s 13us/step - loss: 0.3010 - acc: 0.8743 - val_loss: 0.8939 - val_acc: 0.7059\n",
      "Epoch 402/500\n",
      "191/191 [==============================] - 0s 13us/step - loss: 0.3006 - acc: 0.8639 - val_loss: 0.8932 - val_acc: 0.7059\n",
      "Epoch 403/500\n",
      "191/191 [==============================] - 0s 12us/step - loss: 0.3002 - acc: 0.8743 - val_loss: 0.8949 - val_acc: 0.7059\n",
      "Epoch 404/500\n",
      "191/191 [==============================] - 0s 15us/step - loss: 0.2998 - acc: 0.8639 - val_loss: 0.8912 - val_acc: 0.7059\n",
      "Epoch 405/500\n",
      "191/191 [==============================] - 0s 15us/step - loss: 0.2994 - acc: 0.8743 - val_loss: 0.8982 - val_acc: 0.7059\n",
      "Epoch 406/500\n",
      "191/191 [==============================] - 0s 15us/step - loss: 0.2990 - acc: 0.8586 - val_loss: 0.8906 - val_acc: 0.7059\n",
      "Epoch 407/500\n",
      "191/191 [==============================] - 0s 16us/step - loss: 0.2986 - acc: 0.8743 - val_loss: 0.8948 - val_acc: 0.7059\n",
      "Epoch 408/500\n",
      "191/191 [==============================] - 0s 13us/step - loss: 0.2981 - acc: 0.8691 - val_loss: 0.8982 - val_acc: 0.7059\n",
      "Epoch 409/500\n",
      "191/191 [==============================] - 0s 15us/step - loss: 0.2978 - acc: 0.8586 - val_loss: 0.8905 - val_acc: 0.7059\n",
      "Epoch 410/500\n",
      "191/191 [==============================] - 0s 13us/step - loss: 0.2974 - acc: 0.8743 - val_loss: 0.8968 - val_acc: 0.7059\n",
      "Epoch 411/500\n",
      "191/191 [==============================] - 0s 13us/step - loss: 0.2969 - acc: 0.8586 - val_loss: 0.8964 - val_acc: 0.7059\n",
      "Epoch 412/500\n",
      "191/191 [==============================] - 0s 11us/step - loss: 0.2965 - acc: 0.8639 - val_loss: 0.8920 - val_acc: 0.7059\n",
      "Epoch 413/500\n",
      "191/191 [==============================] - 0s 12us/step - loss: 0.2962 - acc: 0.8691 - val_loss: 0.8986 - val_acc: 0.7059\n",
      "Epoch 414/500\n",
      "191/191 [==============================] - 0s 13us/step - loss: 0.2958 - acc: 0.8586 - val_loss: 0.8943 - val_acc: 0.7059\n",
      "Epoch 415/500\n",
      "191/191 [==============================] - 0s 14us/step - loss: 0.2953 - acc: 0.8691 - val_loss: 0.8933 - val_acc: 0.7059\n",
      "Epoch 416/500\n",
      "191/191 [==============================] - 0s 15us/step - loss: 0.2950 - acc: 0.8691 - val_loss: 0.8993 - val_acc: 0.7059\n",
      "Epoch 417/500\n",
      "191/191 [==============================] - 0s 12us/step - loss: 0.2946 - acc: 0.8639 - val_loss: 0.8935 - val_acc: 0.7059\n",
      "Epoch 418/500\n",
      "191/191 [==============================] - 0s 10us/step - loss: 0.2942 - acc: 0.8691 - val_loss: 0.8952 - val_acc: 0.7059\n",
      "Epoch 419/500\n",
      "191/191 [==============================] - 0s 13us/step - loss: 0.2938 - acc: 0.8691 - val_loss: 0.8983 - val_acc: 0.7059\n",
      "Epoch 420/500\n",
      "191/191 [==============================] - 0s 12us/step - loss: 0.2934 - acc: 0.8639 - val_loss: 0.8931 - val_acc: 0.7059\n",
      "Epoch 421/500\n",
      "191/191 [==============================] - 0s 12us/step - loss: 0.2930 - acc: 0.8691 - val_loss: 0.8969 - val_acc: 0.7059\n",
      "Epoch 422/500\n",
      "191/191 [==============================] - 0s 10us/step - loss: 0.2926 - acc: 0.8691 - val_loss: 0.8972 - val_acc: 0.7059\n",
      "Epoch 423/500\n",
      "191/191 [==============================] - 0s 11us/step - loss: 0.2922 - acc: 0.8691 - val_loss: 0.8938 - val_acc: 0.7059\n",
      "Epoch 424/500\n",
      "191/191 [==============================] - 0s 10us/step - loss: 0.2918 - acc: 0.8691 - val_loss: 0.8982 - val_acc: 0.7059\n",
      "Epoch 425/500\n",
      "191/191 [==============================] - 0s 10us/step - loss: 0.2914 - acc: 0.8691 - val_loss: 0.8960 - val_acc: 0.7059\n",
      "Epoch 426/500\n",
      "191/191 [==============================] - 0s 17us/step - loss: 0.2910 - acc: 0.8691 - val_loss: 0.8953 - val_acc: 0.7059\n",
      "Epoch 427/500\n",
      "191/191 [==============================] - 0s 13us/step - loss: 0.2906 - acc: 0.8691 - val_loss: 0.8991 - val_acc: 0.7059\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 428/500\n",
      "191/191 [==============================] - 0s 12us/step - loss: 0.2903 - acc: 0.8691 - val_loss: 0.8957 - val_acc: 0.7059\n",
      "Epoch 429/500\n",
      "191/191 [==============================] - 0s 12us/step - loss: 0.2899 - acc: 0.8691 - val_loss: 0.8971 - val_acc: 0.7059\n",
      "Epoch 430/500\n",
      "191/191 [==============================] - 0s 10us/step - loss: 0.2895 - acc: 0.8691 - val_loss: 0.8990 - val_acc: 0.7059\n",
      "Epoch 431/500\n",
      "191/191 [==============================] - 0s 16us/step - loss: 0.2891 - acc: 0.8691 - val_loss: 0.8964 - val_acc: 0.7059\n",
      "Epoch 432/500\n",
      "191/191 [==============================] - 0s 13us/step - loss: 0.2887 - acc: 0.8691 - val_loss: 0.8989 - val_acc: 0.7059\n",
      "Epoch 433/500\n",
      "191/191 [==============================] - 0s 12us/step - loss: 0.2883 - acc: 0.8691 - val_loss: 0.8985 - val_acc: 0.7059\n",
      "Epoch 434/500\n",
      "191/191 [==============================] - 0s 16us/step - loss: 0.2880 - acc: 0.8691 - val_loss: 0.8972 - val_acc: 0.7059\n",
      "Epoch 435/500\n",
      "191/191 [==============================] - 0s 17us/step - loss: 0.2876 - acc: 0.8691 - val_loss: 0.9001 - val_acc: 0.7059\n",
      "Epoch 436/500\n",
      "191/191 [==============================] - 0s 13us/step - loss: 0.2872 - acc: 0.8691 - val_loss: 0.8983 - val_acc: 0.7059\n",
      "Epoch 437/500\n",
      "191/191 [==============================] - 0s 18us/step - loss: 0.2868 - acc: 0.8691 - val_loss: 0.8984 - val_acc: 0.7059\n",
      "Epoch 438/500\n",
      "191/191 [==============================] - 0s 13us/step - loss: 0.2865 - acc: 0.8691 - val_loss: 0.9003 - val_acc: 0.7059\n",
      "Epoch 439/500\n",
      "191/191 [==============================] - 0s 13us/step - loss: 0.2861 - acc: 0.8691 - val_loss: 0.8984 - val_acc: 0.7059\n",
      "Epoch 440/500\n",
      "191/191 [==============================] - 0s 13us/step - loss: 0.2857 - acc: 0.8691 - val_loss: 0.8998 - val_acc: 0.7059\n",
      "Epoch 441/500\n",
      "191/191 [==============================] - 0s 12us/step - loss: 0.2853 - acc: 0.8691 - val_loss: 0.8998 - val_acc: 0.7059\n",
      "Epoch 442/500\n",
      "191/191 [==============================] - 0s 11us/step - loss: 0.2849 - acc: 0.8691 - val_loss: 0.8986 - val_acc: 0.7059\n",
      "Epoch 443/500\n",
      "191/191 [==============================] - 0s 13us/step - loss: 0.2846 - acc: 0.8691 - val_loss: 0.9007 - val_acc: 0.7059\n",
      "Epoch 444/500\n",
      "191/191 [==============================] - 0s 12us/step - loss: 0.2842 - acc: 0.8691 - val_loss: 0.8996 - val_acc: 0.7059\n",
      "Epoch 445/500\n",
      "191/191 [==============================] - 0s 12us/step - loss: 0.2838 - acc: 0.8691 - val_loss: 0.8996 - val_acc: 0.7059\n",
      "Epoch 446/500\n",
      "191/191 [==============================] - 0s 13us/step - loss: 0.2834 - acc: 0.8691 - val_loss: 0.9009 - val_acc: 0.7059\n",
      "Epoch 447/500\n",
      "191/191 [==============================] - 0s 15us/step - loss: 0.2830 - acc: 0.8691 - val_loss: 0.8997 - val_acc: 0.7059\n",
      "Epoch 448/500\n",
      "191/191 [==============================] - 0s 14us/step - loss: 0.2827 - acc: 0.8691 - val_loss: 0.9010 - val_acc: 0.7059\n",
      "Epoch 449/500\n",
      "191/191 [==============================] - 0s 11us/step - loss: 0.2823 - acc: 0.8691 - val_loss: 0.9010 - val_acc: 0.7059\n",
      "Epoch 450/500\n",
      "191/191 [==============================] - 0s 13us/step - loss: 0.2819 - acc: 0.8691 - val_loss: 0.9006 - val_acc: 0.7059\n",
      "Epoch 451/500\n",
      "191/191 [==============================] - 0s 14us/step - loss: 0.2815 - acc: 0.8691 - val_loss: 0.9020 - val_acc: 0.7059\n",
      "Epoch 452/500\n",
      "191/191 [==============================] - 0s 12us/step - loss: 0.2812 - acc: 0.8691 - val_loss: 0.9012 - val_acc: 0.7059\n",
      "Epoch 453/500\n",
      "191/191 [==============================] - 0s 14us/step - loss: 0.2808 - acc: 0.8691 - val_loss: 0.9020 - val_acc: 0.7059\n",
      "Epoch 454/500\n",
      "191/191 [==============================] - 0s 11us/step - loss: 0.2804 - acc: 0.8691 - val_loss: 0.9025 - val_acc: 0.7059\n",
      "Epoch 455/500\n",
      "191/191 [==============================] - 0s 13us/step - loss: 0.2800 - acc: 0.8691 - val_loss: 0.9021 - val_acc: 0.7059\n",
      "Epoch 456/500\n",
      "191/191 [==============================] - 0s 17us/step - loss: 0.2797 - acc: 0.8691 - val_loss: 0.9032 - val_acc: 0.7059\n",
      "Epoch 457/500\n",
      "191/191 [==============================] - 0s 11us/step - loss: 0.2793 - acc: 0.8691 - val_loss: 0.9029 - val_acc: 0.7059\n",
      "Epoch 458/500\n",
      "191/191 [==============================] - 0s 16us/step - loss: 0.2789 - acc: 0.8691 - val_loss: 0.9036 - val_acc: 0.7059\n",
      "Epoch 459/500\n",
      "191/191 [==============================] - 0s 11us/step - loss: 0.2786 - acc: 0.8743 - val_loss: 0.9044 - val_acc: 0.7059\n",
      "Epoch 460/500\n",
      "191/191 [==============================] - 0s 15us/step - loss: 0.2782 - acc: 0.8743 - val_loss: 0.9041 - val_acc: 0.7059\n",
      "Epoch 461/500\n",
      "191/191 [==============================] - 0s 14us/step - loss: 0.2778 - acc: 0.8743 - val_loss: 0.9051 - val_acc: 0.7059\n",
      "Epoch 462/500\n",
      "191/191 [==============================] - 0s 17us/step - loss: 0.2775 - acc: 0.8743 - val_loss: 0.9053 - val_acc: 0.7059\n",
      "Epoch 463/500\n",
      "191/191 [==============================] - 0s 12us/step - loss: 0.2771 - acc: 0.8743 - val_loss: 0.9057 - val_acc: 0.7059\n",
      "Epoch 464/500\n",
      "191/191 [==============================] - 0s 12us/step - loss: 0.2767 - acc: 0.8743 - val_loss: 0.9065 - val_acc: 0.7059\n",
      "Epoch 465/500\n",
      "191/191 [==============================] - 0s 13us/step - loss: 0.2764 - acc: 0.8743 - val_loss: 0.9065 - val_acc: 0.7059\n",
      "Epoch 466/500\n",
      "191/191 [==============================] - 0s 11us/step - loss: 0.2760 - acc: 0.8743 - val_loss: 0.9074 - val_acc: 0.7059\n",
      "Epoch 467/500\n",
      "191/191 [==============================] - 0s 14us/step - loss: 0.2757 - acc: 0.8743 - val_loss: 0.9077 - val_acc: 0.7059\n",
      "Epoch 468/500\n",
      "191/191 [==============================] - 0s 12us/step - loss: 0.2753 - acc: 0.8743 - val_loss: 0.9081 - val_acc: 0.7059\n",
      "Epoch 469/500\n",
      "191/191 [==============================] - 0s 13us/step - loss: 0.2749 - acc: 0.8743 - val_loss: 0.9091 - val_acc: 0.7059\n",
      "Epoch 470/500\n",
      "191/191 [==============================] - 0s 15us/step - loss: 0.2746 - acc: 0.8743 - val_loss: 0.9093 - val_acc: 0.7059\n",
      "Epoch 471/500\n",
      "191/191 [==============================] - 0s 11us/step - loss: 0.2742 - acc: 0.8743 - val_loss: 0.9101 - val_acc: 0.7059\n",
      "Epoch 472/500\n",
      "191/191 [==============================] - 0s 12us/step - loss: 0.2739 - acc: 0.8743 - val_loss: 0.9104 - val_acc: 0.7059\n",
      "Epoch 473/500\n",
      "191/191 [==============================] - 0s 16us/step - loss: 0.2735 - acc: 0.8743 - val_loss: 0.9108 - val_acc: 0.7059\n",
      "Epoch 474/500\n",
      "191/191 [==============================] - 0s 13us/step - loss: 0.2732 - acc: 0.8743 - val_loss: 0.9116 - val_acc: 0.7059\n",
      "Epoch 475/500\n",
      "191/191 [==============================] - 0s 14us/step - loss: 0.2728 - acc: 0.8743 - val_loss: 0.9117 - val_acc: 0.7059\n",
      "Epoch 476/500\n",
      "191/191 [==============================] - 0s 11us/step - loss: 0.2724 - acc: 0.8743 - val_loss: 0.9125 - val_acc: 0.7059\n",
      "Epoch 477/500\n",
      "191/191 [==============================] - 0s 11us/step - loss: 0.2721 - acc: 0.8743 - val_loss: 0.9128 - val_acc: 0.7059\n",
      "Epoch 478/500\n",
      "191/191 [==============================] - 0s 16us/step - loss: 0.2717 - acc: 0.8743 - val_loss: 0.9134 - val_acc: 0.7059\n",
      "Epoch 479/500\n",
      "191/191 [==============================] - 0s 13us/step - loss: 0.2714 - acc: 0.8743 - val_loss: 0.9138 - val_acc: 0.7059\n",
      "Epoch 480/500\n",
      "191/191 [==============================] - 0s 16us/step - loss: 0.2710 - acc: 0.8743 - val_loss: 0.9141 - val_acc: 0.7059\n",
      "Epoch 481/500\n",
      "191/191 [==============================] - 0s 12us/step - loss: 0.2707 - acc: 0.8743 - val_loss: 0.9147 - val_acc: 0.6765\n",
      "Epoch 482/500\n",
      "191/191 [==============================] - 0s 10us/step - loss: 0.2703 - acc: 0.8743 - val_loss: 0.9148 - val_acc: 0.6765\n",
      "Epoch 483/500\n",
      "191/191 [==============================] - 0s 11us/step - loss: 0.2700 - acc: 0.8743 - val_loss: 0.9154 - val_acc: 0.6765\n",
      "Epoch 484/500\n",
      "191/191 [==============================] - 0s 14us/step - loss: 0.2696 - acc: 0.8743 - val_loss: 0.9156 - val_acc: 0.6765\n",
      "Epoch 485/500\n",
      "191/191 [==============================] - 0s 13us/step - loss: 0.2693 - acc: 0.8743 - val_loss: 0.9161 - val_acc: 0.6765\n",
      "Epoch 486/500\n",
      "191/191 [==============================] - 0s 11us/step - loss: 0.2689 - acc: 0.8743 - val_loss: 0.9164 - val_acc: 0.6765\n",
      "Epoch 487/500\n",
      "191/191 [==============================] - 0s 11us/step - loss: 0.2686 - acc: 0.8743 - val_loss: 0.9168 - val_acc: 0.6765\n",
      "Epoch 488/500\n",
      "191/191 [==============================] - 0s 10us/step - loss: 0.2682 - acc: 0.8743 - val_loss: 0.9172 - val_acc: 0.6765\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 489/500\n",
      "191/191 [==============================] - 0s 13us/step - loss: 0.2679 - acc: 0.8743 - val_loss: 0.9174 - val_acc: 0.6765\n",
      "Epoch 490/500\n",
      "191/191 [==============================] - 0s 15us/step - loss: 0.2675 - acc: 0.8743 - val_loss: 0.9179 - val_acc: 0.6765\n",
      "Epoch 491/500\n",
      "191/191 [==============================] - 0s 13us/step - loss: 0.2672 - acc: 0.8743 - val_loss: 0.9181 - val_acc: 0.6765\n",
      "Epoch 492/500\n",
      "191/191 [==============================] - 0s 11us/step - loss: 0.2669 - acc: 0.8743 - val_loss: 0.9186 - val_acc: 0.6765\n",
      "Epoch 493/500\n",
      "191/191 [==============================] - 0s 9us/step - loss: 0.2665 - acc: 0.8743 - val_loss: 0.9187 - val_acc: 0.6765\n",
      "Epoch 494/500\n",
      "191/191 [==============================] - 0s 11us/step - loss: 0.2662 - acc: 0.8743 - val_loss: 0.9192 - val_acc: 0.6765\n",
      "Epoch 495/500\n",
      "191/191 [==============================] - 0s 12us/step - loss: 0.2658 - acc: 0.8743 - val_loss: 0.9194 - val_acc: 0.6765\n",
      "Epoch 496/500\n",
      "191/191 [==============================] - 0s 13us/step - loss: 0.2655 - acc: 0.8743 - val_loss: 0.9197 - val_acc: 0.6765\n",
      "Epoch 497/500\n",
      "191/191 [==============================] - 0s 16us/step - loss: 0.2652 - acc: 0.8743 - val_loss: 0.9199 - val_acc: 0.6765\n",
      "Epoch 498/500\n",
      "191/191 [==============================] - 0s 11us/step - loss: 0.2648 - acc: 0.8743 - val_loss: 0.9201 - val_acc: 0.6765\n",
      "Epoch 499/500\n",
      "191/191 [==============================] - 0s 10us/step - loss: 0.2645 - acc: 0.8743 - val_loss: 0.9204 - val_acc: 0.6765\n",
      "Epoch 500/500\n",
      "191/191 [==============================] - 0s 12us/step - loss: 0.2641 - acc: 0.8743 - val_loss: 0.9204 - val_acc: 0.6765\n"
     ]
    }
   ],
   "source": [
    "# use the training data to fit the model.  Choose numbers for epochs and the validation split\n",
    "history = model.fit(,\n",
    "                    ,\n",
    "                    epochs=,\n",
    "                    validation_split=,\n",
    "                    batch_size=500,\n",
    "                   verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/w8TlrP0xT+sYAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe7cc24d6a0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# check losses for overfitting and performance\n",
    "plt.plot(history.history['loss'], label = 'training loss')\n",
    "plt.plot(history.history['val_loss'], label = 'validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['loss', 'val_loss', 'acc', 'val_acc'])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nd6WUikMa3JVSKg79fwzhxBRg9JjeAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe7cc24dda0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# check accuracy for overfitting and performance\n",
    "# create the plots similary as above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "225/225 [==============================] - 0s 22us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3630162700017293, 0.8444444444444444]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is how we get the [loss, accuracy]\n",
    "model.evaluate(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57/57 [==============================] - 0s 35us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5911086180753875, 0.7719298297898811]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use model.evaluate on the test set to see performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (bonus) next step: use dropout and/or regularization to prevent overfitting.  Dropout goes after dense layers.  The standard dropout rate is 0.5.  Example:\n",
    "\n",
    "```python\n",
    "x1 = Dense(10)(inputs)\n",
    "x2 = Dropout(0.5)(x1)\n",
    "preds = Dense(1, activation='sigmoid')(x2)\n",
    "```\n",
    "\n",
    "0.5 means half of the connections between x1 and the preds layer are dropped (ignored) during training.  0.1 would mean 10% of the connections would be dropped during training.  For such as small net as we have, the range of 0.1-0.3 makes more sense.\n",
    "\n",
    "https://keras.io/layers/core/#dropout\n",
    "\n",
    "https://keras.io/regularizers/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
