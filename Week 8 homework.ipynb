{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code is here: https://github.com/khpeek/Q-learning-Tic-Tac-Toe\n",
    "1. Name the function where the Q-learning is actually happening (the long Q-learning equation is being used).\n",
    "2. Show the two lines of code where the Q-learning update happens. \n",
    "3. Change the default Q-value from 1 to 0, and train for 10k episodes. Play the agents a few times. Are the behaviors of the agents different with 1 or 0 as the default Q?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tkinter as tk\n",
    "import copy\n",
    "import pickle as pickle "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. The q-learning function is at line 130 in the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def learn_Q(self, move):                        # If Q-learning is toggled on, \"learn_Q\" should be called after receiving a move from an instance of Player and before implementing the move (using Board's \"place_mark\" method)\n",
    "    state_key = QPlayer.make_and_maybe_add_key(self.board, self.current_player.mark, self.Q)\n",
    "    next_board = self.board.get_next_board(move, self.current_player.mark)\n",
    "    reward = next_board.give_reward()\n",
    "    next_state_key = QPlayer.make_and_maybe_add_key(next_board, self.other_player.mark, self.Q)\n",
    "    if next_board.over():\n",
    "        expected = reward\n",
    "    else:\n",
    "        next_Qs = self.Q[next_state_key]             # The Q values represent the expected future reward for player X for each available move in the next state (after the move has been made)\n",
    "        if self.current_player.mark == \"X\":\n",
    "            expected = reward + (self.gamma * min(next_Qs.values()))        # If the current player is X, the next player is O, and the move with the minimum Q value should be chosen according to our \"sign convention\"\n",
    "        elif self.current_player.mark == \"O\":\n",
    "            expected = reward + (self.gamma * max(next_Qs.values()))        # If the current player is O, the next player is X, and the move with the maximum Q vlue should be chosen\n",
    "    change = self.alpha * (expected - self.Q[state_key][move])\n",
    "    self.Q[state_key][move] += change\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) The code for updating the Q-values is at Line 143"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "        change = self.alpha * (expected - self.Q[state_key][move])\n",
    "        self.Q[state_key][move] += change"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Line 190: Define rewards: Assigning reward for winning and negative reward for loosing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Line 255: epsilon greedy exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Default Qvalue can be found at line 269, with a note that says  \"encourages exploration\"\n",
    "- It chooses a random move, and otherwise it follows the policy dictated by `Q` -- that is, if the player has mark \"X\" (\"O\"), choose the move with the highest (lowest) Q-value, in accordance with our 'sign convention'. During training, `epsilon` is set to a high value to encourage exploration, whereas for the actual match against a human, it is set to zero for optimal performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    def make_and_maybe_add_key(board, mark, Q):     # Make a dictionary key for the current state (board + player turn) and if Q does not yet have it, add it to Q\n",
    "        default_Qvalue = 1.0       # Encourages exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I changed the qvalue to 0.0 then editted the \n",
    "Tic Tax Toe QPlayer Training py file\n",
    "to run for 10 k episodes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N_episodes = 100000\n",
    "for episodes in range(N_episodes):\n",
    "    game.play()\n",
    "    game.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Once finished it created a new file \"Q_epsilon_09_Nepisodes_100000.p\"\n",
    "- Update the Tic_Tac_Toe_Human_vs_QPlayer.py file to direct to this file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Q = pickle.load(open(\"Q_epsilon_09_Nepisodes_100000.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I played multiple games against it and it was still very good"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
