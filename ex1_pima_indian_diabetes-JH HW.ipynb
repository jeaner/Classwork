{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# By: Jeanette Henry\n",
    "\n",
    "# Predict the onset of diabetes based on diagnostic measures using ANN\n",
    "In this exercise we will build a simple ANN for a binary classification problem.   \n",
    "Kaggle API command to download dataset:     \n",
    "`kaggle datasets download -d uciml/pima-indians-diabetes-database`\n",
    "\n",
    "Or Download from here: [https://www.kaggle.com/uciml/pima-indians-diabetes-database/data](https://www.kaggle.com/uciml/pima-indians-diabetes-database/data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Activation, Dense, Dropout, BatchNormalization\n",
    "from keras.models import Model, load_model\n",
    "import keras.backend as K\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import numpy as np\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy.random import seed\n",
    "seed(223)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(223)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
       "0            6      148             72             35        0  33.6   \n",
       "1            1       85             66             29        0  26.6   \n",
       "2            8      183             64              0        0  23.3   \n",
       "3            1       89             66             23       94  28.1   \n",
       "4            0      137             40             35      168  43.1   \n",
       "\n",
       "   DiabetesPedigreeFunction  Age  Outcome  \n",
       "0                     0.627   50        1  \n",
       "1                     0.351   31        0  \n",
       "2                     0.672   32        1  \n",
       "3                     0.167   21        0  \n",
       "4                     2.288   33        1  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = 'C:/MSDS/Deep/wk1/diabetes.csv'\n",
    "#this is specific to my case. use your own path to the data\n",
    "df = pd.read_csv(filename)\n",
    "df.head() #shows the first 5 lines of your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 768 entries, 0 to 767\n",
      "Data columns (total 9 columns):\n",
      "Pregnancies                 768 non-null int64\n",
      "Glucose                     768 non-null int64\n",
      "BloodPressure               768 non-null int64\n",
      "SkinThickness               768 non-null int64\n",
      "Insulin                     768 non-null int64\n",
      "BMI                         768 non-null float64\n",
      "DiabetesPedigreeFunction    768 non-null float64\n",
      "Age                         768 non-null int64\n",
      "Outcome                     768 non-null int64\n",
      "dtypes: float64(2), int64(7)\n",
      "memory usage: 54.1 KB\n"
     ]
    }
   ],
   "source": [
    "df.info() #shows attributes and their properties. \n",
    "#seems there is no null value nor wrong data type such as string value \n",
    "#for numbers which sometimes happens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create X and Y in numpy array format from pandas dataframe \n",
    "Y = np.array(df['Outcome'])\n",
    "X = np.array(df.drop('Outcome',axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(768, 8) (768,)\n"
     ]
    }
   ],
   "source": [
    "# Check the X and Y shapes \n",
    "print(X.shape, Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# YOUR TURN\n",
    "# Split the dataset to train and test set with the ratio of 80% train and 20% test\n",
    "# hint : use sklearn train_test_split()\n",
    "# use naming as X_train, Y_train for train data and X_test, Y_test for test data.\n",
    "# Your Answer: \n",
    "\n",
    "\n",
    "# Check (print) the data shape for each\n",
    "# Your Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, #features,\n",
    "                                                    Y, #targets,\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=42,\n",
    "                                                    stratify=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a simple Keras model\n",
    "\n",
    "Keras is a high-level neural networks API, written in Python and capable of running on top of TensorFlow, CNTK, or Theano. It can work with various software and hardware platforms. You can build and train neural network models easily without having to worry about how to handle those specific software/hardware platforms. For more info, see [Keras documentation](https://keras.io/).\n",
    "\n",
    "We'll use the `Sequential()` model in Keras. \n",
    "The Sequential model is a linear stack of layers. You can create a Sequential model by passing a list of layer instances to the constructor.\n",
    "\n",
    "\n",
    "```python\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(32, input_shape=(784,)),\n",
    "    Activation('relu'),\n",
    "    Dense(10),\n",
    "    Activation('softmax'),\n",
    "])\n",
    "```\n",
    "`input_shape=(784,)` means the data has 784 features (Just ignore this number for now).  \n",
    "Or alternatively, you can add layers using .add() method to the layers object `model`.\n",
    "```python\n",
    "model = Sequential()\n",
    "model.add(Dense(32, input_dim=784))\n",
    "model.add(Activation('relu'))\n",
    "```\n",
    "This works too.\n",
    "```python\n",
    "l1 = Dense(32,input_dim=784)\n",
    "l2 = Activation('relu')\n",
    "\n",
    "model = Sequential([l1,l2])\n",
    "```\n",
    "You can wrap it in a function.\n",
    "```python\n",
    "def mymodel():\n",
    "    model=Sequential()\n",
    "    model.add(Dense(32,input_dim=784))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(32))\n",
    "    model.add(Activation('relu'))\n",
    "    return model\n",
    "```\n",
    "\n",
    "For more info, see [About Keras Sequential Model](https://keras.io/getting-started/sequential-model-guide/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# YOUR TURN\n",
    "# build a model with 3 hidden layers (the first hidden layer is already given) and the output layer to classify (binary)\n",
    "# which activation functions should you use for hidden layers and the output layer?\n",
    "# Your Answer:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#From keras\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD\n",
    "import keras.backend as K\n",
    "K.clear_session()\n",
    "model = Sequential()\n",
    "model.add(Dense(64, activation='relu', input_dim=8)) \n",
    "model.add(Dense(32, activation='tanh'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 64)                576       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 2,689\n",
      "Trainable params: 2,689\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# prints out the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# compile your model\n",
    "# we use binary cross entropy since this problem is a binary classification problem. \n",
    "# let's add accuracy metric to display \n",
    "# we use adam optimizer (let's not worry about what is it for now. it's easier to use than other optimizers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss ='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# YOUR TURN\n",
    "# Fit your model on the train data using batch size 32 and for 20 epochs\n",
    "# hint: use model.fit() function\n",
    "# Your Answer:\n",
    "# ?model.fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "614/614 [==============================] - 0s 56us/step - loss: 0.4772 - acc: 0.7541\n",
      "Epoch 2/20\n",
      "614/614 [==============================] - 0s 63us/step - loss: 0.4944 - acc: 0.7443\n",
      "Epoch 3/20\n",
      "614/614 [==============================] - 0s 58us/step - loss: 0.5234 - acc: 0.7117\n",
      "Epoch 4/20\n",
      "614/614 [==============================] - 0s 57us/step - loss: 0.5000 - acc: 0.7313\n",
      "Epoch 5/20\n",
      "614/614 [==============================] - 0s 54us/step - loss: 0.4832 - acc: 0.7508\n",
      "Epoch 6/20\n",
      "614/614 [==============================] - 0s 66us/step - loss: 0.4731 - acc: 0.7557\n",
      "Epoch 7/20\n",
      "614/614 [==============================] - 0s 53us/step - loss: 0.4633 - acc: 0.7801\n",
      "Epoch 8/20\n",
      "614/614 [==============================] - 0s 67us/step - loss: 0.4610 - acc: 0.7736\n",
      "Epoch 9/20\n",
      "614/614 [==============================] - 0s 53us/step - loss: 0.4697 - acc: 0.7687\n",
      "Epoch 10/20\n",
      "614/614 [==============================] - 0s 64us/step - loss: 0.4606 - acc: 0.7769\n",
      "Epoch 11/20\n",
      "614/614 [==============================] - 0s 61us/step - loss: 0.4570 - acc: 0.7736\n",
      "Epoch 12/20\n",
      "614/614 [==============================] - 0s 58us/step - loss: 0.4588 - acc: 0.7834\n",
      "Epoch 13/20\n",
      "614/614 [==============================] - 0s 64us/step - loss: 0.4688 - acc: 0.7752\n",
      "Epoch 14/20\n",
      "614/614 [==============================] - 0s 61us/step - loss: 0.4609 - acc: 0.7752\n",
      "Epoch 15/20\n",
      "614/614 [==============================] - 0s 58us/step - loss: 0.4482 - acc: 0.7801\n",
      "Epoch 16/20\n",
      "614/614 [==============================] - 0s 65us/step - loss: 0.4504 - acc: 0.7834\n",
      "Epoch 17/20\n",
      "614/614 [==============================] - 0s 60us/step - loss: 0.4523 - acc: 0.7915\n",
      "Epoch 18/20\n",
      "614/614 [==============================] - 0s 55us/step - loss: 0.4490 - acc: 0.7801\n",
      "Epoch 19/20\n",
      "614/614 [==============================] - 0s 61us/step - loss: 0.4574 - acc: 0.7818\n",
      "Epoch 20/20\n",
      "614/614 [==============================] - 0s 54us/step - loss: 0.4879 - acc: 0.7296\n"
     ]
    }
   ],
   "source": [
    "history=model.fit(x_train, y_train, \n",
    "                  batch_size=32,\n",
    "                  epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "154/154 [==============================] - 0s 55us/step\n",
      "[0.62608670104633679, 0.74025974335608546]\n"
     ]
    }
   ],
   "source": [
    "tst_score = model.evaluate(x_test, y_test)\n",
    "print(tst_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "614/614 [==============================] - 0s 30us/step\n",
      "[0.44545967575975659, 0.79641693791659729]\n"
     ]
    }
   ],
   "source": [
    "train_score = model.evaluate(x_train, y_train)\n",
    "print(train_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#  YOUR TURN\n",
    "\n",
    "#  Q1. What was the best accuracy from the above result?\n",
    "#  Your Answer:\n",
    "### my best accuracy was epoch 19 at 78.18%\n",
    "Epoch 19/20\n",
    "614/614 [==============================] - 0s 61us/step \n",
    "- loss: 0.4574 \n",
    "- acc: 0.7818\n",
    "### Test accuracy was 74.0% and train score was 79.6%, slightly overfit\n",
    "\n",
    "#  Q2. What happens if you change epochs? What is your optimal epochs?\n",
    "#  Your Answer:\n",
    "### my best accuracy increased, Epoch 44 was 81.11%\n",
    "Epoch 44/50\n",
    "614/614 [==============================] - 0s 58us/step \n",
    "- loss: 0.4178 \n",
    "- acc: 0.8111\n",
    "### Test accuracy was 74.7% and train score was 82.1%, so also overfit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "614/614 [==============================] - 0s 56us/step - loss: 0.4639 - acc: 0.7671\n",
      "Epoch 2/50\n",
      "614/614 [==============================] - 0s 67us/step - loss: 0.4581 - acc: 0.7769\n",
      "Epoch 3/50\n",
      "614/614 [==============================] - 0s 51us/step - loss: 0.4517 - acc: 0.7883\n",
      "Epoch 4/50\n",
      "614/614 [==============================] - 0s 57us/step - loss: 0.4732 - acc: 0.7687\n",
      "Epoch 5/50\n",
      "614/614 [==============================] - 0s 55us/step - loss: 0.4560 - acc: 0.7834\n",
      "Epoch 6/50\n",
      "614/614 [==============================] - 0s 62us/step - loss: 0.4523 - acc: 0.8013\n",
      "Epoch 7/50\n",
      "614/614 [==============================] - 0s 53us/step - loss: 0.4449 - acc: 0.7997\n",
      "Epoch 8/50\n",
      "614/614 [==============================] - 0s 59us/step - loss: 0.4499 - acc: 0.7980\n",
      "Epoch 9/50\n",
      "614/614 [==============================] - 0s 61us/step - loss: 0.4498 - acc: 0.7964\n",
      "Epoch 10/50\n",
      "614/614 [==============================] - 0s 59us/step - loss: 0.4514 - acc: 0.7932\n",
      "Epoch 11/50\n",
      "614/614 [==============================] - 0s 57us/step - loss: 0.4585 - acc: 0.7736\n",
      "Epoch 12/50\n",
      "614/614 [==============================] - 0s 62us/step - loss: 0.4505 - acc: 0.7834\n",
      "Epoch 13/50\n",
      "614/614 [==============================] - 0s 57us/step - loss: 0.4512 - acc: 0.7834\n",
      "Epoch 14/50\n",
      "614/614 [==============================] - 0s 58us/step - loss: 0.4811 - acc: 0.7638\n",
      "Epoch 15/50\n",
      "614/614 [==============================] - 0s 59us/step - loss: 0.4896 - acc: 0.7573\n",
      "Epoch 16/50\n",
      "614/614 [==============================] - 0s 57us/step - loss: 0.4565 - acc: 0.7704\n",
      "Epoch 17/50\n",
      "614/614 [==============================] - 0s 63us/step - loss: 0.4635 - acc: 0.7801\n",
      "Epoch 18/50\n",
      "614/614 [==============================] - 0s 62us/step - loss: 0.4555 - acc: 0.7785\n",
      "Epoch 19/50\n",
      "614/614 [==============================] - 0s 65us/step - loss: 0.4407 - acc: 0.7932\n",
      "Epoch 20/50\n",
      "614/614 [==============================] - 0s 57us/step - loss: 0.4441 - acc: 0.7850\n",
      "Epoch 21/50\n",
      "614/614 [==============================] - 0s 57us/step - loss: 0.4987 - acc: 0.7345\n",
      "Epoch 22/50\n",
      "614/614 [==============================] - 0s 55us/step - loss: 0.4552 - acc: 0.7866\n",
      "Epoch 23/50\n",
      "614/614 [==============================] - 0s 56us/step - loss: 0.4561 - acc: 0.7850\n",
      "Epoch 24/50\n",
      "614/614 [==============================] - 0s 59us/step - loss: 0.4445 - acc: 0.7769\n",
      "Epoch 25/50\n",
      "614/614 [==============================] - 0s 68us/step - loss: 0.4289 - acc: 0.7899\n",
      "Epoch 26/50\n",
      "614/614 [==============================] - 0s 55us/step - loss: 0.4456 - acc: 0.7752\n",
      "Epoch 27/50\n",
      "614/614 [==============================] - 0s 55us/step - loss: 0.4287 - acc: 0.8046\n",
      "Epoch 28/50\n",
      "614/614 [==============================] - 0s 57us/step - loss: 0.4397 - acc: 0.7899\n",
      "Epoch 29/50\n",
      "614/614 [==============================] - 0s 52us/step - loss: 0.4304 - acc: 0.7932\n",
      "Epoch 30/50\n",
      "614/614 [==============================] - 0s 65us/step - loss: 0.4571 - acc: 0.7736\n",
      "Epoch 31/50\n",
      "614/614 [==============================] - 0s 62us/step - loss: 0.4348 - acc: 0.7883\n",
      "Epoch 32/50\n",
      "614/614 [==============================] - 0s 54us/step - loss: 0.4496 - acc: 0.7769\n",
      "Epoch 33/50\n",
      "614/614 [==============================] - 0s 56us/step - loss: 0.4477 - acc: 0.7866\n",
      "Epoch 34/50\n",
      "614/614 [==============================] - 0s 54us/step - loss: 0.4427 - acc: 0.7801\n",
      "Epoch 35/50\n",
      "614/614 [==============================] - 0s 61us/step - loss: 0.4621 - acc: 0.7752\n",
      "Epoch 36/50\n",
      "614/614 [==============================] - 0s 57us/step - loss: 0.4415 - acc: 0.7818\n",
      "Epoch 37/50\n",
      "614/614 [==============================] - 0s 53us/step - loss: 0.4400 - acc: 0.7883\n",
      "Epoch 38/50\n",
      "614/614 [==============================] - 0s 67us/step - loss: 0.4328 - acc: 0.7980\n",
      "Epoch 39/50\n",
      "614/614 [==============================] - 0s 55us/step - loss: 0.4395 - acc: 0.7899\n",
      "Epoch 40/50\n",
      "614/614 [==============================] - 0s 60us/step - loss: 0.4269 - acc: 0.7980\n",
      "Epoch 41/50\n",
      "614/614 [==============================] - 0s 63us/step - loss: 0.4260 - acc: 0.7997\n",
      "Epoch 42/50\n",
      "614/614 [==============================] - 0s 82us/step - loss: 0.4265 - acc: 0.7964\n",
      "Epoch 43/50\n",
      "614/614 [==============================] - 0s 54us/step - loss: 0.4399 - acc: 0.7964\n",
      "Epoch 44/50\n",
      "614/614 [==============================] - 0s 58us/step - loss: 0.4178 - acc: 0.8111\n",
      "Epoch 45/50\n",
      "614/614 [==============================] - 0s 54us/step - loss: 0.4341 - acc: 0.7899\n",
      "Epoch 46/50\n",
      "614/614 [==============================] - 0s 58us/step - loss: 0.4401 - acc: 0.7752\n",
      "Epoch 47/50\n",
      "614/614 [==============================] - 0s 55us/step - loss: 0.4186 - acc: 0.8078\n",
      "Epoch 48/50\n",
      "614/614 [==============================] - 0s 65us/step - loss: 0.4241 - acc: 0.8029\n",
      "Epoch 49/50\n",
      "614/614 [==============================] - 0s 54us/step - loss: 0.4229 - acc: 0.8013\n",
      "Epoch 50/50\n",
      "614/614 [==============================] - 0s 59us/step - loss: 0.4254 - acc: 0.7964\n"
     ]
    }
   ],
   "source": [
    "mod1 = model.fit(x_train, y_train, \n",
    "                  batch_size=32,\n",
    "                  epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "154/154 [==============================] - 0s 68us/step\n",
      "[0.63895298753465923, 0.74675324907550567]\n"
     ]
    }
   ],
   "source": [
    "tst_score = model.evaluate(x_test, y_test)\n",
    "print(tst_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "614/614 [==============================] - 0s 35us/step\n",
      "[0.40970837448629571, 0.8208469053433074]\n"
     ]
    }
   ],
   "source": [
    "train_score = model.evaluate(x_train, y_train)\n",
    "print(train_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Have more control over your model hyperparameters\n",
    "Above example showed how to build a sequential model. As you might know already from machine learning, learning rate is an important hyperparameter for taining. In the above example, `.compile()` used the Adam optimizer by calling the optimizer name `'adam'` (string).  \n",
    "```python\n",
    "model.compile(optimizer='adam', loss ='binary_crossentropy', metrics=['accuracy'])\n",
    "```\n",
    "Under the hood, if you call the string 'adam', it calls the optimizer Adam (keras.optimizers.Adam) and uses default options (e.g. learning rate is 0.001). Check the [optimizer](https://keras.io/optimizers/) documentation for more details. Since we'd like to control the learning rate, we can instantiate an adam optimizer (and change the learning rate) by\n",
    "```python\n",
    "adam = optimizers.Adam(lr=0.005)\n",
    "```\n",
    "and use in the `.compile()` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# YOUR TURN\n",
    "# instantiate your previous model again without compiling yet,\n",
    "# Your Answer:\n",
    "### my first model is \"mod1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.callbacks.History object at 0x000001879F0378D0>\n"
     ]
    }
   ],
   "source": [
    "print(mod1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "instantiate your adam optimizer and change the learning rate to 0.01 and compile\n",
    "Your Answer:\n",
    "\n",
    "Note: it's a good idea to wrap models in a function, especially working with jupyter notebook because running cells here and there can mess up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# YOUR TURN\n",
    "# fit the model\n",
    "# Your Answer:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "614/614 [==============================] - 0s 608us/step - loss: 0.6615 - acc: 0.6336\n",
      "Epoch 2/50\n",
      "614/614 [==============================] - 0s 50us/step - loss: 0.6327 - acc: 0.6726\n",
      "Epoch 3/50\n",
      "614/614 [==============================] - 0s 49us/step - loss: 0.6334 - acc: 0.6580\n",
      "Epoch 4/50\n",
      "614/614 [==============================] - 0s 57us/step - loss: 0.6202 - acc: 0.6531\n",
      "Epoch 5/50\n",
      "614/614 [==============================] - 0s 51us/step - loss: 0.6480 - acc: 0.6059\n",
      "Epoch 6/50\n",
      "614/614 [==============================] - 0s 52us/step - loss: 0.6340 - acc: 0.6515\n",
      "Epoch 7/50\n",
      "614/614 [==============================] - 0s 52us/step - loss: 0.6538 - acc: 0.6205\n",
      "Epoch 8/50\n",
      "614/614 [==============================] - 0s 53us/step - loss: 0.6850 - acc: 0.5977\n",
      "Epoch 9/50\n",
      "614/614 [==============================] - 0s 60us/step - loss: 0.6668 - acc: 0.6531\n",
      "Epoch 10/50\n",
      "614/614 [==============================] - 0s 58us/step - loss: 0.6493 - acc: 0.6531\n",
      "Epoch 11/50\n",
      "614/614 [==============================] - 0s 51us/step - loss: 0.6590 - acc: 0.6531\n",
      "Epoch 12/50\n",
      "614/614 [==============================] - 0s 52us/step - loss: 0.6484 - acc: 0.6531\n",
      "Epoch 13/50\n",
      "614/614 [==============================] - 0s 63us/step - loss: 0.6520 - acc: 0.6531\n",
      "Epoch 14/50\n",
      "614/614 [==============================] - 0s 58us/step - loss: 0.6500 - acc: 0.6531\n",
      "Epoch 15/50\n",
      "614/614 [==============================] - 0s 55us/step - loss: 0.6641 - acc: 0.6531\n",
      "Epoch 16/50\n",
      "614/614 [==============================] - 0s 56us/step - loss: 0.6490 - acc: 0.6531\n",
      "Epoch 17/50\n",
      "614/614 [==============================] - 0s 53us/step - loss: 0.6542 - acc: 0.6531\n",
      "Epoch 18/50\n",
      "614/614 [==============================] - 0s 48us/step - loss: 0.6617 - acc: 0.6531\n",
      "Epoch 19/50\n",
      "614/614 [==============================] - 0s 51us/step - loss: 0.6465 - acc: 0.6531\n",
      "Epoch 20/50\n",
      "614/614 [==============================] - 0s 53us/step - loss: 0.6477 - acc: 0.6531\n",
      "Epoch 21/50\n",
      "614/614 [==============================] - 0s 51us/step - loss: 0.6505 - acc: 0.6531\n",
      "Epoch 22/50\n",
      "614/614 [==============================] - 0s 57us/step - loss: 0.6455 - acc: 0.6531\n",
      "Epoch 23/50\n",
      "614/614 [==============================] - 0s 50us/step - loss: 0.6520 - acc: 0.6531\n",
      "Epoch 24/50\n",
      "614/614 [==============================] - 0s 49us/step - loss: 0.6561 - acc: 0.6531\n",
      "Epoch 25/50\n",
      "614/614 [==============================] - 0s 53us/step - loss: 0.6510 - acc: 0.6531\n",
      "Epoch 26/50\n",
      "614/614 [==============================] - 0s 57us/step - loss: 0.6532 - acc: 0.6531\n",
      "Epoch 27/50\n",
      "614/614 [==============================] - 0s 57us/step - loss: 0.6481 - acc: 0.6531\n",
      "Epoch 28/50\n",
      "614/614 [==============================] - 0s 52us/step - loss: 0.6606 - acc: 0.6531\n",
      "Epoch 29/50\n",
      "614/614 [==============================] - 0s 51us/step - loss: 0.6505 - acc: 0.6531\n",
      "Epoch 30/50\n",
      "614/614 [==============================] - 0s 56us/step - loss: 0.6526 - acc: 0.6531\n",
      "Epoch 31/50\n",
      "614/614 [==============================] - 0s 43us/step - loss: 0.6643 - acc: 0.6531\n",
      "Epoch 32/50\n",
      "614/614 [==============================] - 0s 62us/step - loss: 0.6514 - acc: 0.6531\n",
      "Epoch 33/50\n",
      "614/614 [==============================] - 0s 52us/step - loss: 0.6485 - acc: 0.6531\n",
      "Epoch 34/50\n",
      "614/614 [==============================] - 0s 56us/step - loss: 0.6471 - acc: 0.6531\n",
      "Epoch 35/50\n",
      "614/614 [==============================] - 0s 52us/step - loss: 0.6543 - acc: 0.6531\n",
      "Epoch 36/50\n",
      "614/614 [==============================] - 0s 51us/step - loss: 0.6522 - acc: 0.6531\n",
      "Epoch 37/50\n",
      "614/614 [==============================] - 0s 61us/step - loss: 0.6535 - acc: 0.6531\n",
      "Epoch 38/50\n",
      "614/614 [==============================] - 0s 52us/step - loss: 0.6526 - acc: 0.6531\n",
      "Epoch 39/50\n",
      "614/614 [==============================] - 0s 44us/step - loss: 0.6569 - acc: 0.6531\n",
      "Epoch 40/50\n",
      "614/614 [==============================] - 0s 62us/step - loss: 0.6470 - acc: 0.6531\n",
      "Epoch 41/50\n",
      "614/614 [==============================] - 0s 53us/step - loss: 0.6546 - acc: 0.6531\n",
      "Epoch 42/50\n",
      "614/614 [==============================] - 0s 58us/step - loss: 0.6587 - acc: 0.6531\n",
      "Epoch 43/50\n",
      "614/614 [==============================] - 0s 59us/step - loss: 0.6489 - acc: 0.6531\n",
      "Epoch 44/50\n",
      "614/614 [==============================] - 0s 56us/step - loss: 0.6530 - acc: 0.6531\n",
      "Epoch 45/50\n",
      "614/614 [==============================] - 0s 57us/step - loss: 0.6569 - acc: 0.6531\n",
      "Epoch 46/50\n",
      "614/614 [==============================] - 0s 44us/step - loss: 0.6587 - acc: 0.6531\n",
      "Epoch 47/50\n",
      "614/614 [==============================] - 0s 54us/step - loss: 0.6510 - acc: 0.6531\n",
      "Epoch 48/50\n",
      "614/614 [==============================] - 0s 52us/step - loss: 0.6557 - acc: 0.6531\n",
      "Epoch 49/50\n",
      "614/614 [==============================] - 0s 60us/step - loss: 0.6509 - acc: 0.6531\n",
      "Epoch 50/50\n",
      "614/614 [==============================] - 0s 58us/step - loss: 0.6498 - acc: 0.6531\n"
     ]
    }
   ],
   "source": [
    "K.clear_session()\n",
    "adamB = optimizers.Adam(lr=0.005)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(64, activation='relu', input_dim=8)) \n",
    "model.add(Dense(32, activation='tanh'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer=(optimizers.Adam(lr=0.01)),\n",
    "              loss ='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "mod2 = model.fit(x_train, y_train, \n",
    "                  batch_size=32,\n",
    "                  epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "154/154 [==============================] - 0s 195us/step\n",
      "[0.65613707396891208, 0.64285714208305655]\n"
     ]
    }
   ],
   "source": [
    "tst_score = model.evaluate(x_test, y_test)\n",
    "print(tst_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "614/614 [==============================] - 0s 34us/step\n",
      "[0.64817874206393855, 0.6530944627348686]\n"
     ]
    }
   ],
   "source": [
    "train_score = model.evaluate(x_train, y_train)\n",
    "print(train_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "YOUR TURN\n",
    "\n",
    "# Using your model(s) above, tweak learning rate, epoch, and batch size\n",
    "# Q3. Observe the relationship between the learning rate and epoch. \n",
    "## Q3a. If you reduce the learning rate, what happens to the epochs needed to achieve the same accuracy?\n",
    "- lr=0.00001 loss ~0.59 accuracy ~66%\n",
    "- lr=.0000000001 loss ~ 0.96 caused very low accuracy (~38%)\n",
    "## Q3b. If you increase the learning rate, at some point does it fail?\n",
    "- lr=0.9 causes loss ~5.0 accuracy ~65%\n",
    "- lr= 10 causes loss ~10.4 accuracy ~34%\n",
    "- lr=1000000000 still did not fail\n",
    "- lr=100000000000000000000 - loss: nan - acc: 0.0244 for first epoch then fails with  acc: 0.0000e+00 for all other epochs\n",
    "## Q3c. Find a sweet spot (may not be unique)\n",
    "- lr=.00010 loss: 0.4976 - acc: 0.7720\n",
    "- lr=.00032 loss: 0.4732 - acc: 0.7834\n",
    "- lr=.00040 loss: 0.4504 - acc: 0.7980\n",
    "- lr=.00045 loss: 0.3916 - acc: 0.8371\n",
    "- lr=.00050 loss: 0.4269 - acc: 0.8290\n",
    "- lr=.00055 loss: 0.3927 - acc: 0.8127\n",
    "- lr=.00075 loss: 0.4600 - acc: 0.7752\n",
    "### Learning rate of .0045 gave me the best accuracy with 83.71%\n",
    "\n",
    "# Q4. Change the number of elements in layers and add more layers. Adjust learning rate, and other hyperparameters as necessary.\n",
    "## Q4a. What is your best configuation?\n",
    "## Q4b. What is your best train accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "614/614 [==============================] - 0s 787us/step - loss: 0.6847 - acc: 0.5977\n",
      "Epoch 2/200\n",
      "614/614 [==============================] - 0s 58us/step - loss: 0.6185 - acc: 0.6515\n",
      "Epoch 3/200\n",
      "614/614 [==============================] - 0s 56us/step - loss: 0.5948 - acc: 0.6612\n",
      "Epoch 4/200\n",
      "614/614 [==============================] - 0s 56us/step - loss: 0.5832 - acc: 0.6792\n",
      "Epoch 5/200\n",
      "614/614 [==============================] - 0s 65us/step - loss: 0.5726 - acc: 0.6987\n",
      "Epoch 6/200\n",
      "614/614 [==============================] - 0s 58us/step - loss: 0.5638 - acc: 0.7117\n",
      "Epoch 7/200\n",
      "614/614 [==============================] - 0s 64us/step - loss: 0.5614 - acc: 0.6954\n",
      "Epoch 8/200\n",
      "614/614 [==============================] - 0s 63us/step - loss: 0.5557 - acc: 0.7166\n",
      "Epoch 9/200\n",
      "614/614 [==============================] - 0s 57us/step - loss: 0.5563 - acc: 0.7101\n",
      "Epoch 10/200\n",
      "614/614 [==============================] - 0s 69us/step - loss: 0.5650 - acc: 0.7020\n",
      "Epoch 11/200\n",
      "614/614 [==============================] - 0s 56us/step - loss: 0.5433 - acc: 0.7329\n",
      "Epoch 12/200\n",
      "614/614 [==============================] - 0s 65us/step - loss: 0.5406 - acc: 0.7248\n",
      "Epoch 13/200\n",
      "614/614 [==============================] - 0s 56us/step - loss: 0.5400 - acc: 0.7231\n",
      "Epoch 14/200\n",
      "614/614 [==============================] - 0s 64us/step - loss: 0.5343 - acc: 0.7248\n",
      "Epoch 15/200\n",
      "614/614 [==============================] - 0s 57us/step - loss: 0.5359 - acc: 0.7215\n",
      "Epoch 16/200\n",
      "614/614 [==============================] - 0s 60us/step - loss: 0.5259 - acc: 0.7150\n",
      "Epoch 17/200\n",
      "614/614 [==============================] - 0s 63us/step - loss: 0.5275 - acc: 0.7313\n",
      "Epoch 18/200\n",
      "614/614 [==============================] - 0s 60us/step - loss: 0.5181 - acc: 0.7362\n",
      "Epoch 19/200\n",
      "614/614 [==============================] - 0s 60us/step - loss: 0.5225 - acc: 0.7394\n",
      "Epoch 20/200\n",
      "614/614 [==============================] - 0s 68us/step - loss: 0.5110 - acc: 0.7492\n",
      "Epoch 21/200\n",
      "614/614 [==============================] - 0s 50us/step - loss: 0.5138 - acc: 0.7345\n",
      "Epoch 22/200\n",
      "614/614 [==============================] - 0s 76us/step - loss: 0.5054 - acc: 0.7476\n",
      "Epoch 23/200\n",
      "614/614 [==============================] - 0s 62us/step - loss: 0.5002 - acc: 0.7443\n",
      "Epoch 24/200\n",
      "614/614 [==============================] - 0s 49us/step - loss: 0.4991 - acc: 0.7459\n",
      "Epoch 25/200\n",
      "614/614 [==============================] - 0s 57us/step - loss: 0.5004 - acc: 0.7313\n",
      "Epoch 26/200\n",
      "614/614 [==============================] - 0s 59us/step - loss: 0.5091 - acc: 0.7410\n",
      "Epoch 27/200\n",
      "614/614 [==============================] - 0s 59us/step - loss: 0.4880 - acc: 0.7671\n",
      "Epoch 28/200\n",
      "614/614 [==============================] - 0s 58us/step - loss: 0.4886 - acc: 0.7508\n",
      "Epoch 29/200\n",
      "614/614 [==============================] - 0s 66us/step - loss: 0.4942 - acc: 0.7394\n",
      "Epoch 30/200\n",
      "614/614 [==============================] - 0s 57us/step - loss: 0.4852 - acc: 0.7427\n",
      "Epoch 31/200\n",
      "614/614 [==============================] - 0s 64us/step - loss: 0.4735 - acc: 0.7655\n",
      "Epoch 32/200\n",
      "614/614 [==============================] - 0s 63us/step - loss: 0.4730 - acc: 0.7704\n",
      "Epoch 33/200\n",
      "614/614 [==============================] - 0s 62us/step - loss: 0.4740 - acc: 0.7590\n",
      "Epoch 34/200\n",
      "614/614 [==============================] - 0s 58us/step - loss: 0.4783 - acc: 0.7671\n",
      "Epoch 35/200\n",
      "614/614 [==============================] - 0s 59us/step - loss: 0.4655 - acc: 0.7736\n",
      "Epoch 36/200\n",
      "614/614 [==============================] - 0s 63us/step - loss: 0.4721 - acc: 0.7638\n",
      "Epoch 37/200\n",
      "614/614 [==============================] - 0s 67us/step - loss: 0.4587 - acc: 0.7834\n",
      "Epoch 38/200\n",
      "614/614 [==============================] - 0s 73us/step - loss: 0.4551 - acc: 0.7769\n",
      "Epoch 39/200\n",
      "614/614 [==============================] - 0s 63us/step - loss: 0.4606 - acc: 0.7622\n",
      "Epoch 40/200\n",
      "614/614 [==============================] - 0s 56us/step - loss: 0.4852 - acc: 0.7655\n",
      "Epoch 41/200\n",
      "614/614 [==============================] - 0s 58us/step - loss: 0.5117 - acc: 0.7248\n",
      "Epoch 42/200\n",
      "614/614 [==============================] - 0s 58us/step - loss: 0.4644 - acc: 0.7785\n",
      "Epoch 43/200\n",
      "614/614 [==============================] - 0s 58us/step - loss: 0.4450 - acc: 0.7997\n",
      "Epoch 44/200\n",
      "614/614 [==============================] - 0s 58us/step - loss: 0.4516 - acc: 0.7736\n",
      "Epoch 45/200\n",
      "614/614 [==============================] - 0s 59us/step - loss: 0.4437 - acc: 0.7834\n",
      "Epoch 46/200\n",
      "614/614 [==============================] - 0s 62us/step - loss: 0.4467 - acc: 0.7915\n",
      "Epoch 47/200\n",
      "614/614 [==============================] - 0s 71us/step - loss: 0.4417 - acc: 0.7932\n",
      "Epoch 48/200\n",
      "614/614 [==============================] - 0s 63us/step - loss: 0.4422 - acc: 0.7932\n",
      "Epoch 49/200\n",
      "614/614 [==============================] - 0s 61us/step - loss: 0.4298 - acc: 0.8029\n",
      "Epoch 50/200\n",
      "614/614 [==============================] - 0s 57us/step - loss: 0.4685 - acc: 0.7752\n",
      "Epoch 51/200\n",
      "614/614 [==============================] - 0s 66us/step - loss: 0.4523 - acc: 0.7834\n",
      "Epoch 52/200\n",
      "614/614 [==============================] - 0s 58us/step - loss: 0.4344 - acc: 0.7932\n",
      "Epoch 53/200\n",
      "614/614 [==============================] - 0s 61us/step - loss: 0.4178 - acc: 0.7948\n",
      "Epoch 54/200\n",
      "614/614 [==============================] - 0s 58us/step - loss: 0.4118 - acc: 0.8127\n",
      "Epoch 55/200\n",
      "614/614 [==============================] - 0s 58us/step - loss: 0.4070 - acc: 0.8029\n",
      "Epoch 56/200\n",
      "614/614 [==============================] - 0s 67us/step - loss: 0.4307 - acc: 0.7964\n",
      "Epoch 57/200\n",
      "614/614 [==============================] - 0s 64us/step - loss: 0.4180 - acc: 0.8094\n",
      "Epoch 58/200\n",
      "614/614 [==============================] - 0s 63us/step - loss: 0.4221 - acc: 0.7866\n",
      "Epoch 59/200\n",
      "614/614 [==============================] - 0s 64us/step - loss: 0.4051 - acc: 0.8062\n",
      "Epoch 60/200\n",
      "614/614 [==============================] - 0s 49us/step - loss: 0.4186 - acc: 0.8062\n",
      "Epoch 61/200\n",
      "614/614 [==============================] - 0s 64us/step - loss: 0.4202 - acc: 0.8029\n",
      "Epoch 62/200\n",
      "614/614 [==============================] - 0s 64us/step - loss: 0.4224 - acc: 0.8013\n",
      "Epoch 63/200\n",
      "614/614 [==============================] - 0s 62us/step - loss: 0.4047 - acc: 0.8046\n",
      "Epoch 64/200\n",
      "614/614 [==============================] - 0s 56us/step - loss: 0.3912 - acc: 0.8143\n",
      "Epoch 65/200\n",
      "614/614 [==============================] - 0s 65us/step - loss: 0.4256 - acc: 0.7769\n",
      "Epoch 66/200\n",
      "614/614 [==============================] - 0s 69us/step - loss: 0.4078 - acc: 0.8029\n",
      "Epoch 67/200\n",
      "614/614 [==============================] - 0s 59us/step - loss: 0.3907 - acc: 0.8208\n",
      "Epoch 68/200\n",
      "614/614 [==============================] - 0s 54us/step - loss: 0.3923 - acc: 0.8208\n",
      "Epoch 69/200\n",
      "614/614 [==============================] - 0s 61us/step - loss: 0.4158 - acc: 0.8078\n",
      "Epoch 70/200\n",
      "614/614 [==============================] - 0s 66us/step - loss: 0.3960 - acc: 0.8046\n",
      "Epoch 71/200\n",
      "614/614 [==============================] - 0s 64us/step - loss: 0.3970 - acc: 0.8143\n",
      "Epoch 72/200\n",
      "614/614 [==============================] - 0s 61us/step - loss: 0.3970 - acc: 0.8111\n",
      "Epoch 73/200\n",
      "614/614 [==============================] - 0s 60us/step - loss: 0.3840 - acc: 0.8241\n",
      "Epoch 74/200\n",
      "614/614 [==============================] - 0s 60us/step - loss: 0.3999 - acc: 0.7915\n",
      "Epoch 75/200\n",
      "614/614 [==============================] - 0s 62us/step - loss: 0.3938 - acc: 0.8160\n",
      "Epoch 76/200\n",
      "614/614 [==============================] - 0s 64us/step - loss: 0.3747 - acc: 0.8176\n",
      "Epoch 77/200\n",
      "614/614 [==============================] - 0s 58us/step - loss: 0.3791 - acc: 0.8143\n",
      "Epoch 78/200\n",
      "614/614 [==============================] - 0s 61us/step - loss: 0.3693 - acc: 0.8306\n",
      "Epoch 79/200\n",
      "614/614 [==============================] - 0s 59us/step - loss: 0.3784 - acc: 0.8111\n",
      "Epoch 80/200\n",
      "614/614 [==============================] - 0s 59us/step - loss: 0.3648 - acc: 0.8274\n",
      "Epoch 81/200\n",
      "614/614 [==============================] - 0s 58us/step - loss: 0.3572 - acc: 0.8306\n",
      "Epoch 82/200\n",
      "614/614 [==============================] - 0s 59us/step - loss: 0.3856 - acc: 0.8257\n",
      "Epoch 83/200\n",
      "614/614 [==============================] - 0s 59us/step - loss: 0.3657 - acc: 0.8290\n",
      "Epoch 84/200\n",
      "614/614 [==============================] - 0s 57us/step - loss: 0.3502 - acc: 0.8453\n",
      "Epoch 85/200\n",
      "614/614 [==============================] - 0s 60us/step - loss: 0.3510 - acc: 0.8469\n",
      "Epoch 86/200\n",
      "614/614 [==============================] - 0s 61us/step - loss: 0.3556 - acc: 0.8355\n",
      "Epoch 87/200\n",
      "614/614 [==============================] - 0s 60us/step - loss: 0.3660 - acc: 0.8436\n",
      "Epoch 88/200\n",
      "614/614 [==============================] - 0s 51us/step - loss: 0.3546 - acc: 0.8404\n",
      "Epoch 89/200\n",
      "614/614 [==============================] - 0s 60us/step - loss: 0.3410 - acc: 0.8469\n",
      "Epoch 90/200\n",
      "614/614 [==============================] - 0s 56us/step - loss: 0.3414 - acc: 0.8420\n",
      "Epoch 91/200\n",
      "614/614 [==============================] - 0s 52us/step - loss: 0.3434 - acc: 0.8583\n",
      "Epoch 92/200\n",
      "614/614 [==============================] - 0s 61us/step - loss: 0.3562 - acc: 0.8257\n",
      "Epoch 93/200\n",
      "614/614 [==============================] - 0s 55us/step - loss: 0.3661 - acc: 0.8339\n",
      "Epoch 94/200\n",
      "614/614 [==============================] - 0s 65us/step - loss: 0.3937 - acc: 0.8176\n",
      "Epoch 95/200\n",
      "614/614 [==============================] - 0s 63us/step - loss: 0.3591 - acc: 0.8290\n",
      "Epoch 96/200\n",
      "614/614 [==============================] - 0s 60us/step - loss: 0.3594 - acc: 0.8322\n",
      "Epoch 97/200\n",
      "614/614 [==============================] - 0s 65us/step - loss: 0.3530 - acc: 0.8371\n",
      "Epoch 98/200\n",
      "614/614 [==============================] - 0s 63us/step - loss: 0.3425 - acc: 0.8257\n",
      "Epoch 99/200\n",
      "614/614 [==============================] - 0s 67us/step - loss: 0.3432 - acc: 0.8502\n",
      "Epoch 100/200\n",
      "614/614 [==============================] - 0s 60us/step - loss: 0.3311 - acc: 0.8518\n",
      "Epoch 101/200\n",
      "614/614 [==============================] - 0s 65us/step - loss: 0.3221 - acc: 0.8567\n",
      "Epoch 102/200\n",
      "614/614 [==============================] - 0s 63us/step - loss: 0.3276 - acc: 0.8567\n",
      "Epoch 103/200\n",
      "614/614 [==============================] - 0s 64us/step - loss: 0.3287 - acc: 0.8485\n",
      "Epoch 104/200\n",
      "614/614 [==============================] - ETA: 0s - loss: 0.3956 - acc: 0.750 - 0s 61us/step - loss: 0.3213 - acc: 0.8502\n",
      "Epoch 105/200\n",
      "614/614 [==============================] - 0s 60us/step - loss: 0.3094 - acc: 0.8648\n",
      "Epoch 106/200\n",
      "614/614 [==============================] - 0s 66us/step - loss: 0.3217 - acc: 0.8681\n",
      "Epoch 107/200\n",
      "614/614 [==============================] - 0s 64us/step - loss: 0.3305 - acc: 0.8404\n",
      "Epoch 108/200\n",
      "614/614 [==============================] - 0s 61us/step - loss: 0.3449 - acc: 0.8388\n",
      "Epoch 109/200\n",
      "614/614 [==============================] - 0s 70us/step - loss: 0.3627 - acc: 0.8339\n",
      "Epoch 110/200\n",
      "614/614 [==============================] - 0s 55us/step - loss: 0.3413 - acc: 0.8420\n",
      "Epoch 111/200\n",
      "614/614 [==============================] - 0s 69us/step - loss: 0.3155 - acc: 0.8583\n",
      "Epoch 112/200\n",
      "614/614 [==============================] - 0s 59us/step - loss: 0.3180 - acc: 0.8632\n",
      "Epoch 113/200\n",
      "614/614 [==============================] - 0s 60us/step - loss: 0.3389 - acc: 0.8371\n",
      "Epoch 114/200\n",
      "614/614 [==============================] - 0s 63us/step - loss: 0.3560 - acc: 0.8241\n",
      "Epoch 115/200\n",
      "614/614 [==============================] - 0s 66us/step - loss: 0.3321 - acc: 0.8534\n",
      "Epoch 116/200\n",
      "614/614 [==============================] - 0s 56us/step - loss: 0.3279 - acc: 0.8550\n",
      "Epoch 117/200\n",
      "614/614 [==============================] - 0s 58us/step - loss: 0.3093 - acc: 0.8632\n",
      "Epoch 118/200\n",
      "614/614 [==============================] - 0s 66us/step - loss: 0.3192 - acc: 0.8583\n",
      "Epoch 119/200\n",
      "614/614 [==============================] - 0s 57us/step - loss: 0.3077 - acc: 0.8664\n",
      "Epoch 120/200\n",
      "614/614 [==============================] - 0s 63us/step - loss: 0.3361 - acc: 0.8469\n",
      "Epoch 121/200\n",
      "614/614 [==============================] - 0s 56us/step - loss: 0.3370 - acc: 0.8371\n",
      "Epoch 122/200\n",
      "614/614 [==============================] - 0s 63us/step - loss: 0.3229 - acc: 0.8404\n",
      "Epoch 123/200\n",
      "614/614 [==============================] - 0s 60us/step - loss: 0.3327 - acc: 0.8388\n",
      "Epoch 124/200\n",
      "614/614 [==============================] - 0s 72us/step - loss: 0.3238 - acc: 0.8567\n",
      "Epoch 125/200\n",
      "614/614 [==============================] - 0s 66us/step - loss: 0.3291 - acc: 0.8502\n",
      "Epoch 126/200\n",
      "614/614 [==============================] - 0s 52us/step - loss: 0.3304 - acc: 0.8469\n",
      "Epoch 127/200\n",
      "614/614 [==============================] - 0s 60us/step - loss: 0.3354 - acc: 0.8453\n",
      "Epoch 128/200\n",
      "614/614 [==============================] - 0s 52us/step - loss: 0.3207 - acc: 0.8518\n",
      "Epoch 129/200\n",
      "614/614 [==============================] - 0s 53us/step - loss: 0.2978 - acc: 0.8648\n",
      "Epoch 130/200\n",
      "614/614 [==============================] - 0s 63us/step - loss: 0.3156 - acc: 0.8436\n",
      "Epoch 131/200\n",
      "614/614 [==============================] - 0s 55us/step - loss: 0.2913 - acc: 0.8697\n",
      "Epoch 132/200\n",
      "614/614 [==============================] - 0s 66us/step - loss: 0.2943 - acc: 0.8599\n",
      "Epoch 133/200\n",
      "614/614 [==============================] - 0s 57us/step - loss: 0.3222 - acc: 0.8616\n",
      "Epoch 134/200\n",
      "614/614 [==============================] - 0s 64us/step - loss: 0.3178 - acc: 0.8469\n",
      "Epoch 135/200\n",
      "614/614 [==============================] - 0s 48us/step - loss: 0.3228 - acc: 0.8436\n",
      "Epoch 136/200\n",
      "614/614 [==============================] - 0s 63us/step - loss: 0.3045 - acc: 0.8616\n",
      "Epoch 137/200\n",
      "614/614 [==============================] - 0s 58us/step - loss: 0.2902 - acc: 0.8616\n",
      "Epoch 138/200\n",
      "614/614 [==============================] - 0s 58us/step - loss: 0.2853 - acc: 0.8664\n",
      "Epoch 139/200\n",
      "614/614 [==============================] - 0s 66us/step - loss: 0.2870 - acc: 0.8730\n",
      "Epoch 140/200\n",
      "614/614 [==============================] - 0s 58us/step - loss: 0.3104 - acc: 0.8681\n",
      "Epoch 141/200\n",
      "614/614 [==============================] - 0s 59us/step - loss: 0.2785 - acc: 0.8795\n",
      "Epoch 142/200\n",
      "614/614 [==============================] - 0s 64us/step - loss: 0.2779 - acc: 0.8762\n",
      "Epoch 143/200\n",
      "614/614 [==============================] - 0s 56us/step - loss: 0.2952 - acc: 0.8697\n",
      "Epoch 144/200\n",
      "614/614 [==============================] - 0s 57us/step - loss: 0.3040 - acc: 0.8616\n",
      "Epoch 145/200\n",
      "614/614 [==============================] - 0s 65us/step - loss: 0.2803 - acc: 0.8713\n",
      "Epoch 146/200\n",
      "614/614 [==============================] - 0s 56us/step - loss: 0.2810 - acc: 0.8811\n",
      "Epoch 147/200\n",
      "614/614 [==============================] - 0s 53us/step - loss: 0.2890 - acc: 0.8746\n",
      "Epoch 148/200\n",
      "614/614 [==============================] - 0s 58us/step - loss: 0.2984 - acc: 0.8681\n",
      "Epoch 149/200\n",
      "614/614 [==============================] - 0s 64us/step - loss: 0.3252 - acc: 0.8469\n",
      "Epoch 150/200\n",
      "614/614 [==============================] - 0s 57us/step - loss: 0.3010 - acc: 0.8599\n",
      "Epoch 151/200\n",
      "614/614 [==============================] - 0s 69us/step - loss: 0.2975 - acc: 0.8599\n",
      "Epoch 152/200\n",
      "614/614 [==============================] - 0s 58us/step - loss: 0.2821 - acc: 0.8664\n",
      "Epoch 153/200\n",
      "614/614 [==============================] - 0s 56us/step - loss: 0.2734 - acc: 0.8827\n",
      "Epoch 154/200\n",
      "614/614 [==============================] - 0s 56us/step - loss: 0.2915 - acc: 0.8616\n",
      "Epoch 155/200\n",
      "614/614 [==============================] - 0s 58us/step - loss: 0.3088 - acc: 0.8583\n",
      "Epoch 156/200\n",
      "614/614 [==============================] - 0s 58us/step - loss: 0.2958 - acc: 0.8534\n",
      "Epoch 157/200\n",
      "614/614 [==============================] - 0s 51us/step - loss: 0.2690 - acc: 0.8925\n",
      "Epoch 158/200\n",
      "614/614 [==============================] - 0s 56us/step - loss: 0.2625 - acc: 0.8730\n",
      "Epoch 159/200\n",
      "614/614 [==============================] - 0s 57us/step - loss: 0.2502 - acc: 0.8925\n",
      "Epoch 160/200\n",
      "614/614 [==============================] - 0s 57us/step - loss: 0.2661 - acc: 0.8713\n",
      "Epoch 161/200\n",
      "614/614 [==============================] - 0s 66us/step - loss: 0.2651 - acc: 0.8697\n",
      "Epoch 162/200\n",
      "614/614 [==============================] - 0s 60us/step - loss: 0.2691 - acc: 0.8860\n",
      "Epoch 163/200\n",
      "614/614 [==============================] - 0s 60us/step - loss: 0.2843 - acc: 0.8713\n",
      "Epoch 164/200\n",
      "614/614 [==============================] - 0s 57us/step - loss: 0.2809 - acc: 0.8664\n",
      "Epoch 165/200\n",
      "614/614 [==============================] - 0s 59us/step - loss: 0.2671 - acc: 0.8795\n",
      "Epoch 166/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "614/614 [==============================] - 0s 57us/step - loss: 0.2551 - acc: 0.9121\n",
      "Epoch 167/200\n",
      "614/614 [==============================] - 0s 56us/step - loss: 0.2758 - acc: 0.8697\n",
      "Epoch 168/200\n",
      "614/614 [==============================] - 0s 56us/step - loss: 0.2897 - acc: 0.8697\n",
      "Epoch 169/200\n",
      "614/614 [==============================] - 0s 75us/step - loss: 0.2594 - acc: 0.8909\n",
      "Epoch 170/200\n",
      "614/614 [==============================] - 0s 62us/step - loss: 0.2564 - acc: 0.8795\n",
      "Epoch 171/200\n",
      "614/614 [==============================] - 0s 60us/step - loss: 0.2972 - acc: 0.8697\n",
      "Epoch 172/200\n",
      "614/614 [==============================] - 0s 70us/step - loss: 0.3161 - acc: 0.8599\n",
      "Epoch 173/200\n",
      "614/614 [==============================] - 0s 69us/step - loss: 0.3159 - acc: 0.8518\n",
      "Epoch 174/200\n",
      "614/614 [==============================] - 0s 64us/step - loss: 0.2842 - acc: 0.8616\n",
      "Epoch 175/200\n",
      "614/614 [==============================] - 0s 65us/step - loss: 0.2643 - acc: 0.8795\n",
      "Epoch 176/200\n",
      "614/614 [==============================] - 0s 55us/step - loss: 0.2584 - acc: 0.8779\n",
      "Epoch 177/200\n",
      "614/614 [==============================] - 0s 64us/step - loss: 0.2470 - acc: 0.8941\n",
      "Epoch 178/200\n",
      "614/614 [==============================] - 0s 59us/step - loss: 0.2367 - acc: 0.8990\n",
      "Epoch 179/200\n",
      "614/614 [==============================] - 0s 64us/step - loss: 0.2313 - acc: 0.8974\n",
      "Epoch 180/200\n",
      "614/614 [==============================] - 0s 67us/step - loss: 0.2584 - acc: 0.8876\n",
      "Epoch 181/200\n",
      "614/614 [==============================] - 0s 54us/step - loss: 0.2409 - acc: 0.8925\n",
      "Epoch 182/200\n",
      "614/614 [==============================] - 0s 61us/step - loss: 0.2387 - acc: 0.8909\n",
      "Epoch 183/200\n",
      "614/614 [==============================] - 0s 63us/step - loss: 0.2364 - acc: 0.9023\n",
      "Epoch 184/200\n",
      "614/614 [==============================] - 0s 51us/step - loss: 0.2313 - acc: 0.8909\n",
      "Epoch 185/200\n",
      "614/614 [==============================] - 0s 56us/step - loss: 0.2255 - acc: 0.9055\n",
      "Epoch 186/200\n",
      "614/614 [==============================] - 0s 57us/step - loss: 0.2655 - acc: 0.8925\n",
      "Epoch 187/200\n",
      "614/614 [==============================] - 0s 67us/step - loss: 0.2694 - acc: 0.8795\n",
      "Epoch 188/200\n",
      "614/614 [==============================] - 0s 57us/step - loss: 0.2389 - acc: 0.8795\n",
      "Epoch 189/200\n",
      "614/614 [==============================] - 0s 53us/step - loss: 0.2224 - acc: 0.9007\n",
      "Epoch 190/200\n",
      "614/614 [==============================] - 0s 69us/step - loss: 0.2427 - acc: 0.8893\n",
      "Epoch 191/200\n",
      "614/614 [==============================] - 0s 61us/step - loss: 0.2329 - acc: 0.9039\n",
      "Epoch 192/200\n",
      "614/614 [==============================] - 0s 63us/step - loss: 0.2359 - acc: 0.9007\n",
      "Epoch 193/200\n",
      "614/614 [==============================] - 0s 58us/step - loss: 0.2473 - acc: 0.8860\n",
      "Epoch 194/200\n",
      "614/614 [==============================] - 0s 62us/step - loss: 0.2612 - acc: 0.8844\n",
      "Epoch 195/200\n",
      "614/614 [==============================] - 0s 62us/step - loss: 0.2586 - acc: 0.8925\n",
      "Epoch 196/200\n",
      "614/614 [==============================] - 0s 56us/step - loss: 0.2338 - acc: 0.8990\n",
      "Epoch 197/200\n",
      "614/614 [==============================] - 0s 63us/step - loss: 0.2398 - acc: 0.9055\n",
      "Epoch 198/200\n",
      "614/614 [==============================] - 0s 59us/step - loss: 0.2163 - acc: 0.9055\n",
      "Epoch 199/200\n",
      "614/614 [==============================] - 0s 52us/step - loss: 0.2227 - acc: 0.9023\n",
      "Epoch 200/200\n",
      "614/614 [==============================] - 0s 74us/step - loss: 0.2157 - acc: 0.9088\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(64, activation='relu', input_dim=8)) \n",
    "model.add(Dense(64, activation='tanh'))\n",
    "model.add(Dense(36, activation='relu'))\n",
    "model.add(Dense(36, activation='tanh'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer=(optimizers.Adam(lr=.00045)),\n",
    "              loss ='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "mod2 = model.fit(x_train, y_train, \n",
    "                  batch_size=32,\n",
    "                  epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "154/154 [==============================] - 0s 256us/step\n",
      "[1.0593324535852904, 0.64935065244699453]\n"
     ]
    }
   ],
   "source": [
    "tst_score = model.evaluate(x_test, y_test)\n",
    "print(tst_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "614/614 [==============================] - 0s 35us/step\n",
      "[0.19721443140545575, 0.91368078175895762]\n"
     ]
    }
   ],
   "source": [
    "train_score = model.evaluate(x_train, y_train)\n",
    "print(train_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The above model gives me a great train accuracy with 91.3% but the test accuracy is only 64.9% so it is over-fit :("
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(154,) (154,)\n"
     ]
    }
   ],
   "source": [
    "Y_pred = model.predict(x_test) #prediction probability \n",
    "Y_class = Y_pred.round().reshape(y_test.shape) # round up probabilities and reshape the array\n",
    "print(y_test.shape, Y_class.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def acc(yp, yt):\n",
    "    assert yp.shape==yt.shape, 'shape of yp and yt do not match'\n",
    "    return np.mean(yp == yt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# YOUR TURN\n",
    "# print out the test accuracy \n",
    "# Your Answer:\n",
    "\n",
    "# Q5. Is the test accuracy as good as the train accuracy? \n",
    "# Your Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I showed how to find train accuracy with model.evaluate above, however here is another way with sklearn and how to get a confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(Y_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.64935064935064934"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test, Y_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[74 25]\n",
      " [29 26]]\n"
     ]
    }
   ],
   "source": [
    "#confusion matrix\n",
    "\n",
    "#sklearn.metrics.confusion_matrix(y_true, y_pred, \n",
    "#labels=None, sample_weight=None)\n",
    "import sklearn\n",
    "cnf_matrix = sklearn.metrics.confusion_matrix(y_test,\n",
    "                                              Y_class,\n",
    "                                              labels=None, \n",
    "                                              sample_weight=None)\n",
    "print(cnf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.64393135895134757"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#precision\n",
    "\n",
    "from sklearn.metrics import precision_score\n",
    "precision_score(y_test, Y_class, average='weighted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitor validation loss\n",
    "You can also monitor validation loss and validation metric (accuracy in our example) by adding an option ` validation_split` to the `.fit()` function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " YOUR TURN\n",
    "## Refer to Keras documentation and add validation_split to .fit() and run training\n",
    " Your Answer: see below\n",
    "\n",
    "## Q6. What do you observe in train and validation accuraceis? Is there an overfit?\n",
    " Your Answer:\n",
    " ### Yes, there is an overfit because the train accuracy is higher than the validation by over 10%\n",
    "\n",
    "## Q7. Change your hyperparameters to maximize your validation accuracy. What are your best configuations?\n",
    " Your Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 491 samples, validate on 123 samples\n",
      "Epoch 1/200\n",
      "491/491 [==============================] - 0s 976us/step - loss: 0.6983 - acc: 0.5804 - val_loss: 0.6709 - val_acc: 0.6098\n",
      "Epoch 2/200\n",
      "491/491 [==============================] - 0s 60us/step - loss: 0.6195 - acc: 0.6599 - val_loss: 0.6143 - val_acc: 0.6179\n",
      "Epoch 3/200\n",
      "491/491 [==============================] - 0s 67us/step - loss: 0.5891 - acc: 0.6660 - val_loss: 0.5987 - val_acc: 0.6260\n",
      "Epoch 4/200\n",
      "491/491 [==============================] - 0s 73us/step - loss: 0.5807 - acc: 0.6680 - val_loss: 0.5943 - val_acc: 0.6423\n",
      "Epoch 5/200\n",
      "491/491 [==============================] - 0s 62us/step - loss: 0.5776 - acc: 0.7026 - val_loss: 0.5871 - val_acc: 0.6341\n",
      "Epoch 6/200\n",
      "491/491 [==============================] - 0s 63us/step - loss: 0.5660 - acc: 0.7108 - val_loss: 0.5856 - val_acc: 0.6341\n",
      "Epoch 7/200\n",
      "491/491 [==============================] - 0s 59us/step - loss: 0.5568 - acc: 0.7271 - val_loss: 0.5973 - val_acc: 0.6098\n",
      "Epoch 8/200\n",
      "491/491 [==============================] - 0s 81us/step - loss: 0.5470 - acc: 0.7271 - val_loss: 0.5911 - val_acc: 0.6179\n",
      "Epoch 9/200\n",
      "491/491 [==============================] - 0s 71us/step - loss: 0.5415 - acc: 0.7352 - val_loss: 0.5715 - val_acc: 0.6504\n",
      "Epoch 10/200\n",
      "491/491 [==============================] - 0s 63us/step - loss: 0.5330 - acc: 0.7413 - val_loss: 0.5839 - val_acc: 0.6748\n",
      "Epoch 11/200\n",
      "491/491 [==============================] - 0s 62us/step - loss: 0.5337 - acc: 0.7291 - val_loss: 0.5816 - val_acc: 0.6667\n",
      "Epoch 12/200\n",
      "491/491 [==============================] - 0s 67us/step - loss: 0.5240 - acc: 0.7413 - val_loss: 0.5829 - val_acc: 0.6829\n",
      "Epoch 13/200\n",
      "491/491 [==============================] - 0s 71us/step - loss: 0.5239 - acc: 0.7332 - val_loss: 0.5778 - val_acc: 0.6748\n",
      "Epoch 14/200\n",
      "491/491 [==============================] - 0s 67us/step - loss: 0.5200 - acc: 0.7556 - val_loss: 0.5812 - val_acc: 0.6423\n",
      "Epoch 15/200\n",
      "491/491 [==============================] - 0s 65us/step - loss: 0.5302 - acc: 0.7454 - val_loss: 0.5848 - val_acc: 0.6992\n",
      "Epoch 16/200\n",
      "491/491 [==============================] - 0s 74us/step - loss: 0.5161 - acc: 0.7393 - val_loss: 0.5722 - val_acc: 0.6748\n",
      "Epoch 17/200\n",
      "491/491 [==============================] - 0s 81us/step - loss: 0.5110 - acc: 0.7515 - val_loss: 0.5816 - val_acc: 0.6341\n",
      "Epoch 18/200\n",
      "491/491 [==============================] - 0s 63us/step - loss: 0.5209 - acc: 0.7230 - val_loss: 0.5798 - val_acc: 0.6911\n",
      "Epoch 19/200\n",
      "491/491 [==============================] - 0s 67us/step - loss: 0.5198 - acc: 0.7475 - val_loss: 0.5817 - val_acc: 0.6504\n",
      "Epoch 20/200\n",
      "491/491 [==============================] - 0s 62us/step - loss: 0.5047 - acc: 0.7515 - val_loss: 0.5767 - val_acc: 0.6748\n",
      "Epoch 21/200\n",
      "491/491 [==============================] - 0s 65us/step - loss: 0.5133 - acc: 0.7373 - val_loss: 0.5764 - val_acc: 0.7073\n",
      "Epoch 22/200\n",
      "491/491 [==============================] - 0s 67us/step - loss: 0.5106 - acc: 0.7475 - val_loss: 0.5905 - val_acc: 0.6585\n",
      "Epoch 23/200\n",
      "491/491 [==============================] - 0s 69us/step - loss: 0.5052 - acc: 0.7536 - val_loss: 0.5844 - val_acc: 0.6667\n",
      "Epoch 24/200\n",
      "491/491 [==============================] - 0s 69us/step - loss: 0.4964 - acc: 0.7413 - val_loss: 0.5767 - val_acc: 0.6829\n",
      "Epoch 25/200\n",
      "491/491 [==============================] - 0s 70us/step - loss: 0.4998 - acc: 0.7475 - val_loss: 0.5718 - val_acc: 0.6748\n",
      "Epoch 26/200\n",
      "491/491 [==============================] - 0s 78us/step - loss: 0.4914 - acc: 0.7637 - val_loss: 0.5807 - val_acc: 0.6748\n",
      "Epoch 27/200\n",
      "491/491 [==============================] - 0s 64us/step - loss: 0.4863 - acc: 0.7760 - val_loss: 0.5835 - val_acc: 0.6585\n",
      "Epoch 28/200\n",
      "491/491 [==============================] - 0s 70us/step - loss: 0.4781 - acc: 0.7597 - val_loss: 0.5703 - val_acc: 0.7154\n",
      "Epoch 29/200\n",
      "491/491 [==============================] - 0s 72us/step - loss: 0.4836 - acc: 0.7637 - val_loss: 0.5698 - val_acc: 0.6585\n",
      "Epoch 30/200\n",
      "491/491 [==============================] - 0s 63us/step - loss: 0.4803 - acc: 0.7637 - val_loss: 0.5857 - val_acc: 0.6911\n",
      "Epoch 31/200\n",
      "491/491 [==============================] - 0s 54us/step - loss: 0.4784 - acc: 0.7556 - val_loss: 0.5774 - val_acc: 0.7073\n",
      "Epoch 32/200\n",
      "491/491 [==============================] - 0s 66us/step - loss: 0.4726 - acc: 0.7658 - val_loss: 0.5793 - val_acc: 0.6911\n",
      "Epoch 33/200\n",
      "491/491 [==============================] - 0s 80us/step - loss: 0.4712 - acc: 0.7780 - val_loss: 0.5768 - val_acc: 0.6911\n",
      "Epoch 34/200\n",
      "491/491 [==============================] - 0s 64us/step - loss: 0.4674 - acc: 0.7862 - val_loss: 0.5559 - val_acc: 0.7154\n",
      "Epoch 35/200\n",
      "491/491 [==============================] - 0s 67us/step - loss: 0.4713 - acc: 0.7699 - val_loss: 0.5949 - val_acc: 0.6748\n",
      "Epoch 36/200\n",
      "491/491 [==============================] - 0s 72us/step - loss: 0.4729 - acc: 0.7719 - val_loss: 0.5791 - val_acc: 0.6911\n",
      "Epoch 37/200\n",
      "491/491 [==============================] - 0s 72us/step - loss: 0.4699 - acc: 0.7699 - val_loss: 0.5627 - val_acc: 0.6748\n",
      "Epoch 38/200\n",
      "491/491 [==============================] - 0s 75us/step - loss: 0.4673 - acc: 0.7760 - val_loss: 0.5575 - val_acc: 0.6911\n",
      "Epoch 39/200\n",
      "491/491 [==============================] - 0s 72us/step - loss: 0.4692 - acc: 0.7841 - val_loss: 0.5874 - val_acc: 0.7236\n",
      "Epoch 40/200\n",
      "491/491 [==============================] - 0s 71us/step - loss: 0.4772 - acc: 0.7617 - val_loss: 0.5795 - val_acc: 0.7073\n",
      "Epoch 41/200\n",
      "491/491 [==============================] - 0s 83us/step - loss: 0.4680 - acc: 0.7597 - val_loss: 0.5691 - val_acc: 0.6992\n",
      "Epoch 42/200\n",
      "491/491 [==============================] - 0s 75us/step - loss: 0.4653 - acc: 0.7780 - val_loss: 0.5825 - val_acc: 0.7073\n",
      "Epoch 43/200\n",
      "491/491 [==============================] - 0s 68us/step - loss: 0.4654 - acc: 0.7760 - val_loss: 0.5724 - val_acc: 0.7073\n",
      "Epoch 44/200\n",
      "491/491 [==============================] - 0s 80us/step - loss: 0.4616 - acc: 0.7862 - val_loss: 0.5753 - val_acc: 0.7154\n",
      "Epoch 45/200\n",
      "491/491 [==============================] - 0s 73us/step - loss: 0.4528 - acc: 0.7923 - val_loss: 0.5767 - val_acc: 0.6911\n",
      "Epoch 46/200\n",
      "491/491 [==============================] - 0s 115us/step - loss: 0.4588 - acc: 0.7780 - val_loss: 0.5705 - val_acc: 0.7154\n",
      "Epoch 47/200\n",
      "491/491 [==============================] - 0s 67us/step - loss: 0.4566 - acc: 0.7760 - val_loss: 0.5733 - val_acc: 0.6911\n",
      "Epoch 48/200\n",
      "491/491 [==============================] - 0s 74us/step - loss: 0.4581 - acc: 0.7780 - val_loss: 0.5887 - val_acc: 0.7073\n",
      "Epoch 49/200\n",
      "491/491 [==============================] - 0s 70us/step - loss: 0.4531 - acc: 0.7719 - val_loss: 0.5740 - val_acc: 0.7073\n",
      "Epoch 50/200\n",
      "491/491 [==============================] - 0s 66us/step - loss: 0.4451 - acc: 0.7943 - val_loss: 0.5969 - val_acc: 0.6585\n",
      "Epoch 51/200\n",
      "491/491 [==============================] - 0s 64us/step - loss: 0.4487 - acc: 0.7719 - val_loss: 0.5742 - val_acc: 0.6748\n",
      "Epoch 52/200\n",
      "491/491 [==============================] - 0s 75us/step - loss: 0.4440 - acc: 0.7984 - val_loss: 0.5732 - val_acc: 0.7398\n",
      "Epoch 53/200\n",
      "491/491 [==============================] - 0s 73us/step - loss: 0.4533 - acc: 0.7821 - val_loss: 0.5744 - val_acc: 0.7073\n",
      "Epoch 54/200\n",
      "491/491 [==============================] - 0s 74us/step - loss: 0.4475 - acc: 0.7862 - val_loss: 0.5783 - val_acc: 0.7236\n",
      "Epoch 55/200\n",
      "491/491 [==============================] - 0s 85us/step - loss: 0.4443 - acc: 0.7800 - val_loss: 0.6063 - val_acc: 0.6992\n",
      "Epoch 56/200\n",
      "491/491 [==============================] - 0s 72us/step - loss: 0.4397 - acc: 0.8065 - val_loss: 0.5750 - val_acc: 0.7236\n",
      "Epoch 57/200\n",
      "491/491 [==============================] - 0s 67us/step - loss: 0.4367 - acc: 0.7963 - val_loss: 0.5877 - val_acc: 0.7154\n",
      "Epoch 58/200\n",
      "491/491 [==============================] - 0s 65us/step - loss: 0.4427 - acc: 0.7841 - val_loss: 0.5851 - val_acc: 0.7236\n",
      "Epoch 59/200\n",
      "491/491 [==============================] - 0s 62us/step - loss: 0.4231 - acc: 0.7902 - val_loss: 0.5612 - val_acc: 0.7154\n",
      "Epoch 60/200\n",
      "491/491 [==============================] - 0s 68us/step - loss: 0.4452 - acc: 0.7841 - val_loss: 0.5704 - val_acc: 0.7317\n",
      "Epoch 61/200\n",
      "491/491 [==============================] - 0s 67us/step - loss: 0.4254 - acc: 0.8004 - val_loss: 0.5688 - val_acc: 0.7154\n",
      "Epoch 62/200\n",
      "491/491 [==============================] - 0s 72us/step - loss: 0.4155 - acc: 0.8167 - val_loss: 0.5813 - val_acc: 0.7236\n",
      "Epoch 63/200\n",
      "491/491 [==============================] - 0s 69us/step - loss: 0.4196 - acc: 0.8045 - val_loss: 0.5817 - val_acc: 0.7398\n",
      "Epoch 64/200\n",
      "491/491 [==============================] - 0s 79us/step - loss: 0.4236 - acc: 0.7984 - val_loss: 0.5824 - val_acc: 0.7073\n",
      "Epoch 65/200\n",
      "491/491 [==============================] - 0s 69us/step - loss: 0.4247 - acc: 0.7963 - val_loss: 0.5957 - val_acc: 0.6911\n",
      "Epoch 66/200\n",
      "491/491 [==============================] - 0s 69us/step - loss: 0.4112 - acc: 0.8065 - val_loss: 0.5766 - val_acc: 0.7317\n",
      "Epoch 67/200\n",
      "491/491 [==============================] - 0s 72us/step - loss: 0.4073 - acc: 0.8126 - val_loss: 0.5796 - val_acc: 0.7236\n",
      "Epoch 68/200\n",
      "491/491 [==============================] - 0s 81us/step - loss: 0.4032 - acc: 0.8065 - val_loss: 0.5786 - val_acc: 0.7480\n",
      "Epoch 69/200\n",
      "491/491 [==============================] - 0s 70us/step - loss: 0.4217 - acc: 0.7984 - val_loss: 0.5672 - val_acc: 0.7236\n",
      "Epoch 70/200\n",
      "491/491 [==============================] - 0s 73us/step - loss: 0.4314 - acc: 0.7963 - val_loss: 0.5884 - val_acc: 0.6748\n",
      "Epoch 71/200\n",
      "491/491 [==============================] - 0s 63us/step - loss: 0.4161 - acc: 0.8004 - val_loss: 0.5701 - val_acc: 0.7154\n",
      "Epoch 72/200\n",
      "491/491 [==============================] - 0s 63us/step - loss: 0.4092 - acc: 0.8106 - val_loss: 0.5822 - val_acc: 0.7398\n",
      "Epoch 73/200\n",
      "491/491 [==============================] - 0s 73us/step - loss: 0.4090 - acc: 0.8147 - val_loss: 0.5879 - val_acc: 0.7154\n",
      "Epoch 74/200\n",
      "491/491 [==============================] - 0s 82us/step - loss: 0.4081 - acc: 0.8004 - val_loss: 0.5884 - val_acc: 0.7561\n",
      "Epoch 75/200\n",
      "491/491 [==============================] - 0s 79us/step - loss: 0.4018 - acc: 0.7984 - val_loss: 0.5741 - val_acc: 0.7561\n",
      "Epoch 76/200\n",
      "491/491 [==============================] - 0s 77us/step - loss: 0.3919 - acc: 0.8269 - val_loss: 0.6006 - val_acc: 0.7317\n",
      "Epoch 77/200\n",
      "491/491 [==============================] - 0s 69us/step - loss: 0.4107 - acc: 0.8045 - val_loss: 0.5978 - val_acc: 0.7073\n",
      "Epoch 78/200\n",
      "491/491 [==============================] - 0s 72us/step - loss: 0.4101 - acc: 0.7943 - val_loss: 0.5820 - val_acc: 0.7480\n",
      "Epoch 79/200\n",
      "491/491 [==============================] - 0s 70us/step - loss: 0.4096 - acc: 0.7902 - val_loss: 0.5888 - val_acc: 0.7317\n",
      "Epoch 80/200\n",
      "491/491 [==============================] - 0s 58us/step - loss: 0.4105 - acc: 0.8167 - val_loss: 0.5939 - val_acc: 0.7154\n",
      "Epoch 81/200\n",
      "491/491 [==============================] - 0s 70us/step - loss: 0.4151 - acc: 0.7902 - val_loss: 0.5874 - val_acc: 0.6829\n",
      "Epoch 82/200\n",
      "491/491 [==============================] - 0s 64us/step - loss: 0.4088 - acc: 0.7984 - val_loss: 0.5481 - val_acc: 0.7480\n",
      "Epoch 83/200\n",
      "491/491 [==============================] - 0s 65us/step - loss: 0.4167 - acc: 0.7882 - val_loss: 0.5990 - val_acc: 0.7236\n",
      "Epoch 84/200\n",
      "491/491 [==============================] - 0s 59us/step - loss: 0.4072 - acc: 0.8106 - val_loss: 0.6136 - val_acc: 0.6911\n",
      "Epoch 85/200\n",
      "491/491 [==============================] - 0s 65us/step - loss: 0.4027 - acc: 0.7923 - val_loss: 0.6025 - val_acc: 0.7154\n",
      "Epoch 86/200\n",
      "491/491 [==============================] - 0s 70us/step - loss: 0.4017 - acc: 0.8045 - val_loss: 0.5933 - val_acc: 0.7317\n",
      "Epoch 87/200\n",
      "491/491 [==============================] - 0s 55us/step - loss: 0.3792 - acc: 0.8106 - val_loss: 0.6209 - val_acc: 0.6829\n",
      "Epoch 88/200\n",
      "491/491 [==============================] - 0s 66us/step - loss: 0.4084 - acc: 0.8045 - val_loss: 0.5932 - val_acc: 0.7236\n",
      "Epoch 89/200\n",
      "491/491 [==============================] - 0s 70us/step - loss: 0.3803 - acc: 0.8269 - val_loss: 0.5945 - val_acc: 0.7317\n",
      "Epoch 90/200\n",
      "491/491 [==============================] - 0s 68us/step - loss: 0.3882 - acc: 0.8208 - val_loss: 0.5745 - val_acc: 0.7398\n",
      "Epoch 91/200\n",
      "491/491 [==============================] - 0s 77us/step - loss: 0.3718 - acc: 0.8310 - val_loss: 0.5900 - val_acc: 0.7236\n",
      "Epoch 92/200\n",
      "491/491 [==============================] - 0s 66us/step - loss: 0.3786 - acc: 0.8208 - val_loss: 0.5960 - val_acc: 0.7154\n",
      "Epoch 93/200\n",
      "491/491 [==============================] - 0s 70us/step - loss: 0.3777 - acc: 0.8248 - val_loss: 0.6062 - val_acc: 0.7398\n",
      "Epoch 94/200\n",
      "491/491 [==============================] - 0s 72us/step - loss: 0.3738 - acc: 0.8147 - val_loss: 0.5886 - val_acc: 0.7317\n",
      "Epoch 95/200\n",
      "491/491 [==============================] - 0s 68us/step - loss: 0.3604 - acc: 0.8391 - val_loss: 0.5811 - val_acc: 0.7154\n",
      "Epoch 96/200\n",
      "491/491 [==============================] - 0s 66us/step - loss: 0.3690 - acc: 0.8228 - val_loss: 0.6013 - val_acc: 0.7480\n",
      "Epoch 97/200\n",
      "491/491 [==============================] - 0s 71us/step - loss: 0.3607 - acc: 0.8391 - val_loss: 0.5878 - val_acc: 0.7480\n",
      "Epoch 98/200\n",
      "491/491 [==============================] - 0s 63us/step - loss: 0.3613 - acc: 0.8228 - val_loss: 0.6017 - val_acc: 0.7480\n",
      "Epoch 99/200\n",
      "491/491 [==============================] - 0s 63us/step - loss: 0.3979 - acc: 0.8024 - val_loss: 0.6485 - val_acc: 0.7154\n",
      "Epoch 100/200\n",
      "491/491 [==============================] - 0s 70us/step - loss: 0.3887 - acc: 0.8248 - val_loss: 0.6000 - val_acc: 0.7398\n",
      "Epoch 101/200\n",
      "491/491 [==============================] - 0s 69us/step - loss: 0.3732 - acc: 0.8228 - val_loss: 0.6347 - val_acc: 0.7154\n",
      "Epoch 102/200\n",
      "491/491 [==============================] - 0s 73us/step - loss: 0.3620 - acc: 0.8289 - val_loss: 0.5942 - val_acc: 0.6829\n",
      "Epoch 103/200\n",
      "491/491 [==============================] - 0s 77us/step - loss: 0.3576 - acc: 0.8371 - val_loss: 0.6130 - val_acc: 0.7154\n",
      "Epoch 104/200\n",
      "491/491 [==============================] - 0s 74us/step - loss: 0.3512 - acc: 0.8350 - val_loss: 0.6322 - val_acc: 0.7073\n",
      "Epoch 105/200\n",
      "491/491 [==============================] - 0s 62us/step - loss: 0.3443 - acc: 0.8411 - val_loss: 0.6249 - val_acc: 0.6992\n",
      "Epoch 106/200\n",
      "491/491 [==============================] - 0s 66us/step - loss: 0.3495 - acc: 0.8371 - val_loss: 0.5814 - val_acc: 0.7480\n",
      "Epoch 107/200\n",
      "491/491 [==============================] - 0s 72us/step - loss: 0.3572 - acc: 0.8289 - val_loss: 0.6436 - val_acc: 0.6829\n",
      "Epoch 108/200\n",
      "491/491 [==============================] - 0s 69us/step - loss: 0.3525 - acc: 0.8350 - val_loss: 0.5950 - val_acc: 0.6992\n",
      "Epoch 109/200\n",
      "491/491 [==============================] - 0s 66us/step - loss: 0.3447 - acc: 0.8534 - val_loss: 0.6292 - val_acc: 0.7236\n",
      "Epoch 110/200\n",
      "491/491 [==============================] - 0s 64us/step - loss: 0.3362 - acc: 0.8411 - val_loss: 0.6262 - val_acc: 0.6992\n",
      "Epoch 111/200\n",
      "491/491 [==============================] - 0s 70us/step - loss: 0.3304 - acc: 0.8534 - val_loss: 0.6291 - val_acc: 0.7317\n",
      "Epoch 112/200\n",
      "491/491 [==============================] - 0s 69us/step - loss: 0.3491 - acc: 0.8167 - val_loss: 0.6209 - val_acc: 0.7398\n",
      "Epoch 113/200\n",
      "491/491 [==============================] - 0s 68us/step - loss: 0.3490 - acc: 0.8330 - val_loss: 0.6301 - val_acc: 0.7317\n",
      "Epoch 114/200\n",
      "491/491 [==============================] - 0s 64us/step - loss: 0.3393 - acc: 0.8452 - val_loss: 0.6010 - val_acc: 0.6748\n",
      "Epoch 115/200\n",
      "491/491 [==============================] - 0s 65us/step - loss: 0.3356 - acc: 0.8493 - val_loss: 0.6702 - val_acc: 0.6667\n",
      "Epoch 116/200\n",
      "491/491 [==============================] - 0s 66us/step - loss: 0.3512 - acc: 0.8330 - val_loss: 0.6058 - val_acc: 0.7073\n",
      "Epoch 117/200\n",
      "491/491 [==============================] - 0s 67us/step - loss: 0.3550 - acc: 0.8452 - val_loss: 0.6733 - val_acc: 0.6748\n",
      "Epoch 118/200\n",
      "491/491 [==============================] - 0s 59us/step - loss: 0.3440 - acc: 0.8452 - val_loss: 0.6141 - val_acc: 0.7236\n",
      "Epoch 119/200\n",
      "491/491 [==============================] - 0s 64us/step - loss: 0.3368 - acc: 0.8432 - val_loss: 0.6189 - val_acc: 0.6992\n",
      "Epoch 120/200\n",
      "491/491 [==============================] - 0s 67us/step - loss: 0.3298 - acc: 0.8513 - val_loss: 0.6236 - val_acc: 0.7073\n",
      "Epoch 121/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "491/491 [==============================] - 0s 86us/step - loss: 0.3296 - acc: 0.8432 - val_loss: 0.6586 - val_acc: 0.6829\n",
      "Epoch 122/200\n",
      "491/491 [==============================] - 0s 85us/step - loss: 0.3407 - acc: 0.8330 - val_loss: 0.6532 - val_acc: 0.7154\n",
      "Epoch 123/200\n",
      "491/491 [==============================] - 0s 84us/step - loss: 0.3461 - acc: 0.8452 - val_loss: 0.6266 - val_acc: 0.7154\n",
      "Epoch 124/200\n",
      "491/491 [==============================] - 0s 75us/step - loss: 0.3367 - acc: 0.8330 - val_loss: 0.6401 - val_acc: 0.7073\n",
      "Epoch 125/200\n",
      "491/491 [==============================] - 0s 81us/step - loss: 0.3085 - acc: 0.8493 - val_loss: 0.6633 - val_acc: 0.6829\n",
      "Epoch 126/200\n",
      "491/491 [==============================] - 0s 74us/step - loss: 0.3311 - acc: 0.8371 - val_loss: 0.6510 - val_acc: 0.6829\n",
      "Epoch 127/200\n",
      "491/491 [==============================] - 0s 84us/step - loss: 0.3454 - acc: 0.8310 - val_loss: 0.6278 - val_acc: 0.6992\n",
      "Epoch 128/200\n",
      "491/491 [==============================] - ETA: 0s - loss: 0.4111 - acc: 0.750 - 0s 70us/step - loss: 0.3438 - acc: 0.8473 - val_loss: 0.6774 - val_acc: 0.6667\n",
      "Epoch 129/200\n",
      "491/491 [==============================] - 0s 74us/step - loss: 0.3203 - acc: 0.8717 - val_loss: 0.6860 - val_acc: 0.6911\n",
      "Epoch 130/200\n",
      "491/491 [==============================] - 0s 88us/step - loss: 0.3144 - acc: 0.8635 - val_loss: 0.6626 - val_acc: 0.7154\n",
      "Epoch 131/200\n",
      "491/491 [==============================] - 0s 67us/step - loss: 0.3293 - acc: 0.8432 - val_loss: 0.6929 - val_acc: 0.6748\n",
      "Epoch 132/200\n",
      "491/491 [==============================] - 0s 74us/step - loss: 0.3201 - acc: 0.8534 - val_loss: 0.6467 - val_acc: 0.7073\n",
      "Epoch 133/200\n",
      "491/491 [==============================] - 0s 76us/step - loss: 0.3173 - acc: 0.8473 - val_loss: 0.6862 - val_acc: 0.6748\n",
      "Epoch 134/200\n",
      "491/491 [==============================] - 0s 86us/step - loss: 0.3170 - acc: 0.8513 - val_loss: 0.6776 - val_acc: 0.6829\n",
      "Epoch 135/200\n",
      "491/491 [==============================] - 0s 77us/step - loss: 0.3052 - acc: 0.8493 - val_loss: 0.6743 - val_acc: 0.6829\n",
      "Epoch 136/200\n",
      "491/491 [==============================] - 0s 91us/step - loss: 0.2898 - acc: 0.8697 - val_loss: 0.6766 - val_acc: 0.6585\n",
      "Epoch 137/200\n",
      "491/491 [==============================] - 0s 93us/step - loss: 0.2833 - acc: 0.8778 - val_loss: 0.6888 - val_acc: 0.6748\n",
      "Epoch 138/200\n",
      "491/491 [==============================] - 0s 65us/step - loss: 0.2825 - acc: 0.8798 - val_loss: 0.6717 - val_acc: 0.6911\n",
      "Epoch 139/200\n",
      "491/491 [==============================] - 0s 70us/step - loss: 0.2808 - acc: 0.8758 - val_loss: 0.7272 - val_acc: 0.6829\n",
      "Epoch 140/200\n",
      "491/491 [==============================] - 0s 65us/step - loss: 0.2915 - acc: 0.8473 - val_loss: 0.6885 - val_acc: 0.6748\n",
      "Epoch 141/200\n",
      "491/491 [==============================] - 0s 68us/step - loss: 0.2982 - acc: 0.8717 - val_loss: 0.6964 - val_acc: 0.6911\n",
      "Epoch 142/200\n",
      "491/491 [==============================] - 0s 70us/step - loss: 0.3019 - acc: 0.8432 - val_loss: 0.6762 - val_acc: 0.6829\n",
      "Epoch 143/200\n",
      "491/491 [==============================] - 0s 74us/step - loss: 0.2797 - acc: 0.8758 - val_loss: 0.6929 - val_acc: 0.6992\n",
      "Epoch 144/200\n",
      "491/491 [==============================] - 0s 67us/step - loss: 0.2775 - acc: 0.8676 - val_loss: 0.7208 - val_acc: 0.6748\n",
      "Epoch 145/200\n",
      "491/491 [==============================] - 0s 59us/step - loss: 0.2840 - acc: 0.8676 - val_loss: 0.7139 - val_acc: 0.6829\n",
      "Epoch 146/200\n",
      "491/491 [==============================] - 0s 70us/step - loss: 0.3162 - acc: 0.8574 - val_loss: 0.7361 - val_acc: 0.6992\n",
      "Epoch 147/200\n",
      "491/491 [==============================] - 0s 65us/step - loss: 0.3099 - acc: 0.8574 - val_loss: 0.6978 - val_acc: 0.6829\n",
      "Epoch 148/200\n",
      "491/491 [==============================] - 0s 60us/step - loss: 0.2964 - acc: 0.8513 - val_loss: 0.7353 - val_acc: 0.6504\n",
      "Epoch 149/200\n",
      "491/491 [==============================] - 0s 76us/step - loss: 0.2827 - acc: 0.8697 - val_loss: 0.7038 - val_acc: 0.6829\n",
      "Epoch 150/200\n",
      "491/491 [==============================] - 0s 65us/step - loss: 0.2927 - acc: 0.8697 - val_loss: 0.6453 - val_acc: 0.7236\n",
      "Epoch 151/200\n",
      "491/491 [==============================] - 0s 68us/step - loss: 0.2802 - acc: 0.8798 - val_loss: 0.7316 - val_acc: 0.6748\n",
      "Epoch 152/200\n",
      "491/491 [==============================] - 0s 70us/step - loss: 0.2706 - acc: 0.8697 - val_loss: 0.6697 - val_acc: 0.6911\n",
      "Epoch 153/200\n",
      "491/491 [==============================] - 0s 57us/step - loss: 0.3071 - acc: 0.8371 - val_loss: 0.7419 - val_acc: 0.6504\n",
      "Epoch 154/200\n",
      "491/491 [==============================] - 0s 64us/step - loss: 0.3023 - acc: 0.8676 - val_loss: 0.6866 - val_acc: 0.6829\n",
      "Epoch 155/200\n",
      "491/491 [==============================] - 0s 66us/step - loss: 0.2708 - acc: 0.8880 - val_loss: 0.7464 - val_acc: 0.6748\n",
      "Epoch 156/200\n",
      "491/491 [==============================] - 0s 64us/step - loss: 0.2641 - acc: 0.8737 - val_loss: 0.6970 - val_acc: 0.6911\n",
      "Epoch 157/200\n",
      "491/491 [==============================] - 0s 61us/step - loss: 0.2573 - acc: 0.8921 - val_loss: 0.7308 - val_acc: 0.6992\n",
      "Epoch 158/200\n",
      "491/491 [==============================] - 0s 77us/step - loss: 0.2530 - acc: 0.9002 - val_loss: 0.7603 - val_acc: 0.6748\n",
      "Epoch 159/200\n",
      "491/491 [==============================] - 0s 78us/step - loss: 0.2714 - acc: 0.8819 - val_loss: 0.7300 - val_acc: 0.6829\n",
      "Epoch 160/200\n",
      "491/491 [==============================] - 0s 78us/step - loss: 0.2579 - acc: 0.8839 - val_loss: 0.7236 - val_acc: 0.6748\n",
      "Epoch 161/200\n",
      "491/491 [==============================] - 0s 63us/step - loss: 0.2423 - acc: 0.8961 - val_loss: 0.7171 - val_acc: 0.6992\n",
      "Epoch 162/200\n",
      "491/491 [==============================] - 0s 72us/step - loss: 0.2465 - acc: 0.8961 - val_loss: 0.7540 - val_acc: 0.6911\n",
      "Epoch 163/200\n",
      "491/491 [==============================] - 0s 69us/step - loss: 0.2560 - acc: 0.8900 - val_loss: 0.7558 - val_acc: 0.6748\n",
      "Epoch 164/200\n",
      "491/491 [==============================] - 0s 66us/step - loss: 0.2587 - acc: 0.8941 - val_loss: 0.7517 - val_acc: 0.6667\n",
      "Epoch 165/200\n",
      "491/491 [==============================] - 0s 67us/step - loss: 0.2488 - acc: 0.8880 - val_loss: 0.7272 - val_acc: 0.6748\n",
      "Epoch 166/200\n",
      "491/491 [==============================] - 0s 63us/step - loss: 0.2738 - acc: 0.8574 - val_loss: 0.7588 - val_acc: 0.6911\n",
      "Epoch 167/200\n",
      "491/491 [==============================] - 0s 66us/step - loss: 0.2597 - acc: 0.8880 - val_loss: 0.7846 - val_acc: 0.6585\n",
      "Epoch 168/200\n",
      "491/491 [==============================] - 0s 62us/step - loss: 0.2451 - acc: 0.8941 - val_loss: 0.7423 - val_acc: 0.6748\n",
      "Epoch 169/200\n",
      "491/491 [==============================] - 0s 61us/step - loss: 0.2501 - acc: 0.8921 - val_loss: 0.7979 - val_acc: 0.6585\n",
      "Epoch 170/200\n",
      "491/491 [==============================] - 0s 62us/step - loss: 0.2714 - acc: 0.8676 - val_loss: 0.7424 - val_acc: 0.6585\n",
      "Epoch 171/200\n",
      "491/491 [==============================] - 0s 58us/step - loss: 0.2635 - acc: 0.8859 - val_loss: 0.7919 - val_acc: 0.6504\n",
      "Epoch 172/200\n",
      "491/491 [==============================] - 0s 68us/step - loss: 0.2648 - acc: 0.8737 - val_loss: 0.7048 - val_acc: 0.6992\n",
      "Epoch 173/200\n",
      "491/491 [==============================] - 0s 64us/step - loss: 0.2523 - acc: 0.8839 - val_loss: 0.8078 - val_acc: 0.6667\n",
      "Epoch 174/200\n",
      "491/491 [==============================] - 0s 56us/step - loss: 0.2523 - acc: 0.8880 - val_loss: 0.7650 - val_acc: 0.6585\n",
      "Epoch 175/200\n",
      "491/491 [==============================] - 0s 55us/step - loss: 0.2698 - acc: 0.8737 - val_loss: 0.7653 - val_acc: 0.6748\n",
      "Epoch 176/200\n",
      "491/491 [==============================] - 0s 64us/step - loss: 0.2587 - acc: 0.8798 - val_loss: 0.7669 - val_acc: 0.6748\n",
      "Epoch 177/200\n",
      "491/491 [==============================] - 0s 65us/step - loss: 0.2725 - acc: 0.8758 - val_loss: 0.7715 - val_acc: 0.6667\n",
      "Epoch 178/200\n",
      "491/491 [==============================] - 0s 64us/step - loss: 0.2574 - acc: 0.8839 - val_loss: 0.7832 - val_acc: 0.6667\n",
      "Epoch 179/200\n",
      "491/491 [==============================] - 0s 55us/step - loss: 0.2374 - acc: 0.9043 - val_loss: 0.7834 - val_acc: 0.6829\n",
      "Epoch 180/200\n",
      "491/491 [==============================] - 0s 58us/step - loss: 0.2443 - acc: 0.8941 - val_loss: 0.7973 - val_acc: 0.6585\n",
      "Epoch 181/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "491/491 [==============================] - 0s 63us/step - loss: 0.2503 - acc: 0.9002 - val_loss: 0.7943 - val_acc: 0.6504\n",
      "Epoch 182/200\n",
      "491/491 [==============================] - 0s 58us/step - loss: 0.2491 - acc: 0.8880 - val_loss: 0.8083 - val_acc: 0.6911\n",
      "Epoch 183/200\n",
      "491/491 [==============================] - 0s 65us/step - loss: 0.2624 - acc: 0.8819 - val_loss: 0.7507 - val_acc: 0.6992\n",
      "Epoch 184/200\n",
      "491/491 [==============================] - 0s 64us/step - loss: 0.2450 - acc: 0.8880 - val_loss: 0.8410 - val_acc: 0.6179\n",
      "Epoch 185/200\n",
      "491/491 [==============================] - 0s 61us/step - loss: 0.2489 - acc: 0.8880 - val_loss: 0.7869 - val_acc: 0.6829\n",
      "Epoch 186/200\n",
      "491/491 [==============================] - 0s 66us/step - loss: 0.2405 - acc: 0.8961 - val_loss: 0.7868 - val_acc: 0.6585\n",
      "Epoch 187/200\n",
      "491/491 [==============================] - 0s 75us/step - loss: 0.2277 - acc: 0.9063 - val_loss: 0.8452 - val_acc: 0.6341\n",
      "Epoch 188/200\n",
      "491/491 [==============================] - 0s 74us/step - loss: 0.2248 - acc: 0.9043 - val_loss: 0.7879 - val_acc: 0.6585\n",
      "Epoch 189/200\n",
      "491/491 [==============================] - 0s 68us/step - loss: 0.2449 - acc: 0.8839 - val_loss: 0.7829 - val_acc: 0.6423\n",
      "Epoch 190/200\n",
      "491/491 [==============================] - 0s 60us/step - loss: 0.2454 - acc: 0.9002 - val_loss: 0.8486 - val_acc: 0.6585\n",
      "Epoch 191/200\n",
      "491/491 [==============================] - 0s 64us/step - loss: 0.2905 - acc: 0.8737 - val_loss: 0.7898 - val_acc: 0.6748\n",
      "Epoch 192/200\n",
      "491/491 [==============================] - 0s 68us/step - loss: 0.2899 - acc: 0.8697 - val_loss: 0.8513 - val_acc: 0.6667\n",
      "Epoch 193/200\n",
      "491/491 [==============================] - 0s 64us/step - loss: 0.2597 - acc: 0.8859 - val_loss: 0.8590 - val_acc: 0.6504\n",
      "Epoch 194/200\n",
      "491/491 [==============================] - 0s 61us/step - loss: 0.2487 - acc: 0.9002 - val_loss: 0.8205 - val_acc: 0.6341\n",
      "Epoch 195/200\n",
      "491/491 [==============================] - 0s 66us/step - loss: 0.2425 - acc: 0.8921 - val_loss: 0.8336 - val_acc: 0.6829\n",
      "Epoch 196/200\n",
      "491/491 [==============================] - 0s 66us/step - loss: 0.2388 - acc: 0.8880 - val_loss: 0.8313 - val_acc: 0.6585\n",
      "Epoch 197/200\n",
      "491/491 [==============================] - 0s 73us/step - loss: 0.2432 - acc: 0.8819 - val_loss: 0.8698 - val_acc: 0.6504\n",
      "Epoch 198/200\n",
      "491/491 [==============================] - 0s 71us/step - loss: 0.2317 - acc: 0.9084 - val_loss: 0.8617 - val_acc: 0.6260\n",
      "Epoch 199/200\n",
      "491/491 [==============================] - 0s 70us/step - loss: 0.2188 - acc: 0.9043 - val_loss: 0.8772 - val_acc: 0.6748\n",
      "Epoch 200/200\n",
      "491/491 [==============================] - 0s 68us/step - loss: 0.2427 - acc: 0.8921 - val_loss: 0.8748 - val_acc: 0.6667\n"
     ]
    }
   ],
   "source": [
    "checkpoint = ModelCheckpoint('3_layer_dense.h5', monitor='val_loss', save_best_only=True)\n",
    "cb = [checkpoint]\n",
    "model = Sequential()\n",
    "model.add(Dense(64, activation='relu', input_dim=8)) \n",
    "model.add(Dense(64, activation='tanh'))\n",
    "model.add(Dense(36, activation='tanh'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer=(optimizers.Adam(lr=.00045)),\n",
    "              loss ='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "history = model.fit(x_train, y_train, \n",
    "                  batch_size=32,\n",
    "                  epochs=200,\n",
    "                validation_split=0.2,\n",
    "                    callbacks=cb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAD8CAYAAABw1c+bAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n6YFER5qpZQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x187a49e8f28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['acc'], label = 'training acc')\n",
    "plt.plot(history.history['val_acc'], label = 'validation acc')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "checkpoint = ModelCheckpoint('3_layer_dense.h5', monitor='val_loss', save_best_only=True)\n",
    "cb = [checkpoint]\n",
    "model = Sequential()\n",
    "model.add(Dense(30, activation='relu', input_dim=8)) \n",
    "model.add(Dense(15, activation='tanh'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "\n",
    "model.compile(optimizer=(optimizers.Adam(lr=.0005)),\n",
    "              loss ='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "history = model.fit(x_train, y_train, \n",
    "                  batch_size=32,\n",
    "                  epochs=200,\n",
    "                validation_split=0.2,\n",
    "                    callbacks=cb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nuYuIFJDKXUSkgP4H8O63KGnePDQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x187a4155550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['acc'], label = 'training acc')\n",
    "plt.plot(history.history['val_acc'], label = 'validation acc')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "154/154 [==============================] - 0s 59us/step\n",
      "[0.60227290376440268, 0.72727272417638211]\n"
     ]
    }
   ],
   "source": [
    "tst_score = model.evaluate(x_test, y_test)\n",
    "print(tst_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "614/614 [==============================] - 0s 33us/step\n",
      "[0.51191548124586717, 0.75081433244170892]\n"
     ]
    }
   ],
   "source": [
    "train_score = model.evaluate(x_train, y_train)\n",
    "print(train_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# There is much less bias train accuracy 75% test 72%"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
